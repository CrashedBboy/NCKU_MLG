{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machine with BPR loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set-up\n",
    "import dependent packages and declare consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\lightfm\\lib\\site-packages\\lightfm\\_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn('LightFM was compiled without OpenMP support. '\n"
     ]
    }
   ],
   "source": [
    "# package initialization\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consts\n",
    "\n",
    "DATASET = '../../data/extracted/LON-A/London_Attractions_Complete_Review.csv'\n",
    "# DATASET = '../../data/extracted/NYC-R/New_York_City_Restaurant_Complete_Review.csv'\n",
    "OCCURENCE_THRESHOLD = 5\n",
    "\n",
    "USER_FEATURES = ['ugender', 'ucity', 'ucountry', 'ulevel']\n",
    "ITEM_FEATURES = ['irating', 'itag']\n",
    "# ITEM_FEATURES = ['iattribute', 'irating', 'itag', 'iprice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read Dataset\n",
    "read dataset in csv format into pandas.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "\n",
    "df = pd.read_csv(DATASET, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: \n",
      " ['Unnamed: 0', 'Unnamed: 0.1', 'iid', 'rid', 'rimages', 'rquote', 'rrate', 'rtime', 'uprofile', 'uage', 'ucity', 'ucountry', 'ugender', 'uhometown', 'uid_index', 'ulevel', 'uname_y', 'usince', 'ustate', 'ustyle', 'iattribute', 'ilocality', 'iname', 'ipopularity', 'ipost', 'irating', 'iregion', 'istreet', 'itag']\n",
      "\n",
      "Shape: \n",
      " (136978, 29)\n"
     ]
    }
   ],
   "source": [
    "# print dataset information\n",
    "\n",
    "print(\"Columns: \\n\", list(df.columns))\n",
    "print(\"\\nShape: \\n\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "* Retain users/items with at least five ratings only\n",
    "* Data splitting\n",
    "  - the latest 20% interactions (by time)\n",
    "  - Randomly split the remaining data into training (70%) and validation (10%) sets\n",
    "* Transform the ratings into binary implicit feedback as ground truth, indicating whether the user has interacted with the specific item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_time(df):\n",
    "    \n",
    "    # here we use 'rid' for sorting becaz it's auto incrementing\n",
    "    return df.sort_values(by=['rid'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_occurrence(df, column, threshold):\n",
    "    return df.groupby(column).filter(lambda x: len(x) >= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_binary(df):\n",
    "    df.loc[df['rrate'] != \"None\", 'rrate'] = 1.0\n",
    "    df.loc[df['rrate'] == \"None\", 'rrate'] = 0.0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(dataframe):\n",
    "    \n",
    "    # sort by time (ascending order)\n",
    "    df = sort_by_time(dataframe)\n",
    "    \n",
    "    # retrieve needed columns\n",
    "    df = df[['uid_index', 'iid', 'rrate'] + USER_FEATURES + ITEM_FEATURES]\n",
    "    \n",
    "    # convert ratings into binarys\n",
    "    df = convert_binary(df)\n",
    "    \n",
    "    df['rrate'] = pd.to_numeric(df['rrate'])\n",
    "    \n",
    "    # Retain users/items with at least five ratings only\n",
    "    df = filter_by_occurrence(df, 'iid', OCCURENCE_THRESHOLD)\n",
    "    df = filter_by_occurrence(df, 'uid_index', OCCURENCE_THRESHOLD)\n",
    "    \n",
    "    # split dataset into training set, validation set and test set\n",
    "    users = df.groupby('uid_index')\n",
    "    \n",
    "    test_df = pd.DataFrame()\n",
    "    train_validation_df = pd.DataFrame()\n",
    "    \n",
    "    # for each user, get its latest 20% rating as test set\n",
    "    for uid in users.size().to_dict().keys():\n",
    "        user = users.get_group(uid)\n",
    "        split_idx = int(len(user)*0.8)\n",
    "        test_df = test_df.append(user.iloc[split_idx:])\n",
    "        train_validation_df = train_validation_df.append(user.iloc[:split_idx])\n",
    "    \n",
    "    train_validation_df = train_validation_df.reindex(np.random.permutation(train_validation_df.index)) # shuffle\n",
    "    train_df = train_validation_df.iloc[:int(len(train_validation_df)*0.875)]\n",
    "    validation_df = train_validation_df.iloc[int(len(train_validation_df)*0.875):]\n",
    "    \n",
    "    return (train_df, validation_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size:  (90209, 9)\n",
      "validation set size:  (12887, 9)\n",
      "test set size:  (33178, 9)\n"
     ]
    }
   ],
   "source": [
    "# dataset preprocessing\n",
    "\n",
    "train_df, validation_df, test_df = data_preprocess(df)\n",
    "print(\"training set size: \", train_df.shape)\n",
    "print(\"validation set size: \", validation_df.shape)\n",
    "print(\"test set size: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. load into LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all unique user ids\n",
    "def get_unique_uids(df):\n",
    "    return df['uid_index'].astype(str).unique()\n",
    "\n",
    "# get user ids of each row in pandas.DataFrame\n",
    "def get_uids(df):\n",
    "    return df['uid_index'].astype(str).values\n",
    "\n",
    "# get list of all unique user ids\n",
    "def get_unique_iids(df):\n",
    "    return df['iid'].astype(str).unique()\n",
    "\n",
    "# get item ids of each row in pandas.DataFrame\n",
    "def get_iids(df):\n",
    "    return df['iid'].astype(str).values\n",
    "\n",
    "# get list of all unique user features\n",
    "def get_unique_user_features(df):\n",
    "    sub_df = df[USER_FEATURES].astype(str)\n",
    "    return np.unique(sub_df.values.flatten())\n",
    "\n",
    "# get user features of each row in pandas.DataFrame\n",
    "def get_user_features(df):\n",
    "    sub_df = df[USER_FEATURES].astype(str)\n",
    "    return sub_df.values\n",
    "\n",
    "# get list of all unique item features\n",
    "def get_unique_item_features(df):\n",
    "    sub_df = df[ITEM_FEATURES].astype(str)\n",
    "    return np.unique(sub_df.values.flatten())\n",
    "\n",
    "# get item features of each row in pandas.DataFrame\n",
    "def get_item_features(df):\n",
    "    sub_df = df[ITEM_FEATURES].astype(str)\n",
    "    return sub_df.values\n",
    "\n",
    "# get list of tuples of user-item interactions\n",
    "def get_iteractions(df, rating=False):\n",
    "    \n",
    "    if rating:\n",
    "        columns = ['uid_index', 'iid', 'rrate']\n",
    "    else:\n",
    "        columns = ['uid_index', 'iid']\n",
    "        \n",
    "    df[['uid_index', 'iid']] = df[['uid_index', 'iid']].astype(str)\n",
    "        \n",
    "    rows = df[columns].values\n",
    "    return (tuple(r) for r in rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique users: 16256\n",
      "# unique items: 693\n",
      "# unique user features: 3133\n",
      "# unique item features: 628\n"
     ]
    }
   ],
   "source": [
    "print(\"# unique users:\", len(get_unique_uids(train_df)))\n",
    "print(\"# unique items:\", len(get_unique_iids(train_df)))\n",
    "print(\"# unique user features:\", len(get_unique_user_features(train_df)))\n",
    "print(\"# unique item features:\", len(get_unique_item_features(train_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tell dataset what dimension of latent/user/item matrix to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.fit(\n",
    "    get_unique_uids(train_df),\n",
    "    get_unique_iids(train_df),\n",
    "    user_features = get_unique_user_features(train_df),\n",
    "    item_features = get_unique_item_features(train_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction matrix shape -- Num users: 16256 x num_items 693.\n"
     ]
    }
   ],
   "source": [
    "num_users, num_items = dataset.interactions_shape()\n",
    "print('Interaction matrix shape -- Num users: {} x num_items {}.'.format(num_users, num_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build interaction matrix, which is the matrix storing user ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(interactions, weights) = dataset.build_interactions(get_iteractions(train_df, rating=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<16256x693 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 90209 stored elements in COOrdinate format>\n",
      "<16256x693 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 90209 stored elements in COOrdinate format>\n"
     ]
    }
   ],
   "source": [
    "print(repr(interactions))\n",
    "print(repr(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build user feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<16256x19389 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 76744 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "uid_ufeats = ((uid, list(ufeats)) for uid, ufeats in zip(get_uids(train_df), get_user_features(train_df)) )\n",
    "\n",
    "user_features = dataset.build_user_features(uid_ufeats)\n",
    "print(repr(user_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build item feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<693x1321 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 2079 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "iid_ifeats = ((iid, list(ifeats)) for iid, ifeats in zip(get_iids(train_df), get_item_features(train_df)) )\n",
    "\n",
    "item_features = dataset.build_item_features(iid_ifeats)\n",
    "print(repr(item_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model declaratoin & fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to tune model parameters, please check [lightfm.LightFM()](https://making.lyst.com/lightfm/docs/lightfm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters  \n",
    "* no_components – the dimensionality of the feature latent embeddings. Defaults to `10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consts for model & training\n",
    "\n",
    "LOSS = 'bpr'\n",
    "LATENT=10\n",
    "\n",
    "EPOCH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x23ab7880948>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LightFM(no_components=latent_n, loss=LOSS)\n",
    "model.fit(interactions, item_features=item_features, user_features=user_features, epochs=EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataframe):\n",
    "    df = dataframe[['uid_index', 'iid', 'rrate']]\n",
    "    z = []\n",
    "    y = []\n",
    "    for i in range(len(df)):\n",
    "        user = df.iloc[i][0]\n",
    "        item = df.iloc[i][1]\n",
    "        rating = df.iloc[i][2]\n",
    "        prediction = model.predict([user], [item])\n",
    "        \n",
    "        z.append(prediction[0])\n",
    "        y.append(rating)\n",
    "    return (np.array(z, dtype=np.float32), np.array(y, dtype=np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid_index</th>\n",
       "      <th>iid</th>\n",
       "      <th>rrate</th>\n",
       "      <th>ugender</th>\n",
       "      <th>ucity</th>\n",
       "      <th>ucountry</th>\n",
       "      <th>ulevel</th>\n",
       "      <th>irating</th>\n",
       "      <th>itag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37006</th>\n",
       "      <td>7427</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Level 2 Contributor</td>\n",
       "      <td>4.5</td>\n",
       "      <td>['blue whale', 't rex', 'dinosaur section', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39685</th>\n",
       "      <td>15108</td>\n",
       "      <td>922</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Level 4 Contributor</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['bethnal green', 'rocking horses', 'sand pit'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51917</th>\n",
       "      <td>11087</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Level 4 Contributor</td>\n",
       "      <td>4.5</td>\n",
       "      <td>['guided tour', 'westminster hall', 'local mp'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81301</th>\n",
       "      <td>6145</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jerusalem</td>\n",
       "      <td>Israel</td>\n",
       "      <td>Level 3 Contributor</td>\n",
       "      <td>4.5</td>\n",
       "      <td>['blue whale', 't rex', 'dinosaur section', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67994</th>\n",
       "      <td>4566</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>Allen</td>\n",
       "      <td>United States</td>\n",
       "      <td>Level 6 Contributor</td>\n",
       "      <td>4.5</td>\n",
       "      <td>['crown jewels', 'beefeater tour', 'yeoman war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24752</th>\n",
       "      <td>9826</td>\n",
       "      <td>997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female</td>\n",
       "      <td>London</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Level 6 Contributor</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['chamber music', 'beautiful hall', 'piano rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94543</th>\n",
       "      <td>9820</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>London</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Level 5 Contributor</td>\n",
       "      <td>4.5</td>\n",
       "      <td>['alexander mcqueen', 'decorative arts', 'wedd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64094</th>\n",
       "      <td>3922</td>\n",
       "      <td>308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>England</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Level 5 Contributor</td>\n",
       "      <td>4.5</td>\n",
       "      <td>['maritime museum', 'royal observatory', 'cutt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43981</th>\n",
       "      <td>5072</td>\n",
       "      <td>934</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Level 6 Contributor</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['main arena', 'atp tennis', 'michael mcintyre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46822</th>\n",
       "      <td>7684</td>\n",
       "      <td>635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Level 6 Contributor</td>\n",
       "      <td>4.5</td>\n",
       "      <td>['press room', 'chelsea fan', 'chelsea fc', 'm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90209 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      uid_index  iid  rrate ugender      ucity        ucountry  \\\n",
       "37006      7427   10    1.0     NaN     London  United Kingdom   \n",
       "39685     15108  922    1.0     NaN        NaN             NaN   \n",
       "51917     11087    7    1.0  female   Adelaide       Australia   \n",
       "81301      6145   10    1.0     NaN  Jerusalem          Israel   \n",
       "67994      4566    2    1.0    male      Allen   United States   \n",
       "...         ...  ...    ...     ...        ...             ...   \n",
       "24752      9826  997    0.0  female     London  United Kingdom   \n",
       "94543      9820    3    1.0    male     London  United Kingdom   \n",
       "64094      3922  308    1.0     NaN    England  United Kingdom   \n",
       "43981      5072  934    1.0    male  Hong Kong       Hong Kong   \n",
       "46822      7684  635    0.0  female   Adelaide       Australia   \n",
       "\n",
       "                    ulevel  irating  \\\n",
       "37006  Level 2 Contributor      4.5   \n",
       "39685  Level 4 Contributor      4.0   \n",
       "51917  Level 4 Contributor      4.5   \n",
       "81301  Level 3 Contributor      4.5   \n",
       "67994  Level 6 Contributor      4.5   \n",
       "...                    ...      ...   \n",
       "24752  Level 6 Contributor      5.0   \n",
       "94543  Level 5 Contributor      4.5   \n",
       "64094  Level 5 Contributor      4.5   \n",
       "43981  Level 6 Contributor      4.0   \n",
       "46822  Level 6 Contributor      4.5   \n",
       "\n",
       "                                                    itag  \n",
       "37006  ['blue whale', 't rex', 'dinosaur section', 'd...  \n",
       "39685  ['bethnal green', 'rocking horses', 'sand pit'...  \n",
       "51917  ['guided tour', 'westminster hall', 'local mp'...  \n",
       "81301  ['blue whale', 't rex', 'dinosaur section', 'd...  \n",
       "67994  ['crown jewels', 'beefeater tour', 'yeoman war...  \n",
       "...                                                  ...  \n",
       "24752  ['chamber music', 'beautiful hall', 'piano rec...  \n",
       "94543  ['alexander mcqueen', 'decorative arts', 'wedd...  \n",
       "64094  ['maritime museum', 'royal observatory', 'cutt...  \n",
       "43981  ['main arena', 'atp tennis', 'michael mcintyre...  \n",
       "46822  ['press room', 'chelsea fan', 'chelsea fc', 'm...  \n",
       "\n",
       "[90209 rows x 9 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_z, validation_y = predict(model, validation_df)\n",
    "validation_z = (validation_z - np.min(validation_z)) / np.ptp(validation_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_z, test_y = predict(model, test_df)\n",
    "test_z = (test_z - np.min(test_z)) / np.ptp(test_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_auc(z, y):\n",
    "    return metrics.roc_auc_score(y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation AUC:  0.5273097241269183\n",
      "test AUC:  0.5394496695632054\n"
     ]
    }
   ],
   "source": [
    "print(\"validation AUC: \", evaluate_auc(validation_z, validation_y))\n",
    "print(\"test AUC: \", evaluate_auc(test_z, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogLoss metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume parameters z & y are ndarray\n",
    "def evaluate_logloss(z, y):\n",
    "    zz = np.ones((z.shape[0], 2))\n",
    "    zz[:, 0] -= z\n",
    "    zz[:, 1] = z\n",
    "    return metrics.log_loss(y, zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation LogLoss:  0.5536838607543384\n",
      "test LogLoss:  0.5712389110270031\n"
     ]
    }
   ],
   "source": [
    "print(\"validation LogLoss: \", evaluate_logloss(validation_z, validation_y))\n",
    "print(\"test LogLoss: \", evaluate_logloss(test_z, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDCG metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume parameters z & y are ndarray\n",
    "def evaluate_ndcg(z, y):\n",
    "    return metrics.ndcg_score(np.expand_dims(y, axis=0), np.expand_dims(z, axis=0), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation NDCG@5:  0.7227265726449517\n",
      "test NDCG@5:  0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"validation NDCG@5: \", evaluate_ndcg(validation_z, validation_y))\n",
    "print(\"test NDCG@5: \", evaluate_ndcg(test_z, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consts for model & training\n",
    "\n",
    "LOSS = 'bpr'\n",
    "LATENT=10\n",
    "\n",
    "EPOCH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_latents(epoch=EPOCH, start=1, end=10, step=1):\n",
    "\n",
    "    history = []\n",
    "    for latent_n in range(start, end+step, step):\n",
    "        \n",
    "        print(\"Using latent size:\", latent_n)\n",
    "        model = LightFM(no_components=latent_n, loss=LOSS)\n",
    "        model.fit(interactions, item_features=item_features, user_features=user_features, epochs=epoch)\n",
    "        \n",
    "        validation_z, validation_y = predict(model, validation_df)\n",
    "        validation_z = (validation_z - np.min(validation_z)) / np.ptp(validation_z)\n",
    "        test_z, test_y = predict(model, test_df)\n",
    "        test_z = (test_z - np.min(test_z)) / np.ptp(test_z)\n",
    "        \n",
    "        history.append({\n",
    "            'epoch': epoch,\n",
    "            'latent': latent_n,\n",
    "            'val_auc': evaluate_auc(validation_z, validation_y),\n",
    "            'test_auc': evaluate_auc(test_z, test_y),\n",
    "            'val_logloss': evaluate_logloss(validation_z, validation_y),\n",
    "            'test_logloss': evaluate_logloss(test_z, test_y),\n",
    "            'val_ndcg': evaluate_ndcg(validation_z, validation_y),\n",
    "            'test_ndcg': evaluate_ndcg(test_z, test_y)\n",
    "        })\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using latent size: 1\n",
      "Using latent size: 2\n",
      "Using latent size: 3\n",
      "Using latent size: 4\n",
      "Using latent size: 5\n",
      "Using latent size: 6\n",
      "Using latent size: 7\n",
      "Using latent size: 8\n",
      "Using latent size: 9\n",
      "Using latent size: 10\n"
     ]
    }
   ],
   "source": [
    "history = train_latents(epoch=EPOCH, start=1, end=10, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| settings | validation AUC | validation LogLoss | validation NDCG@5 | testing AUC | testing LogLoss | testing NDCG@5 |\n",
      "|:-- | -- | -- | -- | -- | -- | -- |\n",
      "| latent_n=1 | 0.47406 | 0.51291 | 1.00000 | 0.48215 | 0.50910 | 1.00000 |\n",
      "| latent_n=2 | 0.46631 | 0.51991 | 1.00000 | 0.45406 | 0.53953 | 1.00000 |\n",
      "| latent_n=3 | 0.50847 | 0.53254 | 1.00000 | 0.52686 | 0.54300 | 1.00000 |\n",
      "| latent_n=4 | 0.43753 | 0.53086 | 1.00000 | 0.43093 | 0.56579 | 1.00000 |\n",
      "| latent_n=5 | 0.50629 | 0.54420 | 1.00000 | 0.52933 | 0.55445 | 1.00000 |\n",
      "| latent_n=6 | 0.48671 | 0.54022 | 1.00000 | 0.49444 | 0.55589 | 1.00000 |\n",
      "| latent_n=7 | 0.53060 | 0.53454 | 1.00000 | 0.55349 | 0.55589 | 1.00000 |\n",
      "| latent_n=8 | 0.51346 | 0.54491 | 1.00000 | 0.53972 | 0.56674 | 1.00000 |\n",
      "| latent_n=9 | 0.49840 | 0.54490 | 1.00000 | 0.51439 | 0.56831 | 1.00000 |\n",
      "| latent_n=10 | 0.50420 | 0.54695 | 1.00000 | 0.51831 | 0.57180 | 1.00000 |\n"
     ]
    }
   ],
   "source": [
    "print(\"| settings | validation AUC | validation LogLoss | validation NDCG@5 | testing AUC | testing LogLoss | testing NDCG@5 |\")\n",
    "print(\"|:-- | -- | -- | -- | -- | -- | -- |\")\n",
    "for his in history:\n",
    "    print(\"| latent_n={} | {:.5f} | {:.5f} | {:.5f} | {:.5f} | {:.5f} | {:.5f} |\".format(\n",
    "        his['latent'],\n",
    "        his['val_auc'],\n",
    "        his['val_logloss'],\n",
    "        his['val_ndcg'],\n",
    "        his['test_auc'],\n",
    "        his['test_logloss'],\n",
    "        his['test_ndcg'],\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LON-A dataset:  \n",
    "\n",
    "| settings | validation AUC | validation LogLoss | validation NDCG@5 | testing AUC | testing LogLoss | testing NDCG@5 |\n",
    "|:-- | -- | -- | -- | -- | -- | -- |\n",
    "| **latent_n=1*** | 0.50749 | 2.67413 | 0.66084 | 0.49727 | 2.15446 | 0.66084 |\n",
    "| latent_n=2 | 0.50484 | 2.79746 | 0.66084 | 0.49862 | 2.95496 | 0.44685 |\n",
    "| latent_n=3 | 0.50840 | 3.45294 | 0.66084 | 0.50411 | 3.46178 | 0.27727 |\n",
    "| latent_n=4 | 0.49764 | 3.70850 | 0.66084 | 0.49985 | 3.64026 | 0.27727 |\n",
    "| latent_n=5 | 0.48261 | 3.67404 | 0.66084 | 0.48091 | 3.81011 | 0.27727 |\n",
    "| latent_n=6 | 0.50135 | 4.22715 | 0.66084 | 0.49615 | 4.19156 | 0.27727 |\n",
    "| latent_n=7 | 0.50788 | 4.42946 | 0.66084 | 0.50081 | 4.46515 | 0.27727 |\n",
    "| latent_n=8 | 0.49772 | 4.40505 | 0.66084 | 0.48850 | 4.60153 | 0.27727 |\n",
    "| latent_n=9 | 0.50088 | 4.71791 | 0.66084 | 0.49938 | 4.48787 | 0.27727 |\n",
    "| latent_n=10 | 0.50002 | 4.66355 | 0.66084 | 0.49069 | 4.69170 | 0.27727 |\n",
    "\n",
    "NYC-R dataset:  \n",
    "\n",
    "| settings | validation AUC | validation LogLoss | validation NDCG@5 | testing AUC | testing LogLoss | testing NDCG@5 |\n",
    "|:-- | -- | -- | -- | -- | -- | -- |\n",
    "| latent_n=1 | 0.47406 | 0.51291 | 1.00000 | 0.48215 | 0.50910 | 1.00000 |\n",
    "| latent_n=2 | 0.46631 | 0.51991 | 1.00000 | 0.45406 | 0.53953 | 1.00000 |\n",
    "| latent_n=3 | 0.50847 | 0.53254 | 1.00000 | 0.52686 | 0.54300 | 1.00000 |\n",
    "| latent_n=4 | 0.43753 | 0.53086 | 1.00000 | 0.43093 | 0.56579 | 1.00000 |\n",
    "| latent_n=5 | 0.50629 | 0.54420 | 1.00000 | 0.52933 | 0.55445 | 1.00000 |\n",
    "| latent_n=6 | 0.48671 | 0.54022 | 1.00000 | 0.49444 | 0.55589 | 1.00000 |\n",
    "| **latent_n=7*** | 0.53060 | 0.53454 | 1.00000 | 0.55349 | 0.55589 | 1.00000 |\n",
    "| latent_n=8 | 0.51346 | 0.54491 | 1.00000 | 0.53972 | 0.56674 | 1.00000 |\n",
    "| latent_n=9 | 0.49840 | 0.54490 | 1.00000 | 0.51439 | 0.56831 | 1.00000 |\n",
    "| latent_n=10 | 0.50420 | 0.54695 | 1.00000 | 0.51831 | 0.57180 | 1.00000 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
