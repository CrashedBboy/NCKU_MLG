{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surprise Baseline Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set-up\n",
    "import dependent packages and declare consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package initialization\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import BaselineOnly\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consts\n",
    "\n",
    "# DATASET = '../../data/extracted/LON-A/London_Attractions_Complete_Review.csv'\n",
    "DATASET = '../../data/extracted/NYC-R/New_York_City_Restaurant_Complete_Review.csv'\n",
    "OCCURENCE_THRESHOLD = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read Dataset\n",
    "read dataset in csv format into pandas.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "\n",
    "df = pd.read_csv(DATASET, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: \n",
      " ['Unnamed: 0', 'Unnamed: 0.1', 'rtime', 'rquote', 'iid', 'rrate', 'rid', 'uage', 'ucity', 'ucountry', 'ugender', 'uhometown', 'uid_index', 'ulevel', 'uname_y', 'usince', 'ustate', 'ustyle', 'iattribute', 'ilocality', 'iname', 'ipopularity', 'ipost', 'iprice', 'irating', 'iregion', 'istreet', 'itag']\n",
      "\n",
      "Shape: \n",
      " (129964, 28)\n"
     ]
    }
   ],
   "source": [
    "# print dataset information\n",
    "\n",
    "print(\"Columns: \\n\", list(df.columns))\n",
    "print(\"\\nShape: \\n\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "* Retain users/items with at least five ratings only\n",
    "* Data splitting\n",
    "  - the latest 20% interactions (by time)\n",
    "  - Randomly split the remaining data into training (70%) and validation (10%) sets\n",
    "* Transform the ratings into binary implicit feedback as ground truth, indicating whether the user has interacted with the specific item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_time(df):\n",
    "    \n",
    "    # here we use 'rid' for sorting becaz it's auto incrementing\n",
    "    return df.sort_values(by=['rid'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_occurrence(df, column, threshold):\n",
    "    return df.groupby(column).filter(lambda x: len(x) >= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_binary(df):\n",
    "    df.loc[df['rrate'] != \"None\", 'rrate'] = 1.0\n",
    "    df.loc[df['rrate'] == \"None\", 'rrate'] = 0.0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(dataframe):\n",
    "    \n",
    "    # sort by time (ascending order)\n",
    "    df = sort_by_time(dataframe)\n",
    "    \n",
    "    # retrieve needed columns\n",
    "    df = df[['uid_index', 'iid', 'rrate']]\n",
    "    \n",
    "    # convert ratings into binarys\n",
    "    df = convert_binary(df)\n",
    "    \n",
    "    df['rrate'] = pd.to_numeric(df['rrate'])\n",
    "    \n",
    "    # Retain users/items with at least five ratings only\n",
    "    df = filter_by_occurrence(df, 'iid', OCCURENCE_THRESHOLD)\n",
    "    df = filter_by_occurrence(df, 'uid_index', OCCURENCE_THRESHOLD)\n",
    "    \n",
    "    # split dataset into training set, validation set and test set\n",
    "    users = df.groupby('uid_index')\n",
    "    \n",
    "    test_df = pd.DataFrame()\n",
    "    train_validation_df = pd.DataFrame()\n",
    "    \n",
    "    # for each user, get its latest 20% rating as test set\n",
    "    for uid in users.size().to_dict().keys():\n",
    "        user = users.get_group(uid)\n",
    "        split_idx = int(len(user)*0.8)\n",
    "        test_df = test_df.append(user.iloc[split_idx:])\n",
    "        train_validation_df = train_validation_df.append(user.iloc[:split_idx])\n",
    "    \n",
    "    train_validation_df = train_validation_df.reindex(np.random.permutation(train_validation_df.index)) # shuffle\n",
    "    train_df = train_validation_df.iloc[:int(len(train_validation_df)*0.875)]\n",
    "    validation_df = train_validation_df.iloc[int(len(train_validation_df)*0.875):]\n",
    "    \n",
    "    return (train_df, validation_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size:  (80722, 3)\n",
      "validation set size:  (11532, 3)\n",
      "test set size:  (29517, 3)\n"
     ]
    }
   ],
   "source": [
    "# dataset preprocessing\n",
    "\n",
    "train_df, validation_df, test_df = data_preprocess(df)\n",
    "print(\"training set size: \", train_df.shape)\n",
    "print(\"validation set size: \", validation_df.shape)\n",
    "print(\"test set size: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load into Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(0, 1))\n",
    "train_dataset = Dataset.load_from_df(train_df, reader).build_full_trainset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model declaration & Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**\n",
    "\n",
    "\\begin{equation}\n",
    "b_{ui} = (\\mu + b_u + b_i)\n",
    "\\end{equation}\n",
    "\n",
    "where $b_{ui}$ is the predicted rating by user $u$ to item $i$\n",
    "\n",
    "**Optimization objective**\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - (\\mu + b_u + b_i)\\right)^2 +\n",
    "\\lambda \\left(b_u^2 + b_i^2 \\right)\n",
    "\\end{equation}\n",
    "\n",
    "**Parameters (using SGD)**\n",
    "- `reg`: The regularization parameter of the cost function that is optimized, defaults to `0.02`\n",
    "- `learning_rate`: The learning rate of SGD, defaults to `0.005`\n",
    "- `n_epochs`: The number of iteration of the SGD procedure. Default is `20`.\n",
    "\n",
    "model configuration: [Surprise | Baseline configuration](https://surprise.readthedocs.io/en/stable/prediction_algorithms.html#baseline-estimates-configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consts\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCH = 10\n",
    "REGULARIZATION = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare model\n",
    "bsl_options = {'method': 'sgd',\n",
    "               'learning_rate': LEARNING_RATE,\n",
    "               'n_epochs': EPOCH,\n",
    "               'reg': REGULARIZATION\n",
    "               }\n",
    "algorithm = BaselineOnly(bsl_options=bsl_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using sgd...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.baseline_only.BaselineOnly at 0x27056387d88>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(algorithm, dataframe):\n",
    "    z = []\n",
    "    y = []\n",
    "    for i in range(len(dataframe)):\n",
    "        user = dataframe.iloc[i][0]\n",
    "        item = dataframe.iloc[i][1]\n",
    "        rating = dataframe.iloc[i][2]\n",
    "        prediction = algorithm.predict(user, item, r_ui=rating, verbose=False)\n",
    "        \n",
    "        if prediction.details['was_impossible'] == False:\n",
    "            z.append(prediction.est)\n",
    "            y.append(rating)\n",
    "    return (np.array(z, dtype=np.float32), np.array(y, dtype=np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_z, validation_y = predict(algorithm, validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_z, test_y = predict(algorithm, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_auc(z, y):\n",
    "    return metrics.roc_auc_score(y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation AUC:  0.9950688258953624\n",
      "test AUC:  0.9918595553439749\n"
     ]
    }
   ],
   "source": [
    "print(\"validation AUC: \", evaluate_auc(validation_z, validation_y))\n",
    "print(\"test AUC: \", evaluate_auc(test_z, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogLoss metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume parameters z & y are ndarray\n",
    "def evaluate_logloss(z, y):\n",
    "    zz = np.ones((z.shape[0], 2))\n",
    "    zz[:, 0] -= z\n",
    "    zz[:, 1] = z\n",
    "    return metrics.log_loss(y, zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation LogLoss:  0.05278888783940757\n",
      "test LogLoss:  0.07449852962532956\n"
     ]
    }
   ],
   "source": [
    "print(\"validation LogLoss: \", evaluate_logloss(validation_z, validation_y))\n",
    "print(\"test LogLoss: \", evaluate_logloss(test_z, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDCG metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume parameters z & y are ndarray\n",
    "def evaluate_ndcg(z, y):\n",
    "    return metrics.ndcg_score(np.expand_dims(y, axis=0), np.expand_dims(z, axis=0), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation NDCG@5:  0.9997153430116709\n",
      "test NDCG@5:  0.9998784933171323\n"
     ]
    }
   ],
   "source": [
    "print(\"validation NDCG@5: \", evaluate_ndcg(validation_z, validation_y))\n",
    "print(\"test NDCG@5: \", evaluate_ndcg(test_z, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consts\n",
    "LEARNING_RATE = 0.005\n",
    "EPOCH = 200\n",
    "REGULARIZATION = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regs(lr=LEARNING_RATE, epoch=EPOCH, start=0, end=0.06, step=0.02):\n",
    "\n",
    "    history = []\n",
    "    reg = start\n",
    "    while reg <= end:\n",
    "        bsl_options = {'method': 'sgd',\n",
    "                       'learning_rate': lr,\n",
    "                       'n_epochs': epoch,\n",
    "                       'reg': reg\n",
    "                       }\n",
    "        print(\"Training model using reg \", reg)\n",
    "        algorithm = BaselineOnly(bsl_options=bsl_options)\n",
    "        algorithm.fit(train_dataset)\n",
    "        validation_z, validation_y = predict(algorithm, validation_df)\n",
    "        test_z, test_y = predict(algorithm, test_df)\n",
    "        \n",
    "        history.append({\n",
    "            'epoch': epoch,\n",
    "            'reg': reg,\n",
    "            'val_auc': evaluate_auc(validation_z, validation_y),\n",
    "            'test_auc': evaluate_auc(test_z, test_y),\n",
    "            'val_logloss': evaluate_logloss(validation_z, validation_y),\n",
    "            'test_logloss': evaluate_logloss(test_z, test_y),\n",
    "            'val_ndcg': evaluate_ndcg(validation_z, validation_y),\n",
    "            'test_ndcg': evaluate_ndcg(test_z, test_y)\n",
    "        })\n",
    "        reg += step\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model using reg  0\n",
      "Estimating biases using sgd...\n",
      "Training model using reg  0.02\n",
      "Estimating biases using sgd...\n",
      "Training model using reg  0.04\n",
      "Estimating biases using sgd...\n",
      "Training model using reg  0.06\n",
      "Estimating biases using sgd...\n"
     ]
    }
   ],
   "source": [
    "history = train_regs(lr=0.005, epoch=200, start=0, end=0.06, step=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"| reg | validation AUC | validation LogLoss | validation NDCG@5 | testing AUC | testing LogLoss | testing NDCG@5 |\")\n",
    "print(\"|:-- | -- | -- | -- | -- | -- | -- |\")\n",
    "for his in history:\n",
    "    print(\"| {} | {:.5f} | {:.5f} | {:.5f} | {:.5f} | {:.5f} | {:.5f} |\".format(\n",
    "        his['reg'],\n",
    "        his['val_auc'],\n",
    "        his['val_logloss'],\n",
    "        his['val_ndcg'],\n",
    "        his['test_auc'],\n",
    "        his['test_logloss'],\n",
    "        his['test_ndcg'],\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(lr=LEARNING_RATE, reg=REGULARIZATION, start=50, end=200, step=50):\n",
    "    \n",
    "    history = []\n",
    "    for epoch in range(start, end+step, step):\n",
    "        \n",
    "        bsl_options = {'method': 'sgd',\n",
    "                       'learning_rate': lr,\n",
    "                       'n_epochs': epoch,\n",
    "                       'reg': reg\n",
    "                       }\n",
    "        print(\"Training model using epoch \", epoch)\n",
    "        algorithm = BaselineOnly(bsl_options=bsl_options)\n",
    "        algorithm.fit(train_dataset)\n",
    "        validation_z, validation_y = predict(algorithm, validation_df)\n",
    "        test_z, test_y = predict(algorithm, test_df)\n",
    "        \n",
    "        history.append({\n",
    "            'epoch': epoch,\n",
    "            'val_auc': evaluate_auc(validation_z, validation_y),\n",
    "            'test_auc': evaluate_auc(test_z, test_y),\n",
    "            'val_logloss': evaluate_logloss(validation_z, validation_y),\n",
    "            'test_logloss': evaluate_logloss(test_z, test_y),\n",
    "            'val_ndcg': evaluate_ndcg(validation_z, validation_y),\n",
    "            'test_ndcg': evaluate_ndcg(test_z, test_y)\n",
    "        })\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model using epoch  50\n",
      "Estimating biases using sgd...\n",
      "Training model using epoch  100\n",
      "Estimating biases using sgd...\n",
      "Training model using epoch  150\n",
      "Estimating biases using sgd...\n",
      "Training model using epoch  200\n",
      "Estimating biases using sgd...\n"
     ]
    }
   ],
   "source": [
    "history = train_epochs(lr=LEARNING_RATE, reg=REGULARIZATION, start=50, end=200, step=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"| epochs | validation AUC | validation LogLoss | validation NDCG@5 | testing AUC | testing LogLoss | testing NDCG@5 |\")\n",
    "print(\"|:-- | -- | -- | -- | -- | -- | -- |\")\n",
    "for his in history:\n",
    "    print(\"| {} | {:.5f} | {:.5f} | {:.5f} | {:.5f} | {:.5f} | {:.5f} |\".format(\n",
    "        his['epoch'],\n",
    "        his['val_auc'],\n",
    "        his['val_logloss'],\n",
    "        his['val_ndcg'],\n",
    "        his['test_auc'],\n",
    "        his['test_logloss'],\n",
    "        his['test_ndcg'],\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Results\n",
    "\n",
    "LON-A dataset:  \n",
    "\n",
    "| settings | validation AUC | validation LogLoss | validation NDCG@5 | testing AUC | testing LogLoss | testing NDCG@5 |\n",
    "|:-- | -- | -- | -- | -- | -- | -- |\n",
    "| lr=5e-3, epoch=100, reg=0 | 0.99829 | 0.04770 | 0.99973 | 0.99727 | 0.06989 | 0.99989 |\n",
    "| lr=5e-3, epoch=150, reg=0 | 0.99837 | 0.04543 | 0.99974 | 0.99741 | 0.06559 | 0.99989 |\n",
    "| **lr=5e-3, epoch=200, reg=0** | 0.99837 | 0.04655 | 0.99975 | 0.99748 | 0.06342 | 0.99990 |\n",
    "| lr=5e-3, epoch=200, reg=0.02 | 0.99787 | 0.05517 | 0.99964 | 0.99714 | 0.07020 | 0.99986 |\n",
    "| lr=5e-3, epoch=200, reg=0.04 | 0.99784 | 0.06068 | 0.99952 | 0.99711 | 0.07267 | 0.99982 |\n",
    "| lr=5e-3, epoch=200, reg=0.06 | 0.99781 | 0.06643 | 0.99939 | 0.99708 | 0.07520 | 0.99977 |\n",
    "\n",
    "NYC-R dataset:  \n",
    "\n",
    "| settings | validation AUC | validation LogLoss | validation NDCG@5 | testing AUC | testing LogLoss | testing NDCG@5 |\n",
    "|:-- | -- | -- | -- | -- | -- | -- |\n",
    "| lr=5e-3, epoch=100, reg=0 | 0.99868 | 0.13270 | 1.00000 | 0.99873 | 0.14736 | 0.99939 |\n",
    "| lr=5e-3, epoch=150, reg=0 | 0.99874 | 0.10398 | 0.99950 | 0.99893 | 0.11389 | 0.99958 |\n",
    "| **lr=5e-3, epoch=200, reg=0** | 0.99880 | 0.08598 | 0.99959 | 0.99901 | 0.09440 | 0.99966 |\n",
    "| lr=5e-3, epoch=200, reg=0.02 | 0.99958 | 0.02471 | 1.00000 | 0.99924 | 0.02754 | 0.99936 |\n",
    "| lr=5e-3, epoch=200, reg=0.04 | 0.99955 | 0.03267 | 1.00000 | 0.99922 | 0.03440 | 0.99942 |\n",
    "| lr=5e-3, epoch=200, reg=0.06 | 0.99952 | 0.04055 | 1.00000 | 0.99918 | 0.04218 | 0.99929 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
