{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Set-up\n",
    "import dependent packages and declare consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# evaluation metrics & dataset processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, ndcg_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# DeepCTR\n",
    "from deepctr_torch.inputs import SparseFeat, VarLenSparseFeat, get_feature_names, combined_dnn_input\n",
    "from deepctr_torch.models import DeepFM, CCPM, WDL, DCN, NFM\n",
    "from deepctr_torch.models.basemodel import BaseModel\n",
    "from deepctr_torch.layers import DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup computing device for pytorch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consts\n",
    "DATASET1 = '../../data/extracted/LON-A/London_Attractions_Complete_Review.csv'\n",
    "DATASET2 = '../../data/extracted/NYC-R/New_York_City_Restaurant_Complete_Review.csv'\n",
    "\n",
    "user_columns = ['uage', 'ugender', 'ucity', 'ucountry', 'uid_index', 'ulevel', 'ustyle']\n",
    "LON_item_columns = ['iid', 'iattribute', 'irating', 'itag']\n",
    "NYC_item_columns = ['iid', 'iattribute', 'iprice', 'irating', 'itag']\n",
    "rating_columns = ['rrate', 'rid']\n",
    "\n",
    "LON_sparse_features = [\"uage\", \"ugender\", \"ucity\", \"ucountry\", \"uid_index\", \"ulevel\", 'iid', 'irating']\n",
    "NYC_sparse_features = [\"uage\", \"ugender\", \"ucity\", \"ucountry\", \"uid_index\", \"ulevel\", 'iid', 'irating', 'iprice']\n",
    "var_sparse_features = ['ustyle', 'iattribute', 'itag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_time(df):\n",
    "    return df.sort_values(by=['rid'], ascending=True)\n",
    "\n",
    "def filter_by_occurrence(df, column, threshold):\n",
    "    return df.groupby(column).filter(lambda x: len(x) >= threshold)\n",
    "\n",
    "def add_var_column(df, column):\n",
    "    key2index = {}\n",
    "    \n",
    "    def split(x):\n",
    "        key_ans = x.split(',')\n",
    "        for key in key_ans:\n",
    "            if key not in key2index:\n",
    "                # Notice : input value 0 is a special \"padding\",so we do not use 0 to encode valid feature for sequence input\n",
    "                key2index[key] = len(key2index) + 1\n",
    "        return list(map(lambda x: key2index[x], key_ans))\n",
    "    \n",
    "    #remove unnecessary characters\n",
    "    df[column] = df[column].str.replace('[', '').str.replace(']', '').str.replace(', ', ',')\n",
    "    column_list = list(map(split, df[column].values))\n",
    "    column_length = np.array(list(map(len, column_list)))\n",
    "    column_maxlen = max(column_length)\n",
    "    column_list = pad_sequences(column_list, maxlen=column_maxlen, padding='post', )\n",
    "    df = pd.concat([df, pd.DataFrame(column_list).add_prefix(str(column))], axis=1)\n",
    "    \n",
    "    return df, key2index, column_maxlen\n",
    "\n",
    "def split_df(df):\n",
    "    df['rating_cumcounts'] = df.groupby(['uid_index'])['rid'].rank(method='first', ascending=True)\n",
    "    tmp = df.groupby('uid_index').size().rename('total_counts')\n",
    "    df = df.join(tmp, on='uid_index', rsuffix='_r')\n",
    "    train_df = df.loc[df['rating_cumcounts'] < (df['total_counts']*0.8)]\n",
    "    test_df = df.loc[df['rating_cumcounts'] >= (df['total_counts']*0.8)]\n",
    "    train_df, validation_df = train_test_split(train_df, test_size=0.1, random_state=1)\n",
    "    \n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "def preprocessing(df):\n",
    "    df = sort_by_time(df)\n",
    "    df = filter_by_occurrence(df, 'uid_index', 5)\n",
    "    df = filter_by_occurrence(df, 'iid', 5)\n",
    "    df['rrate'] = df['rrate'].apply(lambda x: 1 if x != 'None' else 0)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def get_data(DATASET = 'LON'):\n",
    "    assert DATASET in ['LON', 'NYC']\n",
    "    \n",
    "    if DATASET == 'LON':\n",
    "        df = pd.read_csv(DATASET1, sep='\\t')[user_columns + LON_item_columns + rating_columns].fillna('NaN')\n",
    "        sparse_features = LON_sparse_features\n",
    "    else:\n",
    "        df = pd.read_csv(DATASET2, sep='\\t')[user_columns + NYC_item_columns + rating_columns].fillna('NaN')\n",
    "        sparse_features = NYC_sparse_features\n",
    "    \n",
    "    # sort, filter, binarize\n",
    "    df = preprocessing(df)\n",
    "    \n",
    "    #Label encode categorical features\n",
    "    for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        df[feat] = lbe.fit_transform(df[feat].astype('str'))\n",
    "\n",
    "    # Adding variable length categorical columns\n",
    "    column_dict_list, column_maxlen_list = [], []\n",
    "    for column in var_sparse_features:\n",
    "        df, column_dict, column_maxlen = add_var_column(df, column)\n",
    "        column_dict_list.append(column_dict)\n",
    "        column_maxlen_list.append(column_maxlen)\n",
    "        \n",
    "        \n",
    "    fixlen_feature_columns = [SparseFeat(feat, df[feat].nunique(), embedding_dim=4)\n",
    "                              for feat in LON_sparse_features]\n",
    "    # Notice : value 0 is for padding for sequence input feature\n",
    "    varlen_feature_columns = [VarLenSparseFeat(SparseFeat(feat, vocabulary_size=len(column_dict_list[i]) + 1,\n",
    "                                  embedding_dim=4), maxlen=column_maxlen_list[i], combiner='mean',) \n",
    "                                  for i, feat in enumerate(var_sparse_features)]  \n",
    "    linear_feature_columns = fixlen_feature_columns + varlen_feature_columns\n",
    "    dnn_feature_columns = fixlen_feature_columns + varlen_feature_columns\n",
    "    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "    \n",
    "    train_df, val_df, test_df = split_df(df)\n",
    "    \n",
    "    # generate input data for model\n",
    "    train_model_input = {name: train_df[name] for name in sparse_features} \n",
    "    for i, feat in enumerate(var_sparse_features):\n",
    "        train_model_input[feat] = train_df.filter(regex='^'+feat+'.+',axis=1).values\n",
    "    \n",
    "    val_model_input = {name: val_df[name] for name in sparse_features} \n",
    "    for i, feat in enumerate(var_sparse_features):\n",
    "        val_model_input[feat] = val_df.filter(regex='^'+feat+'.+',axis=1).values\n",
    "    \n",
    "    test_model_input = {name: test_df[name] for name in sparse_features}\n",
    "    for i, feat in enumerate(var_sparse_features):\n",
    "        test_model_input[feat] = test_df.filter(regex='^'+feat+'.+',axis=1).values\n",
    "    \n",
    "    train_y, val_y, test_y = train_df['rrate'].values, val_df['rrate'].values, test_df['rrate'].values\n",
    "        \n",
    "    return train_model_input, train_y, val_model_input, val_y, test_model_input, test_y, linear_feature_columns, dnn_feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "* Variable length columns * 3\n",
    "  - ustyle, iattribute, itag\n",
    "* Sparse columns * 8 or 9\n",
    "  - uage, ugender, ucity, ucountry, uid_index, ulevel, iid, irating, iprice(NYC DATASET2)\n",
    "\n",
    "### DeepCTR using dictionary as model input format\n",
    "* there three types of data format in DeepCTR\n",
    "  - SparseFeat : for simple categorical data\n",
    "  - VarLenSparseFeat : for variable length categorical data (e.g. ustyle)\n",
    "  - DenseFeat : for numerical data\n",
    "* SparseFeat & VarLenSparseFeat will go through embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((87440,), (9716,), (39339,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data('LON')\n",
    "\n",
    "train_model_input, train_y, val_model_input, val_y, test_model_input, test_y = data[:6]\n",
    "linear_feature_columns, dnn_feature_columns = data[6:8]\n",
    "\n",
    "train_y.shape, val_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepCTR model detail parameters:\n",
    "https://github.com/shenweichen/DeepCTR-Torch/tree/master/deepctr_torch/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(BaseModel):\n",
    "    def __init__(self,linear_feature_columns, dnn_feature_columns,\n",
    "                 dnn_hidden_units=(128, 128), l2_reg_linear=0.00001,\n",
    "                 l2_reg_embedding=0.00001, l2_reg_dnn=0, init_std=0.0001, seed=1024, dnn_dropout=0,\n",
    "                 dnn_activation='relu', dnn_use_bn=False, task='binary', device='cpu'):\n",
    "\n",
    "        super(FFN, self).__init__(linear_feature_columns=linear_feature_columns,\n",
    "                                  dnn_feature_columns=dnn_feature_columns,\n",
    "                                  dnn_hidden_units=dnn_hidden_units,\n",
    "                                  l2_reg_embedding=l2_reg_embedding, l2_reg_dnn=l2_reg_dnn, init_std=init_std,\n",
    "                                  seed=seed,dnn_dropout=dnn_dropout, dnn_activation=dnn_activation,\n",
    "                                  task=task, device=device)\n",
    "        self.dnn_hidden_units = dnn_hidden_units\n",
    "        self.dnn = DNN(self.compute_input_dim(dnn_feature_columns), dnn_hidden_units,\n",
    "                       activation=dnn_activation, use_bn=dnn_use_bn, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout,\n",
    "                       init_std=init_std, device=device)\n",
    "\n",
    "        dnn_linear_in_feature = dnn_hidden_units[-1]\n",
    "\n",
    "        self.dnn_linear = nn.Linear(dnn_linear_in_feature, 1, bias=False).to(\n",
    "            device)\n",
    "        self.add_regularization_loss(\n",
    "            filter(lambda x: 'weight' in x[0] and 'bn' not in x[0], self.dnn.named_parameters()), l2_reg_dnn)\n",
    "        self.add_regularization_loss(self.dnn_linear.weight, l2_reg_linear)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, X):\n",
    "        logit = self.linear_model(X)\n",
    "        sparse_embedding_list, dense_value_list = self.input_from_feature_columns(X, self.dnn_feature_columns,\n",
    "                                                                                  self.embedding_dict)\n",
    "        dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n",
    "\n",
    "        deep_out = self.dnn(dnn_input)\n",
    "        logit += self.dnn_linear(deep_out)\n",
    "    \n",
    "        y_pred = self.out(logit)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Train on 87440 samples, validate on 9716 samples, 171 steps per epoch\n",
      "Epoch 1/3\n",
      "9s - loss:  0.3145 - logloss:  0.3142 - val_logloss:  0.0897\n",
      "Epoch 2/3\n",
      "8s - loss:  0.0476 - logloss:  0.0476 - val_logloss:  0.0376\n",
      "Epoch 3/3\n",
      "8s - loss:  0.0235 - logloss:  0.0235 - val_logloss:  0.0393\n",
      "Testing AUC scores:  0.9970451299644998\n",
      "Testing los_loss scores:  0.05842061716549185\n"
     ]
    }
   ],
   "source": [
    "model = FFN(linear_feature_columns, dnn_feature_columns, task='binary', device=device)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['logloss'], )\n",
    "history = model.fit(train_model_input, train_y, batch_size=512, epochs=3, verbose=2,\n",
    "                    validation_data=(val_model_input, val_y), use_double=True)\n",
    "\n",
    "preds = model.predict(test_model_input)\n",
    "print('Testing AUC scores: ', roc_auc_score(test_y, preds))\n",
    "print('Testing los_loss scores: ', log_loss(test_y, preds.astype('float64')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Train on 87440 samples, validate on 9716 samples, 171 steps per epoch\n",
      "Epoch 1/3\n",
      "9s - loss:  0.3986 - logloss:  0.3982 - val_logloss:  0.0886\n",
      "Epoch 2/3\n",
      "9s - loss:  0.0340 - logloss:  0.0339 - val_logloss:  0.0358\n",
      "Epoch 3/3\n",
      "9s - loss:  0.0186 - logloss:  0.0186 - val_logloss:  0.0392\n",
      "Testing AUC scores:  0.9977941283900754\n",
      "Testing los_loss scores:  0.047862328554676946\n"
     ]
    }
   ],
   "source": [
    "model = CCPM(linear_feature_columns, dnn_feature_columns, task='binary', device=device)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['logloss'], )\n",
    "history = model.fit(train_model_input, train_y, batch_size=512, epochs=3, verbose=2,\n",
    "                    validation_data=(val_model_input, val_y), use_double=True)\n",
    "\n",
    "preds = model.predict(test_model_input)\n",
    "print('Testing AUC scores: ', roc_auc_score(test_y, preds))\n",
    "print('Testing los_loss scores: ', log_loss(test_y, preds.astype('float64')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WD (Wide & Deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Train on 87440 samples, validate on 9716 samples, 171 steps per epoch\n",
      "Epoch 1/3\n",
      "8s - loss:  0.2900 - logloss:  0.2897 - val_logloss:  0.0769\n",
      "Epoch 2/3\n",
      "8s - loss:  0.0422 - logloss:  0.0422 - val_logloss:  0.0375\n",
      "Epoch 3/3\n",
      "8s - loss:  0.0202 - logloss:  0.0202 - val_logloss:  0.0444\n",
      "Testing AUC scores:  0.9970358596415562\n",
      "Testing los_loss scores:  0.06604857508723155\n"
     ]
    }
   ],
   "source": [
    "model = WDL(linear_feature_columns, dnn_feature_columns, task='binary', device=device)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['logloss'], )\n",
    "history = model.fit(train_model_input, train_y, batch_size=512, epochs=3, verbose=2,\n",
    "                    validation_data=(val_model_input, val_y), use_double=True)\n",
    "\n",
    "preds = model.predict(test_model_input)\n",
    "print('Testing AUC scores: ', roc_auc_score(test_y, preds))\n",
    "print('Testing los_loss scores: ', log_loss(test_y, preds.astype('float64')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCN (Deep & Cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Train on 87440 samples, validate on 9716 samples, 171 steps per epoch\n",
      "Epoch 1/3\n",
      "10s - loss:  0.2972 - logloss:  0.2969 - val_logloss:  0.0551\n",
      "Epoch 2/3\n",
      "10s - loss:  0.0344 - logloss:  0.0344 - val_logloss:  0.0346\n",
      "Epoch 3/3\n",
      "9s - loss:  0.0228 - logloss:  0.0228 - val_logloss:  0.0356\n",
      "Testing AUC scores:  0.9978341807560748\n",
      "Testing los_loss scores:  0.0499696159088416\n"
     ]
    }
   ],
   "source": [
    "model = DCN(linear_feature_columns, dnn_feature_columns, task='binary', device=device)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['logloss'], )\n",
    "history = model.fit(train_model_input, train_y, batch_size=512, epochs=3, verbose=2,\n",
    "                    validation_data=(val_model_input, val_y), use_double=True)\n",
    "\n",
    "preds = model.predict(test_model_input)\n",
    "print('Testing AUC scores: ', roc_auc_score(test_y, preds))\n",
    "print('Testing los_loss scores: ', log_loss(test_y, preds.astype('float64')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NFM (Deep & Cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Train on 87440 samples, validate on 9716 samples, 171 steps per epoch\n",
      "Epoch 1/3\n",
      "8s - loss:  0.3556 - logloss:  0.3553 - val_logloss:  0.0982\n",
      "Epoch 2/3\n",
      "8s - loss:  0.0457 - logloss:  0.0457 - val_logloss:  0.0403\n",
      "Epoch 3/3\n",
      "8s - loss:  0.0225 - logloss:  0.0225 - val_logloss:  0.0399\n",
      "Testing AUC scores:  0.9970965564841615\n",
      "Testing los_loss scores:  0.05822759422259144\n"
     ]
    }
   ],
   "source": [
    "model = NFM(linear_feature_columns, dnn_feature_columns, task='binary', device=device)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['logloss'], )\n",
    "history = model.fit(train_model_input, train_y, batch_size=512, epochs=3, verbose=2,\n",
    "                    validation_data=(val_model_input, val_y), use_double=True)\n",
    "\n",
    "preds = model.predict(test_model_input)\n",
    "print('Testing AUC scores: ', roc_auc_score(test_y, preds))\n",
    "print('Testing los_loss scores: ', log_loss(test_y, preds.astype('float64')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Train on 87440 samples, validate on 9716 samples, 171 steps per epoch\n",
      "Epoch 1/3\n",
      "8s - loss:  0.2695 - logloss:  0.2692 - val_logloss:  0.0677\n",
      "Epoch 2/3\n",
      "8s - loss:  0.0398 - logloss:  0.0398 - val_logloss:  0.0371\n",
      "Epoch 3/3\n",
      "8s - loss:  0.0199 - logloss:  0.0199 - val_logloss:  0.0415\n",
      "Testing AUC scores:  0.9972663382759003\n",
      "Testing los_loss scores:  0.05990438047291399\n"
     ]
    }
   ],
   "source": [
    "model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary', device=device)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['logloss'], )\n",
    "history = model.fit(train_model_input, train_y, batch_size=512, epochs=3, verbose=2,\n",
    "                    validation_data=(val_model_input, val_y), use_double=True)\n",
    "\n",
    "preds = model.predict(test_model_input)\n",
    "print('Testing AUC scores: ', roc_auc_score(test_y, preds))\n",
    "print('Testing los_loss scores: ', log_loss(test_y, preds.astype('float64')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
