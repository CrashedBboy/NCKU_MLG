{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WD (Wide & Deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set-up\n",
    "import dependent packages and declare consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# evaluation metrics & dataset processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# DeepCTR\n",
    "from deepctr_torch.inputs import SparseFeat, VarLenSparseFeat, get_feature_names, combined_dnn_input\n",
    "from deepctr_torch.models import WDL\n",
    "from deepctr_torch.models.basemodel import BaseModel\n",
    "from deepctr_torch.layers import DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup computing device for pytorch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consts\n",
    "\n",
    "# dataset path\n",
    "DATASET1 = '../../data/extracted/LON-A/London_Attractions_Complete_Review.csv'\n",
    "DATASET2 = '../../data/extracted/NYC-R/New_York_City_Restaurant_Complete_Review.csv'\n",
    "\n",
    "OCCURENCE_THRESHOLD = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Variable length columns * 3\n",
    "  - ustyle, iattribute, itag\n",
    "* Sparse columns * 8/9\n",
    "  - uage, ugender, ucity, ucountry, uid_index, ulevel, iid, irating, iprice(NYC only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset columns: user, item, rating\n",
    "user_columns = ['uage', 'ugender', 'ucity', 'ucountry', 'uid_index', 'ulevel', 'ustyle']\n",
    "LON_item_columns = ['iid', 'iattribute', 'irating', 'itag']\n",
    "NYC_item_columns = ['iid', 'iattribute', 'iprice', 'irating', 'itag']\n",
    "rating_columns = ['rrate', 'rid']\n",
    "\n",
    "# dataset columns: sparse (one value) / variable length (multiple values)\n",
    "LON_sparse_features = [\"uage\", \"ugender\", \"ucity\", \"ucountry\", \"uid_index\", \"ulevel\", 'iid', 'irating']\n",
    "NYC_sparse_features = [\"uage\", \"ugender\", \"ucity\", \"ucountry\", \"uid_index\", \"ulevel\", 'iid', 'irating', 'iprice']\n",
    "var_sparse_features = ['ustyle', 'iattribute', 'itag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "* Retain users/items with at least five ratings only\n",
    "* Data splitting\n",
    "  - the latest 20% interactions (by time)\n",
    "  - Randomly split the remaining data into training (70%) and validation (10%) sets\n",
    "* Transform the ratings into binary implicit feedback as ground truth, indicating whether the user has interacted with the specific item\n",
    "* Transform SparseFeat(single value) into categorical data\n",
    "* Transform VarLenSparseFeat(multiple values) columns into muliple columns of categorical data\n",
    "* Then embed categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_time(df):\n",
    "    return df.sort_values(by=['rid'], ascending=True)\n",
    "\n",
    "def filter_by_occurrence(df, column, threshold):\n",
    "    return df.groupby(column).filter(lambda x: len(x) >= OCCURENCE_THRESHOLD)\n",
    "\n",
    "def convert_binary(df):\n",
    "    df.loc[df['rrate'] != \"None\", 'rrate'] = 1\n",
    "    df.loc[df['rrate'] == \"None\", 'rrate'] = 0\n",
    "    return df\n",
    "\n",
    "def add_var_column(df, column):\n",
    "    key2index = {}\n",
    "    \n",
    "    def split(x):\n",
    "        key_ans = x.split(',')\n",
    "        for key in key_ans:\n",
    "            if key not in key2index:\n",
    "                key2index[key] = len(key2index) + 1 # index starts from 1\n",
    "        return list(map(lambda x: key2index[x], key_ans))\n",
    "    \n",
    "    # remove unnecessary characters\n",
    "    df[column] = df[column].str.replace('[', '').str.replace(']', '').str.replace(', ', ',')\n",
    "    column_list = list(map(split, df[column].values))\n",
    "    column_length = np.array(list(map(len, column_list)))\n",
    "    column_maxlen = max(column_length)\n",
    "    column_list = pad_sequences(column_list, maxlen=column_maxlen, padding='post', )\n",
    "    df = pd.concat([df, pd.DataFrame(column_list).add_prefix(str(column))], axis=1)\n",
    "    \n",
    "    return df, key2index, column_maxlen\n",
    "\n",
    "def split_df(df):\n",
    "    df['rating_cumcounts'] = df.groupby(['uid_index'])['rid'].rank(method='first', ascending=True)\n",
    "    tmp = df.groupby('uid_index').size().rename('total_counts')\n",
    "    df = df.join(tmp, on='uid_index', rsuffix='_r')\n",
    "    train_df = df.loc[df['rating_cumcounts'] < (df['total_counts']*0.8)]\n",
    "    test_df = df.loc[df['rating_cumcounts'] >= (df['total_counts']*0.8)]\n",
    "    train_df, validation_df = train_test_split(train_df, test_size=0.1, random_state=1)\n",
    "    \n",
    "    return train_df, validation_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(DATASET = 'LON'):\n",
    "    \n",
    "    if DATASET == 'LON':\n",
    "        df = pd.read_csv(DATASET1, sep='\\t')[user_columns + LON_item_columns + rating_columns].fillna('NaN')\n",
    "        sparse_features = LON_sparse_features\n",
    "    else:\n",
    "        df = pd.read_csv(DATASET2, sep='\\t')[user_columns + NYC_item_columns + rating_columns].fillna('NaN')\n",
    "        sparse_features = NYC_sparse_features\n",
    "    \n",
    "    # sort by time (ascending order)\n",
    "    df = sort_by_time(df)\n",
    "    \n",
    "    # retain users/items with at least five ratings only\n",
    "    df = filter_by_occurrence(df, 'uid_index', 5)\n",
    "    df = filter_by_occurrence(df, 'iid', 5)\n",
    "    \n",
    "    # convert ratings into binarys\n",
    "    df = convert_binary(df)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # transform columns with single values into category. e.g. 'male' => 1, 'female' => 2\n",
    "    for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        df[feat] = lbe.fit_transform(df[feat].astype('str'))\n",
    "\n",
    "    # add variable length categorical columns to dataframe\n",
    "    column_dict_list, column_maxlen_list = [], []\n",
    "    for column in var_sparse_features:\n",
    "        df, column_dict, column_maxlen = add_var_column(df, column)\n",
    "        column_dict_list.append(column_dict)\n",
    "        column_maxlen_list.append(column_maxlen)\n",
    "        \n",
    "    \n",
    "    fixlen_feature_columns = [SparseFeat(feat, df[feat].nunique(), embedding_dim=4)\n",
    "                              for feat in LON_sparse_features]\n",
    "    \n",
    "    # note: vocabulary need to add 1: 0 cannot be used\n",
    "    varlen_feature_columns = [VarLenSparseFeat(SparseFeat(feat, vocabulary_size=len(column_dict_list[i]) + 1,\n",
    "                                  embedding_dim=4), maxlen=column_maxlen_list[i], combiner='mean',) \n",
    "                                  for i, feat in enumerate(var_sparse_features)]  \n",
    "    linear_feature_columns = fixlen_feature_columns + varlen_feature_columns\n",
    "    dnn_feature_columns = fixlen_feature_columns + varlen_feature_columns\n",
    "    #feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "    \n",
    "    train_df, val_df, test_df = split_df(df)\n",
    "    \n",
    "    \n",
    "    # train set\n",
    "    train_model_input = {name: train_df[name] for name in sparse_features} \n",
    "    for i, feat in enumerate(var_sparse_features):\n",
    "        train_model_input[feat] = train_df.filter(regex='^'+feat+'.+',axis=1).values\n",
    "    \n",
    "    # validation set\n",
    "    val_model_input = {name: val_df[name] for name in sparse_features} \n",
    "    for i, feat in enumerate(var_sparse_features):\n",
    "        val_model_input[feat] = val_df.filter(regex='^'+feat+'.+',axis=1).values\n",
    "    \n",
    "    # test set\n",
    "    test_model_input = {name: test_df[name] for name in sparse_features}\n",
    "    for i, feat in enumerate(var_sparse_features):\n",
    "        test_model_input[feat] = test_df.filter(regex='^'+feat+'.+',axis=1).values\n",
    "    \n",
    "    train_y, val_y, test_y = train_df['rrate'].values, val_df['rrate'].values, test_df['rrate'].values\n",
    "        \n",
    "    return train_model_input, train_y, val_model_input, val_y, test_model_input, test_y, linear_feature_columns, dnn_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79986,), (8888,), (36017,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = dataset('LON')\n",
    "data = dataset('NYC')\n",
    "\n",
    "train_model_input, train_y, val_model_input, val_y, test_model_input, test_y = data[:6]\n",
    "linear_feature_columns, dnn_feature_columns = data[6:8]\n",
    "\n",
    "train_y.shape, val_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define WD Model\n",
    "\n",
    "to tuning model parameters, check [DeepCTR-Torch](https://github.com/shenweichen/DeepCTR-Torch/tree/master/deepctr_torch/models)\n",
    "\n",
    "Parameters:\n",
    "- dnn_hidden_units - list, DNN layer architecture, defaults to (128, 128)\n",
    "- l2_reg_linear - regularization factor for linear, defaults to 0.00001\n",
    "- l2_reg_embedding - regularization factor for embedding, defaults to 0.00001\n",
    "- l2_reg_dnn - regularization factor for embedding, defaults to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WDL(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 128), task='binary', device=device)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['logloss'], )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Train on 87440 samples, validate on 9716 samples, 171 steps per epoch\n",
      "Epoch 1/3\n",
      "8s - loss:  0.2857 - logloss:  0.2854 - val_logloss:  0.0769\n",
      "Epoch 2/3\n",
      "9s - loss:  0.0424 - logloss:  0.0423 - val_logloss:  0.0389\n",
      "Epoch 3/3\n",
      "8s - loss:  0.0200 - logloss:  0.0199 - val_logloss:  0.0445\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_model_input, train_y, batch_size=512, epochs=3, verbose=2,\n",
    "                    validation_data=(val_model_input, val_y), use_double=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_auc(z, y):\n",
    "    return metrics.roc_auc_score(y, z)\n",
    "\n",
    "# assume parameters z & y are ndarray\n",
    "def evaluate_logloss(z, y):\n",
    "    zf = z.flatten()\n",
    "    zz = np.ones((zf.shape[0], 2))\n",
    "    zz[:, 0] -= zf\n",
    "    zz[:, 1] = zf\n",
    "    return metrics.log_loss(y, zz)\n",
    "\n",
    "# assume parameters z & y are ndarray\n",
    "def evaluate_ndcg(z, y):\n",
    "    return metrics.ndcg_score(np.expand_dims(y, axis=0), z.flatten().reshape((1, -1)), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing AUC:  0.9970374757753382\n",
      "Testing LogLoss:  0.06161996824701008\n",
      "Testing NDCG@5:  0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "print('Testing AUC: ', evaluate_auc(preds, test_y))\n",
    "print('Testing LogLoss: ', evaluate_logloss(preds.astype('float64'), test_y))\n",
    "print(\"Testing NDCG@5: \", evaluate_ndcg(preds, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consts\n",
    "DNN_UNITS = (256, 128)\n",
    "EPOCH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_units(unit_list):\n",
    "    \n",
    "    history = []\n",
    "    for hidden_unit in unit_list:\n",
    "        \n",
    "        print(\"Using archtect \", repr(hidden_unit))\n",
    "        model = WDL(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=hidden_unit, task='binary', device=device)\n",
    "        model.compile(\"adam\", \"binary_crossentropy\", metrics=['logloss'], )\n",
    "        model.fit(train_model_input, train_y, batch_size=512, epochs=EPOCH, verbose=2,\n",
    "                            validation_data=(val_model_input, val_y), use_double=True)\n",
    "        \n",
    "        test_preds = model.predict(test_model_input)\n",
    "        val_preds = model.predict(val_model_input)\n",
    "        \n",
    "        history.append({\n",
    "            'units': repr(hidden_unit),\n",
    "            'val_auc': evaluate_auc(val_preds, val_y),\n",
    "            'test_auc': evaluate_auc(test_preds, test_y),\n",
    "            'val_logloss': evaluate_logloss(val_preds.astype('float64'), val_y),\n",
    "            'test_logloss': evaluate_logloss(test_preds.astype('float64'), test_y),\n",
    "            'val_ndcg': evaluate_ndcg(val_preds, val_y),\n",
    "            'test_ndcg': evaluate_ndcg(test_preds, test_y)\n",
    "        })\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using archtect  (256, 128)\n",
      "cuda:0\n",
      "Train on 79986 samples, validate on 8888 samples, 157 steps per epoch\n",
      "Epoch 1/3\n",
      "8s - loss:  0.3906 - logloss:  0.3892 - val_logloss:  0.1037\n",
      "Epoch 2/3\n",
      "7s - loss:  0.0408 - logloss:  0.0406 - val_logloss:  0.0216\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8s - loss:  0.0095 - logloss:  0.0095 - val_logloss:  nan\n",
      "Using archtect  (256, 128, 64)\n",
      "cuda:0\n",
      "Train on 79986 samples, validate on 8888 samples, 157 steps per epoch\n",
      "Epoch 1/3\n",
      "8s - loss:  0.4006 - logloss:  0.3993 - val_logloss:  0.1173\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8s - loss:  0.0486 - logloss:  0.0484 - val_logloss:  nan\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8s - loss:  0.0109 - logloss:  0.0109 - val_logloss:  nan\n",
      "Using archtect  (256, 128, 64, 32)\n",
      "cuda:0\n",
      "Train on 79986 samples, validate on 8888 samples, 157 steps per epoch\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9s - loss:  0.4150 - logloss:  0.4136 - val_logloss:  nan\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8s - loss:  0.0801 - logloss:  0.0799 - val_logloss:  nan\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8s - loss:  0.0303 - logloss:  0.0303 - val_logloss:  nan\n",
      "Using archtect  (128, 64)\n",
      "cuda:0\n",
      "Train on 79986 samples, validate on 8888 samples, 157 steps per epoch\n",
      "Epoch 1/3\n",
      "7s - loss:  0.4642 - logloss:  0.4630 - val_logloss:  0.1735\n",
      "Epoch 2/3\n",
      "7s - loss:  0.0684 - logloss:  0.0682 - val_logloss:  0.0310\n",
      "Epoch 3/3\n",
      "8s - loss:  0.0153 - logloss:  0.0153 - val_logloss:  0.0168\n",
      "Using archtect  (128, 64, 32)\n",
      "cuda:0\n",
      "Train on 79986 samples, validate on 8888 samples, 157 steps per epoch\n",
      "Epoch 1/3\n",
      "8s - loss:  0.4523 - logloss:  0.4509 - val_logloss:  0.1697\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8s - loss:  0.0867 - logloss:  0.0865 - val_logloss:  nan\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8s - loss:  0.0268 - logloss:  0.0267 - val_logloss:  nan\n",
      "Using archtect  (128, 64, 32, 16)\n",
      "cuda:0\n",
      "Train on 79986 samples, validate on 8888 samples, 157 steps per epoch\n",
      "Epoch 1/3\n",
      "8s - loss:  0.4745 - logloss:  0.4730 - val_logloss:  0.1887\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8s - loss:  0.1038 - logloss:  0.1036 - val_logloss:  nan\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "E:\\ProgramData\\Anaconda3\\envs\\deepctr\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2290: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8s - loss:  0.0413 - logloss:  0.0414 - val_logloss:  nan\n"
     ]
    }
   ],
   "source": [
    "history = train_units([\n",
    "    (256, 128),\n",
    "    (256, 128, 64),\n",
    "    (256, 128, 64, 32),\n",
    "    (128, 64),\n",
    "    (128, 64, 32),\n",
    "    (128, 64, 32, 16)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| settings | validation AUC | validation LogLoss | validation NDCG@5 | testing AUC | testing LogLoss | testing NDCG@5 |\n",
      "|:-- | -- | -- | -- | -- | -- | -- |\n",
      "| DNN architect=(256, 128) | 0.99980 | 0.01379 | 1.00000 | 0.99951 | 0.01376 | 1.00000 |\n",
      "| DNN architect=(256, 128, 64) | 0.99975 | 0.01462 | 1.00000 | 0.99953 | 0.01483 | 1.00000 |\n",
      "| DNN architect=(256, 128, 64, 32) | 0.99980 | 0.02749 | 1.00000 | 0.99948 | 0.03082 | 0.99964 |\n",
      "| DNN architect=(128, 64) | 0.99969 | 0.01678 | 1.00000 | 0.99951 | 0.01713 | 1.00000 |\n",
      "| DNN architect=(128, 64, 32) | 0.99974 | 0.02485 | 1.00000 | 0.99951 | 0.02525 | 1.00000 |\n",
      "| DNN architect=(128, 64, 32, 16) | 0.99976 | 0.03460 | 1.00000 | 0.99950 | 0.03668 | 1.00000 |\n"
     ]
    }
   ],
   "source": [
    "print(\"| settings | validation AUC | validation LogLoss | validation NDCG@5 | testing AUC | testing LogLoss | testing NDCG@5 |\")\n",
    "print(\"|:-- | -- | -- | -- | -- | -- | -- |\")\n",
    "for his in history:\n",
    "    print(\"| DNN architect={} | {:.5f} | {:.5f} | {:.5f} | {:.5f} | {:.5f} | {:.5f} |\".format(\n",
    "        his['units'],\n",
    "        his['val_auc'],\n",
    "        his['val_logloss'],\n",
    "        his['val_ndcg'],\n",
    "        his['test_auc'],\n",
    "        his['test_logloss'],\n",
    "        his['test_ndcg'],\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LON-A dataset:  \n",
    "\n",
    "| settings | validation AUC | validation LogLoss | validation NDCG@5 | testing AUC | testing LogLoss | testing NDCG@5 |\n",
    "|:-- | -- | -- | -- | -- | -- | -- |\n",
    "| DNN architect=(256, 128) | 0.99825 | 0.04198 | 1.00000 | 0.99709 | 0.05845 | 1.00000 |\n",
    "| DNN architect=(256, 128, 64) | 0.99830 | 0.04765 | 1.00000 | 0.99718 | 0.05913 | 1.00000 |\n",
    "| **DNN architect=(256, 128, 64, 32)*** | 0.99833 | 0.04838 | 1.00000 | 0.99697 | 0.06067 | 1.00000 |\n",
    "| DNN architect=(128, 64) | 0.99817 | 0.03859 | 1.00000 | 0.99694 | 0.05366 | 1.00000 |\n",
    "| DNN architect=(128, 64, 32) | 0.99820 | 0.04680 | 1.00000 | 0.99671 | 0.05701 | 1.00000 |\n",
    "| DNN architect=(128, 64, 32, 16) | 0.99815 | 0.05029 | 1.00000 | 0.99673 | 0.05715 | 1.00000 |\n",
    "\n",
    "NYC-R dataset:  \n",
    "\n",
    "| settings | validation AUC | validation LogLoss | validation NDCG@5 | testing AUC | testing LogLoss | testing NDCG@5 |\n",
    "|:-- | -- | -- | -- | -- | -- | -- |\n",
    "| **DNN architect=(256, 128)*** | 0.99980 | 0.01379 | 1.00000 | 0.99951 | 0.01376 | 1.00000 |\n",
    "| DNN architect=(256, 128, 64) | 0.99975 | 0.01462 | 1.00000 | 0.99953 | 0.01483 | 1.00000 |\n",
    "| DNN architect=(256, 128, 64, 32) | 0.99980 | 0.02749 | 1.00000 | 0.99948 | 0.03082 | 0.99964 |\n",
    "| DNN architect=(128, 64) | 0.99969 | 0.01678 | 1.00000 | 0.99951 | 0.01713 | 1.00000 |\n",
    "| DNN architect=(128, 64, 32) | 0.99974 | 0.02485 | 1.00000 | 0.99951 | 0.02525 | 1.00000 |\n",
    "| DNN architect=(128, 64, 32, 16) | 0.99976 | 0.03460 | 1.00000 | 0.99950 | 0.03668 | 1.00000 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
