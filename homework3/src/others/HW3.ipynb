{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import csv\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, VarLenSparseFeat, get_feature_names\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, ndcg_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from deepctr_torch.inputs import SparseFeat, get_feature_names\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from deepctr_torch.models.pnn import PNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "London_data = 'London_Attractions_Complete_Review.csv'\n",
    "NewYork_data = 'New_York_City_Restaurant_Complete_Review.csv'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LD_feature = ['uage','ugender','ucity','uid_index','ulevel','ustyle','iid','iattribute','irating','itag','rrate','rid']\n",
    "NY_feature = ['uage','ugender','ucity','ucountry','uid_index','ulevel','ustyle','iid','iattribute','iprice','irating','itag','rrate','rid']\n",
    "att_feature = ['ustyle', 'iattribute', 'itag']\n",
    "key2index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_time(df):\n",
    "    return df.sort_values(by='rid', ascending=True)\n",
    "\n",
    "def filter_by_occurrence(df, column, threshold):\n",
    "    return df.groupby(column).filter(lambda x: len(x) >= threshold)\n",
    "\n",
    "def filtering(df, threshold) :\n",
    "    df_i = filter_by_occurrence(df, 'iid', threshold)\n",
    "    df_u = filter_by_occurrence(df_i, 'uid_index', threshold)\n",
    "    if len(df_i) == len(df_u) :\n",
    "        print('Completing deleting the number of user/item above %d' %threshold)\n",
    "        return df_u\n",
    "    else :\n",
    "        return filtering(df_u, threshold)\n",
    "\n",
    "def preprocessing(df, threshold) :\n",
    "    df = df.fillna('NaN')\n",
    "    df = filtering(df, threshold)\n",
    "    df['rrate'] = df['rrate'].apply(lambda x: 1 if x != 'None' else 0)\n",
    "    df = sort_by_time(df)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['index'] = df.index\n",
    "    return df\n",
    "\n",
    "'''\n",
    "def spliting(df, valid_rate=0.1, test_rate=0.2) :\n",
    "    test_df = pd.DataFrame()\n",
    "    left_df = pd.DataFrame()   \n",
    "    #train_df = pd.DataFrame()\n",
    "    #valid_df = pd.DataFrame()\n",
    "    uid = df.groupby('uid_index')\n",
    "    for id in uid.size().to_dict().keys():\n",
    "        user = uid.get_group(id)\n",
    "        test_df = test_df.append(user.iloc[int(len(user)*(1-test_rate)):])\n",
    "        left_df = left_df.append(user.iloc[:int(len(user)*(1-test_rate))])\n",
    "        print(id)\n",
    "    ind = (len(test_df)*valid_rate/test_rate)/len(left_df)\n",
    "    train_df, valid_df = train_test_split(left_df, test_size=ind, random_state=0)\n",
    "    test_df = shuffle(test_df, random_state=0)\n",
    "    return train_df, valid_df, test_df'''\n",
    "\n",
    "def split_df(df):\n",
    "    df['rating_cumcounts'] = df.groupby(['uid_index'])['rid'].rank(method='first', ascending=True)\n",
    "    tmp = df.groupby('uid_index').size().rename('total_counts')\n",
    "    df = df.join(tmp, on='uid_index', rsuffix='_r')\n",
    "    train_df = df.loc[df['rating_cumcounts'] < (df['total_counts']*0.8)]\n",
    "    test_df = df.loc[df['rating_cumcounts'] >= (df['total_counts']*0.8)]\n",
    "    train_df, validation_df = train_test_split(train_df, test_size=0.1, random_state=1)\n",
    "    #((len(test_df)/2)/len(train_df))\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "def split(x):\n",
    "    key_ans = x.split(',')\n",
    "    for key in key_ans:\n",
    "        if key not in key2index:\n",
    "            # Notice : input value 0 is a special \"padding\",so we do not use 0 to encode valid feature for sequence input\n",
    "            key2index[key] = len(key2index) + 1\n",
    "    return list(map(lambda x: key2index[x], key_ans))\n",
    "\n",
    "def var_feature(df, var) :\n",
    "    df[var] = df[var].str.replace('[', '').str.replace(']', '').str.replace(' ', '')\n",
    "    key2index.clear()\n",
    "    var_list = list(map(split, df[var].values))\n",
    "    var_length = np.array(list(map(len, var_list)))\n",
    "    max_len = max(var_length)\n",
    "    # Notice : padding=`post`\n",
    "    var_list = pad_sequences(var_list, maxlen = max_len, padding='post', )\n",
    "    return var_list, key2index, max_len\n",
    "\n",
    "def generate_var(df, var) :\n",
    "    var_out = {}\n",
    "    dict_len, len_list = [], []\n",
    "    for i in var :\n",
    "        var_list, key_dict, max_len = var_feature(df, i)\n",
    "        #print(key_dict)\n",
    "        var_out[i] = var_list\n",
    "        dict_len.append(len(key_dict))\n",
    "        len_list.append(max_len)\n",
    "    varlen_feature_columns = [VarLenSparseFeat(SparseFeat( feat, vocabulary_size=dict_len[i] + 1\n",
    "                                , embedding_dim=4), maxlen=len_list[i], combiner='mean') for i, feat in enumerate(var)]\n",
    "    return var_out, varlen_feature_columns\n",
    "\n",
    "def input_data(data, var, threshold) :\n",
    "    df = pd.read_csv(data, sep='\\t')[var]\n",
    "    df = preprocessing(df, threshold)\n",
    "    \n",
    "    sparse_feature = [l for l in var if l not in att_feature][:-2]\n",
    "    for feat in sparse_feature:\n",
    "        lbe = LabelEncoder()\n",
    "        df[feat] = lbe.fit_transform(df[feat].astype('str'))\n",
    "        \n",
    "    var_input, varlen_feature_columns = generate_var(df, att_feature)\n",
    "    fixlen_feature_columns = [SparseFeat(feat, df[feat].nunique(), embedding_dim=4)\n",
    "                             for feat in sparse_feature]\n",
    "    \n",
    "    linear_feature_columns = fixlen_feature_columns + varlen_feature_columns\n",
    "    dnn_feature_columns = fixlen_feature_columns + varlen_feature_columns\n",
    "    \n",
    "    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "    \n",
    "    train_df, valid_df, test_df = split_df(df)\n",
    "    \n",
    "    train_input = {name: train_df[name] for name in sparse_feature}\n",
    "    valid_input = {name: valid_df[name] for name in sparse_feature}\n",
    "    test_input = {name: test_df[name] for name in sparse_feature}\n",
    "    for feat in att_feature:\n",
    "        train_input[feat] = var_input[feat][train_df['index'].values]\n",
    "        valid_input[feat] = var_input[feat][valid_df['index'].values]\n",
    "        test_input[feat] = var_input[feat][test_df['index'].values]\n",
    "    train_output, val_output, test_output = train_df['rrate'].values, valid_df ['rrate'].values, test_df['rrate'].values\n",
    "    \n",
    "    return train_input, train_output, valid_input, val_output, test_input, test_output, linear_feature_columns, dnn_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuMF_data(df) :\n",
    "    out = pd.DataFrame({\n",
    "        \"userID\" : df['uid_index'].values,\n",
    "        \"itemID\" : df['iid'].values,\n",
    "        \"rating\" : df['rrate'].values,\n",
    "        \"timestamp\" : df['rid'].values\n",
    "    })\n",
    "    return out\n",
    "\n",
    "def split_NCF_df(df):\n",
    "    df['rating_cumcounts'] = df.groupby(['uid_index'])['rid'].rank(method='first', ascending=True)\n",
    "    tmp = df.groupby('uid_index').size().rename('total_counts')\n",
    "    df = df.join(tmp, on='uid_index', rsuffix='_r')\n",
    "    train_df = df.loc[df['rating_cumcounts'] < (df['total_counts']*0.8)]\n",
    "    test_df = df.loc[df['rating_cumcounts'] >= (df['total_counts']*0.8)]\n",
    "        \n",
    "    return train_df, test_df    \n",
    "\n",
    "def NMF_preprocess(filename, feature, threshold) :\n",
    "    df = pd.read_csv(filename, sep='\\t')[feature]\n",
    "    df = preprocessing(df, threshold)\n",
    "    train_df, test_df = split_NCF_df(df)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data for PNN with deepCTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing deleting the number of user/item above 5\n"
     ]
    }
   ],
   "source": [
    "data_CTR = input_data(London_data, LD_feature, 5)\n",
    "#data_CTR = input_data(NewYork_data, NY_feature, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y, test_x, test_y, linear_feature_columns, dnn_feature_columns =  data_CTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87282, 9699, 39282)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_y), len(val_y), len(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data for NCF and save to csv\n",
    "\n",
    "-  uid_index -> userID\n",
    "-  iid_index -> itemID\n",
    "-  rrate -> rating\n",
    "-  rid -> timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing deleting the number of user/item above 5\n",
      "Completing deleting the number of user/item above 5\n"
     ]
    }
   ],
   "source": [
    "LD_train_NCF, LD_test_NCF = NMF_preprocess(London_data, LD_feature, 5)\n",
    "NY_train_NCF, NY_test_NCF = NMF_preprocess(NewYork_data, NY_feature, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pLD_train_NCF = NeuMF_data(LD_train_NCF)\n",
    "pLD_test_NCF = NeuMF_data(LD_test_NCF)\n",
    "pNY_train_NCF = NeuMF_data(NY_train_NCF)\n",
    "pNY_test_NCF = NeuMF_data(NY_test_NCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pLD_train_NCF.to_csv('LD_train.csv', index=False)\n",
    "pLD_test_NCF.to_csv('LD_test.csv', index=False)\n",
    "pNY_train_NCF.to_csv('NY_train.csv', index=False)\n",
    "pNY_test_NCF.to_csv('NY_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepCTR PNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PNN(dnn_feature_columns, task='binary', device=device, use_inner=True, use_outter=False)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['acc'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Train on 87282 samples, validate on 9699 samples, 171 steps per epoch\n",
      "Epoch 1/3\n",
      "9s - loss:  0.3264 - acc:  0.8433 - val_acc:  0.9842\n",
      "Epoch 2/3\n",
      "10s - loss:  0.0619 - acc:  0.9882 - val_acc:  0.9889\n",
      "Epoch 3/3\n",
      "10s - loss:  0.0270 - acc:  0.9929 - val_acc:  0.9874\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_x, train_y, batch_size = 512, epochs=3, verbose=2, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_x, use_double=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing AUC scores:  0.9955130976131802\n",
      "Testing log_loss scores:  0.055174912171581825\n",
      "NDCG@5 :  0.9999102623398726\n"
     ]
    }
   ],
   "source": [
    "print('Testing AUC scores: ', roc_auc_score(test_y, preds))\n",
    "print('Testing log_loss scores: ', log_loss(test_y, preds))\n",
    "print('NDCG@5 : ', ndcg_score(np.expand_dims(test_y, axis=0), preds.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PNN(dnn_feature_columns, task='binary', device=device, use_inner=False, use_outter=True)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['acc'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Train on 87282 samples, validate on 9699 samples, 171 steps per epoch\n",
      "Epoch 1/3\n",
      "11s - loss:  0.3298 - acc:  0.8413 - val_acc:  0.9832\n",
      "Epoch 2/3\n",
      "10s - loss:  0.0722 - acc:  0.9878 - val_acc:  0.9887\n",
      "Epoch 3/3\n",
      "10s - loss:  0.0289 - acc:  0.9932 - val_acc:  0.9880\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_x, train_y, batch_size = 512, epochs=3, verbose=2, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_x, use_double=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing AUC scores:  0.9954970069757177\n",
      "Testing log_loss scores:  0.056984476439332625\n",
      "NDCG@5 :  0.9999100005974142\n"
     ]
    }
   ],
   "source": [
    "print('Testing AUC scores: ', roc_auc_score(test_y, preds))\n",
    "print('Testing log_loss scores: ', log_loss(test_y, preds))\n",
    "print('NDCG@5 : ', ndcg_score(np.expand_dims(test_y, axis=0), preds.reshape(1,-1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
