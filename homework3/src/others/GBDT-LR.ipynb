{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre‐training via GBDT for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.datasets.samples_generator import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, ndcg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_LON = '../data/LON-A/London_Attractions_Complete_Review.csv'\n",
    "DATASET_NYC = '../data/NYC-R/New_York_City_Restaurant_Complete_Review.csv'\n",
    "\n",
    "user_columns = ['uage', 'ugender', 'ucity', 'ucountry', 'uid_index', 'ulevel', 'ustyle']\n",
    "LON_item_columns = ['iid', 'iattribute', 'irating', 'itag']\n",
    "NYC_item_columns = ['iid', 'iattribute', 'iprice', 'irating', 'itag']\n",
    "rating_columns = ['rtime', 'rquote', 'rrate', 'rid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "LON_sparse_features = [\"uage\", \"ugender\", \"ucity\", \"ucountry\", \"uid_index\", \"ulevel\", 'iid', 'irating']\n",
    "NYC_sparse_features = [\"uage\", \"ugender\", \"ucity\", \"ucountry\", \"uid_index\", \"ulevel\", 'iid', 'irating', 'iprice']\n",
    "var_sparse_features = ['ustyle', 'iattribute', 'itag']\n",
    "\n",
    "def sort_by_time(df):\n",
    "    return df.sort_values(by=['rid'], ascending=True)\n",
    "\n",
    "def filter_by_occurrence(df, column, threshold):\n",
    "    return df.groupby(column).filter(lambda x: len(x) >= threshold)\n",
    "\n",
    "def split_df(df):\n",
    "    df['rating_cumcounts'] = df.groupby(['uid_index'])['rid'].rank(method='first', ascending=True)\n",
    "    tmp = df.groupby('uid_index').size().rename('total_counts')\n",
    "    df = df.join(tmp, on='uid_index', rsuffix='_r')\n",
    "    train_df = df.loc[df['rating_cumcounts'] < (df['total_counts']*0.8)]\n",
    "    test_df = df.loc[df['rating_cumcounts'] >= (df['total_counts']*0.8)]\n",
    "    train_df, validation_df = train_test_split(train_df, test_size=0.1, random_state=1)\n",
    "    \n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "def preprocessing(df):\n",
    "    df = sort_by_time(df)\n",
    "    df = filter_by_occurrence(df, 'uid_index', 5)\n",
    "    df = filter_by_occurrence(df, 'iid', 5)\n",
    "    df['rrate'] = df['rrate'].apply(lambda x: 1 if x != 'None' else 0)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def get_data(DATASET = 'LON'):\n",
    "    assert DATASET in ['LON', 'NYC']\n",
    "    \n",
    "    print('Read data...')\n",
    "    if DATASET == 'LON':\n",
    "        df = pd.read_csv(DATASET_LON, sep='\\t')[user_columns + LON_item_columns + rating_columns].fillna('NaN')\n",
    "        sparse_features = LON_sparse_features\n",
    "    else:\n",
    "        df = pd.read_csv(DATASET_NYC, sep='\\t')[user_columns + NYC_item_columns + rating_columns].fillna('NaN')\n",
    "        sparse_features = NYC_sparse_features\n",
    "    \n",
    "    # sort, filter, binarize\n",
    "    df = preprocessing(df)\n",
    "    \n",
    "    #Label encode categorical features\n",
    "    for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        df[feat] = lbe.fit_transform(df[feat].astype('str'))\n",
    "  \n",
    "    train_df, val_df, test_df = split_df(df)      \n",
    "    \n",
    "    train_y, val_y, test_y = train_df[['rrate']], val_df[['rrate']], test_df[['rrate']]\n",
    "    \n",
    "    train_df, val_df, test_df = train_df[sparse_features], val_df[sparse_features], test_df[sparse_features]\n",
    "    \n",
    "    return train_df, val_df, test_df, train_y, val_y, test_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    " - user : uage, ugender, ucity, ucountry, uid_index, ulevel\n",
    " - item : iid, irating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data...\n",
      "\n",
      "train shape: \n",
      " (87440, 8)\n",
      "\n",
      "validation  shape: \n",
      " (9716, 8)\n",
      "\n",
      "test shape: \n",
      " (39339, 8)\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df, train_y, val_y, test_y = get_data('LON')\n",
    "print(\"\\ntrain shape: \\n\",train_df.shape)\n",
    "print(\"\\nvalidation  shape: \\n\",val_df.shape)\n",
    "print(\"\\ntest shape: \\n\",test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT‐LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDTLR(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_estimators=100, max_depth=3,\n",
    "                 min_samples_leaf=1, max_leaf_nodes=None,\n",
    "                 subsample=1.0, learning_rate=0.1,\n",
    "                 max_iter=100, C=1.0, random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.subsample = subsample\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.gbdt_params = {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'max_depth': self.max_depth,\n",
    "            'min_samples_leaf': self.min_samples_leaf,\n",
    "            'max_leaf_nodes': self.max_leaf_nodes,\n",
    "            'subsample': self.subsample,\n",
    "            'learning_rate': self.learning_rate\n",
    "        }\n",
    "\n",
    "        self.lr_params = {\n",
    "            'C': self.C,\n",
    "            'max_iter': self.max_iter\n",
    "        }\n",
    "\n",
    "        self.GBDT = GradientBoostingClassifier(**self.gbdt_params, random_state=random_state)\n",
    "        self.LR = LogisticRegression(**self.lr_params, random_state=random_state)\n",
    "        self.ENC = OneHotEncoder(categories='auto')\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_gbdt, X_lr, Y_gbdt, Y_lr = train_test_split(X, y, test_size=0.5)\n",
    "        self.GBDT.fit(X_gbdt, Y_gbdt)\n",
    "        tree_feature = self.GBDT.apply(X_gbdt)[:, :, 0]\n",
    "        self.ENC.fit(tree_feature)\n",
    "\n",
    "        X = self.ENC.transform(self.GBDT.apply(X_lr)[:, :, 0])\n",
    "        y = Y_lr\n",
    "        return self.LR.fit(X, y)\n",
    "\n",
    "\n",
    "    def predict(self,X):\n",
    "        X = self.ENC.transform(self.GBDT.apply(X)[:, :, 0])\n",
    "        return self.LR.predict(X)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = self.ENC.transform(self.GBDT.apply(X)[:, :, 0])\n",
    "        return self.LR.predict_proba(X)\n",
    "\n",
    "    def predict_log_proba(self,X):\n",
    "        X = self.ENC.transform(self.GBDT.apply(X)[:, :, 0])\n",
    "        return self.LR.predict_log_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        # 这些是GBDT的超参数\n",
    "        'n_estimators': 300, 'max_depth': 7,\n",
    "        'min_samples_leaf': 45, 'max_leaf_nodes': 4,\n",
    "        'subsample': 0.8, 'learning_rate': 0.1,\n",
    "        # 这些是LR的超参数\n",
    "        'max_iter': 2770, 'C': 0.8,\n",
    "        # random_state是公共参数\n",
    "        'random_state': 1234\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDTLR(C=0.8, learning_rate=0.1, max_depth=7, max_iter=2770, max_leaf_nodes=4,\n",
      "       min_samples_leaf=45, n_estimators=300, random_state=1234, subsample=0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\envs\\Python3.6\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "F:\\Anaconda3\\envs\\Python3.6\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model = GBDTLR(**params)\n",
    "print(model)\n",
    "\n",
    "model.fit(train_df, train_y)\n",
    "val_pred_proba = model.predict_proba(val_df)\n",
    "test_pred_proba = model.predict_proba(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation roc_auc: 0.9914227350528894\n",
      "validation log_loss scores: 0.08460939214818365\n",
      "validation NDCG@5 scores: 0.9999999999999999\n",
      "\n",
      "test roc_auc: 0.9857000626292651\n",
      "test log_loss scores: 0.10890890146234473\n",
      "test NDCG@5 scores: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "roc_auc = metrics.roc_auc_score(np.array(val_y), np.array(val_pred_proba)[:,1])\n",
    "logloss = metrics.log_loss(val_y, val_pred_proba.astype('float64'))\n",
    "ndcg_val = ndcg_score(np.expand_dims(np.array(val_y).T[0], axis=0), np.expand_dims(np.array(val_pred_proba)[:,1], axis=0), k=5)\n",
    "print(\"validation roc_auc:\",roc_auc)\n",
    "print('validation log_loss scores:', logloss)\n",
    "print('validation NDCG@5 scores:', ndcg_val)\n",
    "\n",
    "roc_auc = metrics.roc_auc_score(np.array(test_y), np.array(test_pred_proba)[:,1])\n",
    "logloss = metrics.log_loss(test_y, test_pred_proba.astype('float64'))\n",
    "ndcg_test = ndcg_score(np.expand_dims(np.array(test_y).T[0], axis=0), np.expand_dims(np.array(test_pred_proba)[:,1], axis=0), k=5)\n",
    "print(\"\\ntest roc_auc:\",roc_auc)\n",
    "print('test log_loss scores:', logloss)\n",
    "print('test NDCG@5 scores:', ndcg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
