{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization with BPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package initialization\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch\n",
    "from spotlight.datasets import _transport\n",
    "from spotlight.interactions import Interactions\n",
    "from spotlight.cross_validation import random_train_test_split\n",
    "from spotlight.datasets.movielens import get_movielens_dataset\n",
    "from spotlight.factorization.implicit import ImplicitFactorizationModel\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, ndcg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_LON = '../data/LON-A/London_Attractions_Complete_Review.csv'\n",
    "DATASET_NYC = '../data/NYC-R/New_York_City_Restaurant_Complete_Review.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_time(df):\n",
    "    return df.sort_values(by=['rtime'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_occurrence(df, column, threshold):\n",
    "    return df.groupby(column).filter(lambda x: len(x) >= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_binary(df):\n",
    "    df.loc[df['rrate'] != \"None\", 'rrate'] = 1.0\n",
    "    df.loc[df['rrate'] == \"None\", 'rrate'] = 0.0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(dataframe):\n",
    "    \n",
    "    # sort by time (ascending order)\n",
    "    df = sort_by_time(dataframe)\n",
    "    \n",
    "    # retrieve needed columns\n",
    "    df = df[['uid_index', 'iid', 'rrate']]\n",
    "    \n",
    "    # convert ratings into binarys\n",
    "    df = convert_binary(df)\n",
    "    \n",
    "    df['rrate'] = pd.to_numeric(df['rrate'])\n",
    "    \n",
    "    # Retain users/items with at least five ratings only\n",
    "    df = filter_by_occurrence(df, 'iid', 5)\n",
    "    df = filter_by_occurrence(df, 'uid_index', 5)\n",
    "    \n",
    "    # split dataset into training set, validation set and test set\n",
    "    users = df.groupby('uid_index')\n",
    "    \n",
    "    test_df = pd.DataFrame()\n",
    "    train_validation_df = pd.DataFrame()\n",
    "    \n",
    "    # for each user, get its latest 20% rating as test set\n",
    "    for uid in users.size().to_dict().keys():\n",
    "        user = users.get_group(uid)\n",
    "        split_idx = int(len(user)*0.8)\n",
    "        test_df = test_df.append(user.iloc[split_idx:])\n",
    "        train_validation_df = train_validation_df.append(user.iloc[:split_idx])\n",
    "    \n",
    "    train_validation_df = train_validation_df.reindex(np.random.permutation(train_validation_df.index)) # shuffle\n",
    "    train_df = train_validation_df.iloc[:int(len(train_validation_df)*0.875)]\n",
    "    validation_df = train_validation_df.iloc[int(len(train_validation_df)*0.875):]\n",
    "    \n",
    "    return (train_df, validation_df, test_df)\n",
    "    #return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_my_own(dataset):\n",
    "    \n",
    "    usuarios = dataset['uid_index'].to_numpy()\n",
    "    items = dataset['iid'].to_numpy()\n",
    "    ratings = dataset['rrate'].to_numpy()\n",
    "    \n",
    "    return (usuarios, items, ratings)\n",
    "\n",
    "def get_my_own_dataset(data):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    Interactions: :class:`spotlight.interactions.Interactions`\n",
    "        instance of the interactions class\n",
    "    \"\"\"\n",
    "    return Interactions(*_get_my_own(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_train_test_split(interactions,\n",
    "                            test_percentage=0.2,\n",
    "                            random_state=None):\n",
    "    \"\"\"\n",
    "    Randomly split interactions between training and testing.\n",
    "    Parameters\n",
    "    ----------\n",
    "    interactions: :class:`spotlight.interactions.Interactions`\n",
    "        The interactions to shuffle.\n",
    "    test_percentage: float, optional\n",
    "        The fraction of interactions to place in the test set.\n",
    "    random_state: np.random.RandomState, optional\n",
    "        The random state used for the shuffle.\n",
    "    Returns\n",
    "    -------\n",
    "    (train, test): (:class:`spotlight.interactions.Interactions`,\n",
    "                    :class:`spotlight.interactions.Interactions`)\n",
    "         A tuple of (train data, test data)\n",
    "    \"\"\"\n",
    "\n",
    "    interactions = shuffle_interactions(interactions,\n",
    "                                        random_state=random_state)\n",
    "\n",
    "    cutoff = int((1.0 - test_percentage) * len(interactions))\n",
    "\n",
    "    train_idx = slice(None, cutoff)\n",
    "    test_idx = slice(cutoff, None)\n",
    "\n",
    "    train = Interactions(interactions.user_ids[train_idx],\n",
    "                         interactions.item_ids[train_idx],\n",
    "                         ratings=_index_or_none(interactions.ratings,\n",
    "                                                train_idx),\n",
    "                         timestamps=_index_or_none(interactions.timestamps,\n",
    "                                                   train_idx),\n",
    "                         weights=_index_or_none(interactions.weights,\n",
    "                                                train_idx),\n",
    "                         num_users=interactions.num_users,\n",
    "                         num_items=interactions.num_items)\n",
    "    test = Interactions(interactions.user_ids[test_idx],\n",
    "                        interactions.item_ids[test_idx],\n",
    "                        ratings=_index_or_none(interactions.ratings,\n",
    "                                               test_idx),\n",
    "                        timestamps=_index_or_none(interactions.timestamps,\n",
    "                                                  test_idx),\n",
    "                        weights=_index_or_none(interactions.weights,\n",
    "                                               test_idx),\n",
    "                        num_users=interactions.num_users,\n",
    "                        num_items=interactions.num_items)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size:  (90209, 3)\n",
      "validation set size:  (12887, 3)\n",
      "test set size:  (33178, 3)\n"
     ]
    }
   ],
   "source": [
    "# dataset preprocessing\n",
    "\n",
    "df = pd.read_csv(DATASET_LON, sep='\\t')\n",
    "\n",
    "train_df, validation_df, test_df = data_preprocess(df)\n",
    "print(\"training set size: \", train_df.shape)\n",
    "print(\"validation set size: \", validation_df.shape)\n",
    "print(\"test set size: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_my_own_dataset(train_df)\n",
    "val_dataset = get_my_own_dataset(validation_df)\n",
    "test_dataset = get_my_own_dataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPR‚ÄêMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImplicitFactorizationModel(n_iter=3,\n",
    "                                   loss='bpr')\n",
    "model.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImplicitFactorizationModel(loss='bpr',\n",
    "                                   embedding_dim=128,  # latent dimensionality\n",
    "                                   n_iter=5,          # number of epochs of training\n",
    "                                   batch_size=128,     # minibatch size\n",
    "                                   l2=1e-9,            # strength of L2 regularization\n",
    "                                   learning_rate=1e-3)\n",
    "\n",
    "model.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = model.predict(val_dataset.user_ids, val_dataset.item_ids)\n",
    "test_predictions = model.predict(test_dataset.user_ids, test_dataset.item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.1717205,  6.0094385,  6.8720193, ...,  2.4474103, 10.612924 ,\n",
       "        0.6243647], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = torch.FloatTensor(val_predictions)\n",
    "val_preds = nn.Sigmoid()(val_preds).numpy()\n",
    "val_preds_b = (val_preds > 0.5).astype(float)\n",
    "\n",
    "test_preds = torch.FloatTensor(test_predictions)\n",
    "test_preds = nn.Sigmoid()(test_preds).numpy()\n",
    "test_preds_b = (test_preds > 0.5).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y = val_dataset.ratings\n",
    "test_y = test_dataset.ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation roc_auc: 0.5034873528958788\n",
      "validation log_loss scores: 1.5622212511405003\n",
      "validation NDCG@5 scores: 0.8634146341463413\n",
      "\n",
      "test roc_auc: 0.5082576589801349\n",
      "test log_loss scores: 1.6007943182165427\n",
      "test NDCG@5 scores: 0.8273381294964027\n"
     ]
    }
   ],
   "source": [
    "roc_auc = metrics.roc_auc_score(np.array(val_y), np.array(val_preds))\n",
    "logloss = metrics.log_loss(val_y, val_preds.astype('float64'))\n",
    "ndcg_val = ndcg_score(np.expand_dims(np.array(val_y), axis=0), np.expand_dims(np.array(val_preds), axis=0), k=5)\n",
    "print(\"validation roc_auc:\",roc_auc)\n",
    "print('validation log_loss scores:', logloss)\n",
    "print('validation NDCG@5 scores:', ndcg_val)\n",
    "\n",
    "roc_auc = metrics.roc_auc_score(np.array(test_y), np.array(test_preds))\n",
    "logloss = metrics.log_loss(test_y, test_preds.astype('float64'))\n",
    "ndcg_test = ndcg_score(np.expand_dims(np.array(test_y), axis=0), np.expand_dims(np.array(test_preds), axis=0), k=5)\n",
    "print(\"\\ntest roc_auc:\",roc_auc)\n",
    "print('test log_loss scores:', logloss)\n",
    "print('test NDCG@5 scores:', ndcg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
