{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLG Homework 2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "P6vWie0tTC_t",
        "rC9iAW6YdlEc"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRhstY_jYJap",
        "colab_type": "text"
      },
      "source": [
        "### TODO\n",
        "\n",
        "* Evaluation:\n",
        "  - [x] ROC AUC\n",
        "  - [x] Evaluation: precision\n",
        "* Experiment:\n",
        "  - [ ] Baseline1\n",
        "  - [ ] Baseline2: common neighbor, jaccard coefficient, preferrential attachment ... etc(`#commons`)\n",
        "  - [ ] Method3: each-10% `#1`, each-10% `#2`\n",
        "  - [ ] Method4: each-quad `#1`, each-quad `#2`\n",
        "  - [ ] Method5: distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ2fkvxwwVwG",
        "colab_type": "text"
      },
      "source": [
        "## Package Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD-5slsOTfYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6vWie0tTC_t",
        "colab_type": "text"
      },
      "source": [
        "## Parse & Reformat Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F7A0caV1RO8",
        "colab_type": "text"
      },
      "source": [
        "read training data, save to pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrqqlqUn2cfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data = pd.read_csv('train.csv', sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85gFfvxm3lCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NP7pLWwQodq",
        "colab_type": "text"
      },
      "source": [
        "read testing data, save to pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsIJChFGQsf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = pd.read_csv('test.csv', sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d01gwpxNQ1IF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVithvvc1Y9Z",
        "colab_type": "text"
      },
      "source": [
        "read node attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ8OjoHt2old",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "node_features = pd.read_csv('content.csv', header=None, sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tvTTYBi2vbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "node_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZJPeIRyINtU",
        "colab_type": "text"
      },
      "source": [
        "replace node id with attributes in train data and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U39srvff5JOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training data (labels)\n",
        "labels = list(training_data[\"label\"])\n",
        "labels = [str(i) for i in labels]\n",
        "\n",
        "with open(\"./processed_train_labels.csv\", \"w\") as file:\n",
        "    file.write('\\n'.join(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBVQJZX6ILuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training data (attributes)\n",
        "\n",
        "edges = []\n",
        "all_features = []\n",
        "\n",
        "for i in range(training_data.shape[0]):\n",
        "# for i in range(2):\n",
        "\n",
        "    training_row = list(training_data.iloc[i])\n",
        "    edge_id = training_row[0]\n",
        "    from_node, to_node = training_row[1:-1]\n",
        "\n",
        "    edges.append(edge_id)\n",
        "\n",
        "    from_features = node_features.loc[node_features[0] == from_node]\n",
        "    from_features = list(from_features.iloc[0])[1:]\n",
        "    from_features = [str(j) for j in from_features]\n",
        "    from_features_string = \"\".join(from_features)\n",
        "    \n",
        "    to_features = node_features.loc[node_features[0] == to_node]\n",
        "    to_features = list(to_features.iloc[0])[1:]\n",
        "    to_features = [str(j) for j in to_features]\n",
        "    to_features_string = \"\".join(to_features)\n",
        "\n",
        "    all_features.append(from_features_string + \",\" + to_features_string)\n",
        "\n",
        "with open(\"./train_attributes.csv\", \"w\") as file:\n",
        "    file.write('\\n'.join(all_features))\n",
        "\n",
        "with open(\"./train_edge_list.csv\", \"w\") as file:\n",
        "    file.write('\\n'.join(edges))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jpMggWuWo4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test data (attributes)\n",
        "\n",
        "edges = []\n",
        "all_features = []\n",
        "\n",
        "for i in range(test_data.shape[0]):\n",
        "\n",
        "    test_row = list(test_data.iloc[i])\n",
        "    edge_id = test_row[0]\n",
        "    from_node, to_node = test_row[1:]\n",
        "\n",
        "    edges.append(edge_id)\n",
        "\n",
        "    from_features = node_features.loc[node_features[0] == from_node]\n",
        "    from_features = list(from_features.iloc[0])[1:]\n",
        "    from_features = [str(j) for j in from_features]\n",
        "    from_features_string = \"\".join(from_features)\n",
        "    \n",
        "    to_features = node_features.loc[node_features[0] == to_node]\n",
        "    to_features = list(to_features.iloc[0])[1:]\n",
        "    to_features = [str(j) for j in to_features]\n",
        "    to_features_string = \"\".join(to_features)\n",
        "\n",
        "    all_features.append(from_features_string + \",\" + to_features_string)\n",
        "\n",
        "with open(\"./test_attributes.csv\", \"w\") as file:\n",
        "    file.write('\\n'.join(all_features))\n",
        "\n",
        "# with open(\"./test_edge_list.csv\", \"w\") as file:\n",
        "#     file.write('\\n'.join(edges))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC9iAW6YdlEc",
        "colab_type": "text"
      },
      "source": [
        "## Calcualte Features from Reformatted data\n",
        "\n",
        "generate the following features:\n",
        "\n",
        "* [x] **Jaccard Coefficienct**\n",
        "* [x] **Common Neighbor**\n",
        "* [x] **Preferrential attachment** (`#neighborA` x `#neighborB`)\n",
        "* [x] per-10% `#1`\n",
        "* [x] per-10% `#2`\n",
        "* [x] per-25% `#1`\n",
        "* [x] per-25% `#2`\n",
        "* [x] common rate: `#common` / `#neighborA` & `#common` / `#neighborB`\n",
        "* [x] `#neighborA`\n",
        "* [x] `#neighborB`\n",
        "* [x] distance of means of attr position\n",
        "* [x] standard deviation of position of `#neighborA`\n",
        "* [x] standard deviation of position of `#neighborB`\n",
        "* [x] product of standard deviation of attr position\n",
        "* [x] standard deviation of position of `#common`\n",
        "* [ ] weighted common neighbor sum\n",
        "* [ ] weighted common neighbor sum / `#neighborA`\n",
        "* [ ] weighted common neighbor sum / `#neighborB`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssCMvfjk8_Yz",
        "colab_type": "text"
      },
      "source": [
        "for training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_HX-gSzhTG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the reformatted dataset\n",
        "attributes = pd.read_csv('train_attributes.csv', header=None, sep=',')\n",
        "\n",
        "all_features = []\n",
        "\n",
        "for idx in range(attributes.shape[0]):\n",
        "# for idx in range(1):\n",
        "    from_node, to_node = attributes.iloc[idx]\n",
        "\n",
        "    # convert string attributes to a tensor\n",
        "    from_tensor = list(from_node)\n",
        "    from_tensor = [int(a) for a in from_tensor]\n",
        "    from_tensor = torch.tensor(from_tensor, dtype=torch.float)\n",
        "\n",
        "    to_tensor = list(to_node)\n",
        "    to_tensor = [int(a) for a in to_tensor]\n",
        "    to_tensor = torch.tensor(to_tensor, dtype=torch.float)\n",
        "\n",
        "    # add aggregate: like 'union'\n",
        "    added = from_tensor + to_tensor\n",
        "\n",
        "    # product aggregate: like 'join'\n",
        "    producted = from_tensor * to_tensor\n",
        "\n",
        "    # neighbor number\n",
        "    neighbor_from = (from_tensor == 1).sum().item()\n",
        "    neighbor_to = (to_tensor == 1).sum().item()\n",
        "\n",
        "    # [common neighbor]    \n",
        "    common_neighbor = (producted == 1).sum().item()\n",
        "    # print(\"common neighbor: {}\".format(common_neighbor))\n",
        "\n",
        "    # [jaccard coefficient]\n",
        "    neighbor_union = (added > 0).sum().item()\n",
        "    jaccard = common_neighbor / neighbor_union\n",
        "    # print(\"jaccard coefficient: {}\".format(jaccard))\n",
        "\n",
        "    # [preferrential attachment]\n",
        "    preferrential = neighbor_from * neighbor_to\n",
        "    # print(\"preferrential attachment: {}\".format(preferrential))\n",
        "\n",
        "    # [common rate]\n",
        "    common_rate_from = common_neighbor / neighbor_from\n",
        "    common_rate_to = common_neighbor / neighbor_to\n",
        "    # print(\"common rate from: {}\".format(common_rate_from))\n",
        "    # print(\"common rate to: {}\".format(common_rate_to))\n",
        "\n",
        "    # [each-10% 's #1 and #2]\n",
        "    each_ten_ones = []\n",
        "    each_ten_twos = []\n",
        "\n",
        "    interval = 10\n",
        "    for i in range(10):\n",
        "        start_index = int((i * interval / 100) * added.shape[0]) # percentage * total length\n",
        "        end_index = int(((i+1) * interval / 100) * added.shape[0]) # percentage * total length\n",
        "        \n",
        "        ones = (added[start_index:end_index] == 1).sum().item()\n",
        "        twos = (added[start_index:end_index] == 2).sum().item()\n",
        "\n",
        "        each_ten_ones.append(ones)\n",
        "        each_ten_twos.append(twos)\n",
        "\n",
        "    # print(\"each ten ones\")\n",
        "    # print(each_ten_ones)\n",
        "    # print(\"each ten twos\")\n",
        "    # print(each_ten_twos)\n",
        "\n",
        "    # [each-25% 's #1 and #2]\n",
        "    each_quad_ones = []\n",
        "    each_quad_twos = []\n",
        "\n",
        "    interval = 25\n",
        "    for i in range(4):\n",
        "        start_index = int((i * interval / 100) * added.shape[0]) # percentage * total length\n",
        "        end_index = int(((i+1) * interval / 100) * added.shape[0]) # percentage * total length\n",
        "        \n",
        "        ones = (added[start_index:end_index] == 1).sum().item()\n",
        "        twos = (added[start_index:end_index] == 2).sum().item()\n",
        "\n",
        "        each_quad_ones.append(ones)\n",
        "        each_quad_twos.append(twos)\n",
        "\n",
        "    # print(\"each quad ones\")\n",
        "    # print(each_quad_ones)\n",
        "    # print(\"each quad twos\")\n",
        "    # print(each_quad_twos)\n",
        "\n",
        "    # [difference of means]\n",
        "    mean_difference = 1\n",
        "    if (neighbor_from != 0 and neighbor_to != 0):\n",
        "        mean_from = (torch.arange(from_tensor.shape[0]) * from_tensor).sum().item() / neighbor_from\n",
        "        mean_to = (torch.arange(to_tensor.shape[0]) * to_tensor).sum().item() / neighbor_to\n",
        "\n",
        "        mean_difference = abs(mean_from - mean_to) / from_tensor.shape[0] # difference of means / total length\n",
        "\n",
        "    # print(\"difference of mean: {}\".format(mean_difference))\n",
        "\n",
        "    # [standard deviation of position of #neighborA]\n",
        "    s_d_rate_from = 1\n",
        "    if (neighbor_from != 0):\n",
        "        standard_deviation_from = math.sqrt(((torch.arange(from_tensor.shape[0]) - mean_from)**2 * from_tensor).sum().item() / neighbor_from)\n",
        "        s_d_rate_from = standard_deviation_from / from_tensor.shape[0]\n",
        "\n",
        "    # print(\"s_d_rate_from: {}\".format(s_d_rate_from))\n",
        "\n",
        "    # [standard deviation of position of #neighborB]\n",
        "    s_d_rate_to = 1\n",
        "    if (neighbor_to != 0):\n",
        "        standard_deviation_to = math.sqrt(((torch.arange(to_tensor.shape[0]) - mean_to)**2 * to_tensor).sum().item() / neighbor_to)\n",
        "        s_d_rate_to = standard_deviation_to / to_tensor.shape[0]\n",
        "\n",
        "    # print(\"s_d_rate_to: {}\".format(s_d_rate_to))\n",
        "\n",
        "    # [product of standard deviation]\n",
        "    s_d_rate_producted = s_d_rate_from * s_d_rate_to\n",
        "\n",
        "    # [standard deviation of position of #common]\n",
        "    s_d_rate_common = 1\n",
        "    if (common_neighbor != 0):\n",
        "        mean_common = (torch.arange(producted.shape[0]) * producted).sum().item() / common_neighbor\n",
        "        standard_deviation_common = math.sqrt(((torch.arange(producted.shape[0]) - mean_common)**2 * producted).sum().item() / common_neighbor)\n",
        "        s_d_rate_common = standard_deviation_common / producted.shape[0]\n",
        "\n",
        "    # print(\"s_d_rate_common: {}\".format(s_d_rate_common))\n",
        "    \n",
        "    # prepare to export\n",
        "    each_ten_ones_string = \",\".join([str(i) for i in each_ten_ones])\n",
        "\n",
        "    features = [\n",
        "        str(common_neighbor),\n",
        "        str(jaccard),\n",
        "        str(preferrential),\n",
        "        str(common_rate_from),\n",
        "        str(common_rate_to),\n",
        "        \",\".join([str(i) for i in each_ten_ones]),\n",
        "        \",\".join([str(i) for i in each_ten_twos]),\n",
        "        \",\".join([str(i) for i in each_quad_ones]),\n",
        "        \",\".join([str(i) for i in each_quad_twos]),\n",
        "        str(mean_difference),\n",
        "        str(s_d_rate_from),\n",
        "        str(s_d_rate_to),\n",
        "        str(s_d_rate_producted),\n",
        "        str(s_d_rate_common)\n",
        "    ]\n",
        "\n",
        "    all_features.append(\",\".join(features))\n",
        "\n",
        "with open(\"train_features.csv\", \"w\") as file:\n",
        "    file.write(\"\\n\".join(all_features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1dxUKBe9CaJ",
        "colab_type": "text"
      },
      "source": [
        "for test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I1u9YIy9DzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the reformatted dataset\n",
        "attributes = pd.read_csv('test_attributes.csv', header=None, sep=',')\n",
        "\n",
        "all_features = []\n",
        "\n",
        "for idx in range(attributes.shape[0]):\n",
        "# for idx in range(1):\n",
        "    from_node, to_node = attributes.iloc[idx]\n",
        "\n",
        "    # convert string attributes to a tensor\n",
        "    from_tensor = list(from_node)\n",
        "    from_tensor = [int(a) for a in from_tensor]\n",
        "    from_tensor = torch.tensor(from_tensor, dtype=torch.float)\n",
        "\n",
        "    to_tensor = list(to_node)\n",
        "    to_tensor = [int(a) for a in to_tensor]\n",
        "    to_tensor = torch.tensor(to_tensor, dtype=torch.float)\n",
        "\n",
        "    # add aggregate: like 'union'\n",
        "    added = from_tensor + to_tensor\n",
        "\n",
        "    # product aggregate: like 'join'\n",
        "    producted = from_tensor * to_tensor\n",
        "\n",
        "    # neighbor number\n",
        "    neighbor_from = (from_tensor == 1).sum().item()\n",
        "    neighbor_to = (to_tensor == 1).sum().item()\n",
        "\n",
        "    # [common neighbor]    \n",
        "    common_neighbor = (producted == 1).sum().item()\n",
        "    # print(\"common neighbor: {}\".format(common_neighbor))\n",
        "\n",
        "    # [jaccard coefficient]\n",
        "    neighbor_union = (added > 0).sum().item()\n",
        "    jaccard = common_neighbor / neighbor_union\n",
        "    # print(\"jaccard coefficient: {}\".format(jaccard))\n",
        "\n",
        "    # [preferrential attachment]\n",
        "    preferrential = neighbor_from * neighbor_to\n",
        "    # print(\"preferrential attachment: {}\".format(preferrential))\n",
        "\n",
        "    # [common rate]\n",
        "    common_rate_from = common_neighbor / neighbor_from\n",
        "    common_rate_to = common_neighbor / neighbor_to\n",
        "    # print(\"common rate from: {}\".format(common_rate_from))\n",
        "    # print(\"common rate to: {}\".format(common_rate_to))\n",
        "\n",
        "    # [each-10% 's #1 and #2]\n",
        "    each_ten_ones = []\n",
        "    each_ten_twos = []\n",
        "\n",
        "    interval = 10\n",
        "    for i in range(10):\n",
        "        start_index = int((i * interval / 100) * added.shape[0]) # percentage * total length\n",
        "        end_index = int(((i+1) * interval / 100) * added.shape[0]) # percentage * total length\n",
        "        \n",
        "        ones = (added[start_index:end_index] == 1).sum().item()\n",
        "        twos = (added[start_index:end_index] == 2).sum().item()\n",
        "\n",
        "        each_ten_ones.append(ones)\n",
        "        each_ten_twos.append(twos)\n",
        "\n",
        "    # print(\"each ten ones\")\n",
        "    # print(each_ten_ones)\n",
        "    # print(\"each ten twos\")\n",
        "    # print(each_ten_twos)\n",
        "\n",
        "    # [each-25% 's #1 and #2]\n",
        "    each_quad_ones = []\n",
        "    each_quad_twos = []\n",
        "\n",
        "    interval = 25\n",
        "    for i in range(4):\n",
        "        start_index = int((i * interval / 100) * added.shape[0]) # percentage * total length\n",
        "        end_index = int(((i+1) * interval / 100) * added.shape[0]) # percentage * total length\n",
        "        \n",
        "        ones = (added[start_index:end_index] == 1).sum().item()\n",
        "        twos = (added[start_index:end_index] == 2).sum().item()\n",
        "\n",
        "        each_quad_ones.append(ones)\n",
        "        each_quad_twos.append(twos)\n",
        "\n",
        "    # print(\"each quad ones\")\n",
        "    # print(each_quad_ones)\n",
        "    # print(\"each quad twos\")\n",
        "    # print(each_quad_twos)\n",
        "\n",
        "    # [difference of means]\n",
        "    mean_difference = 1\n",
        "    if (neighbor_from != 0 and neighbor_to != 0):\n",
        "        mean_from = (torch.arange(from_tensor.shape[0]) * from_tensor).sum().item() / neighbor_from\n",
        "        mean_to = (torch.arange(to_tensor.shape[0]) * to_tensor).sum().item() / neighbor_to\n",
        "\n",
        "        mean_difference = abs(mean_from - mean_to) / from_tensor.shape[0] # difference of means / total length\n",
        "\n",
        "    # print(\"difference of mean: {}\".format(mean_difference))\n",
        "\n",
        "    # [standard deviation of position of #neighborA]\n",
        "    s_d_rate_from = 1\n",
        "    if (neighbor_from != 0):\n",
        "        standard_deviation_from = math.sqrt(((torch.arange(from_tensor.shape[0]) - mean_from)**2 * from_tensor).sum().item() / neighbor_from)\n",
        "        s_d_rate_from = standard_deviation_from / from_tensor.shape[0]\n",
        "\n",
        "    # print(\"s_d_rate_from: {}\".format(s_d_rate_from))\n",
        "\n",
        "    # [standard deviation of position of #neighborB]\n",
        "    s_d_rate_to = 1\n",
        "    if (neighbor_to != 0):\n",
        "        standard_deviation_to = math.sqrt(((torch.arange(to_tensor.shape[0]) - mean_to)**2 * to_tensor).sum().item() / neighbor_to)\n",
        "        s_d_rate_to = standard_deviation_to / to_tensor.shape[0]\n",
        "\n",
        "    # print(\"s_d_rate_to: {}\".format(s_d_rate_to))\n",
        "\n",
        "    # [product of standard deviation]\n",
        "    s_d_rate_producted = s_d_rate_from * s_d_rate_to\n",
        "\n",
        "    # [standard deviation of position of #common]\n",
        "    s_d_rate_common = 1\n",
        "    if (common_neighbor != 0):\n",
        "        mean_common = (torch.arange(producted.shape[0]) * producted).sum().item() / common_neighbor\n",
        "        standard_deviation_common = math.sqrt(((torch.arange(producted.shape[0]) - mean_common)**2 * producted).sum().item() / common_neighbor)\n",
        "        s_d_rate_common = standard_deviation_common / producted.shape[0]\n",
        "\n",
        "    # print(\"s_d_rate_common: {}\".format(s_d_rate_common))\n",
        "    \n",
        "    # prepare to export\n",
        "    each_ten_ones_string = \",\".join([str(i) for i in each_ten_ones])\n",
        "\n",
        "    features = [\n",
        "        str(common_neighbor),\n",
        "        str(jaccard),\n",
        "        str(preferrential),\n",
        "        str(common_rate_from),\n",
        "        str(common_rate_to),\n",
        "        \",\".join([str(i) for i in each_ten_ones]),\n",
        "        \",\".join([str(i) for i in each_ten_twos]),\n",
        "        \",\".join([str(i) for i in each_quad_ones]),\n",
        "        \",\".join([str(i) for i in each_quad_twos]),\n",
        "        str(mean_difference),\n",
        "        str(s_d_rate_from),\n",
        "        str(s_d_rate_to),\n",
        "        str(s_d_rate_producted),\n",
        "        str(s_d_rate_common)\n",
        "    ]\n",
        "\n",
        "    all_features.append(\",\".join(features))\n",
        "\n",
        "with open(\"test_features.csv\", \"w\") as file:\n",
        "    file.write(\"\\n\".join(all_features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gspanbB2xwvj",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtRiSZMLx0Qd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NodePairDataset(Dataset):\n",
        "\n",
        "    def __init__(self, example_file, label_file):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations\n",
        "        \"\"\"\n",
        "        self.pairs = pd.read_csv(example_file, header=None, sep=',')\n",
        "        self.labels = pd.read_csv(label_file, header=None, sep=',')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.pairs.iloc[idx].to_numpy().flatten()\n",
        "        y = self.labels.iloc[idx].to_numpy().flatten()\n",
        "\n",
        "        return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4BAlFwv1KgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "node_pair_dataset = NodePairDataset(\"train_features.csv\", \"train_labels.csv\")\n",
        "loader = DataLoader(dataset=node_pair_dataset, batch_size=10, shuffle=False)\n",
        "for batch in zip(loader, range(1)):\n",
        "    data, index = batch\n",
        "    x, y = data\n",
        "    print(x)\n",
        "    print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpccKSVnHlQC",
        "colab_type": "text"
      },
      "source": [
        "## Network Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P10E6xdhHoSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinkPredictNetwork(nn.Module):\n",
        "    def __init__(self, features = [], hiddens=[2, 1, 0.5, 0.25], dropouts = False):\n",
        "        super(LinkPredictNetworkOne, self).__init__()\n",
        "\n",
        "        self.feature_indices = torch.tensor(features, dtype=torch.int)\n",
        "        self.input_dimension = self.feature_indices.shape[0]\n",
        "\n",
        "        self.hidden_factors = hiddens\n",
        "\n",
        "        self.dropout_factor = 0\n",
        "        if (dropouts):\n",
        "            self.dropout_factor = 0.2\n",
        "\n",
        "        self.hidden_units = [int(factor * self.input_dimension) for factor in self.hidden_factors]\n",
        "\n",
        "        self.linear1 = nn.Linear( self.input_dimension, self.hidden_units[0], bias=True)\n",
        "        self.dropout1 = nn.Dropout(self.dropout_factor)\n",
        "\n",
        "        self.linear2 = nn.Linear( self.hidden_units[0], self.hidden_units[1], bias=True)\n",
        "        self.dropout2 = nn.Dropout(self.dropout_factor)\n",
        "\n",
        "        self.linear3 = nn.Linear( self.hidden_units[1], self.hidden_units[2], bias=True)\n",
        "        self.dropout3 = nn.Dropout(self.dropout_factor)\n",
        "\n",
        "        self.linear4 = nn.Linear( self.hidden_units[2], self.hidden_units[3], bias=True)\n",
        "        self.dropout4 = nn.Dropout(self.dropout_factor)\n",
        "\n",
        "        self.outputLinear = nn.Linear( self.hidden_units[3], 1, bias=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        # x's shape: [batch_size, input_dimension]\n",
        "\n",
        "        x = x.to(torch.float)\n",
        "\n",
        "        # pick selected features to train\n",
        "        x = torch.index_select(x, 1, self.feature_indices)\n",
        "\n",
        "        x = nn.Tanh(self.linear1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = nn.Tanh(self.linear2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = nn.Tanh(self.linear3(x))\n",
        "        x = self.dropout3(x)\n",
        "        x = nn.Tanh(self.linear4(x))\n",
        "        x = self.dropout4(x)\n",
        "\n",
        "        z = torch.sigmoid(self.outputLinear(x))\n",
        "\n",
        "        return z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WbRB11WgG8i",
        "colab_type": "text"
      },
      "source": [
        "## Loss Function (Optimization Objective) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cbyBiifbzohG",
        "colab": {}
      },
      "source": [
        "def BinaryCrossEntropyLoss(outputs, targets):\n",
        "    return F.binary_cross_entropy(outputs, targets.to(torch.float), reduction='mean')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LguINZlndUMg",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrzcMgsddWrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def roc_auc(predicted, labels):\n",
        "    return roc_auc_score(labels, predicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkX0HX-Oivq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision(predicted, labels):\n",
        "    return average_precision_score(labels, predicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLyj02_6oDM-",
        "colab_type": "text"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6d-grOQmcHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = LinkPredictNetworkTwo().cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftgXh7ytp2hf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gsmAaXNm0c2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "node_pair_dataset = NodePairDataset(\"processed_train_examples_product.csv\", \"processed_train_labels.csv\")\n",
        "for i in range(100):\n",
        "    loader = DataLoader(dataset=node_pair_dataset, batch_size=5000, shuffle=True)\n",
        "    print(\"epoch {}\".format(i))\n",
        "    for x, y in loader:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        z = net(x.cuda())\n",
        "\n",
        "        loss = BinaryCrossEntropyLoss(z, y.cuda())\n",
        "        print(loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}