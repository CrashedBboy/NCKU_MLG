{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages initialization\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import heapq\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self, embedding_size=6, epoch=30,\n",
    "                 num_negatives=4, batch_size=512, lr=0.00005, drop_ratio=0.2, top_k=5):\n",
    "        \n",
    "        self.data_dir = './data/CAMRa2011/'\n",
    "        self.embedding_size = embedding_size\n",
    "        self.epoch = epoch\n",
    "        self.num_negatives = num_negatives\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.top_k = top_k\n",
    "        \n",
    "    def export_json(self, file_path):\n",
    "        \n",
    "        config_dict = {\n",
    "            'embedding_size': self.embedding_size,\n",
    "            'epoch': self.epoch,\n",
    "            'num_negatives': self.num_negatives,\n",
    "            'batch_size': self.batch_size,\n",
    "            'lr': self.lr,\n",
    "            'drop_ratio': self.drop_ratio,\n",
    "            'top_k': self.top_k\n",
    "        }\n",
    "        \n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(json.dumps(config_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to process dataset:\n",
    "\n",
    "1. combine user train set with user test set, group train set with group test set\n",
    "2. compute average user's rating count & average group's rating count\n",
    "3. filter out examples which its users/groups has total rating count lower than 30% of avg\n",
    "4. generate negative instances: pick items that haven't been interacted by users/groups\n",
    "5. split into trainset & testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or:\n",
    "\n",
    "1. generate negative instances using items that haven't been interacted by users/groups\n",
    "2. recalculate user number and group number and item number from both train & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAMRa2011Dataset(object):\n",
    "    \"\"\"CAMRa2011 dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_dir):\n",
    "        \n",
    "        self.pathes = {\n",
    "            'train': {\n",
    "                'user': dataset_dir + \"userRatingTrain.txt\",\n",
    "                'group': dataset_dir + \"groupRatingTrain.txt\"\n",
    "            },\n",
    "            'test': {\n",
    "                'user': dataset_dir + \"userRatingTest.txt\",\n",
    "                'user_negative': dataset_dir + \"userRatingNegative.txt\",\n",
    "                'group': dataset_dir + \"groupRatingTest.txt\",\n",
    "                'group_negative': dataset_dir + \"groupRatingNegative.txt\",\n",
    "            },\n",
    "            'group_user': dataset_dir + \"groupMember.txt\"\n",
    "        }\n",
    "        \n",
    "        # number of items & users (needed by embedding layer)\n",
    "        self.max_iid = 0\n",
    "        \n",
    "        # get the mapping of users and groups\n",
    "        # format: {gid: [uid, uid, ..], gid: [uid, uid, ..], ...}\n",
    "        self.group_members = self.get_group_user_mapping()\n",
    "        \n",
    "        # get interaction matrix from uid-iid training set\n",
    "        # train_user_matrix[uid, iid] = [1 | 0]\n",
    "        self.train_user_matrix = self.get_interaction_matrix(self.pathes['train']['user'])\n",
    "        \n",
    "        # format: [[uid, iid], [uid, iid], ...]\n",
    "        # only pairs of users & items have interactions would appear in the list\n",
    "        self.test_user_list = self.get_interaction_list(self.pathes['test']['user'])\n",
    "        \n",
    "        # format: [[uid, ...], [uid, ...], ...]\n",
    "        # test_user_negative_list & test_user_list follow the same order\n",
    "        # e.g. test_user_negative_list[0] is for test_user_list[0]\n",
    "        self.test_user_negative_list = self.get_negatives(self.pathes['test']['user_negative'])\n",
    "        \n",
    "        # get interaction matrix from gid-iid training set\n",
    "        self.train_group_matrix = self.get_interaction_matrix(self.pathes['train']['group'])\n",
    "\n",
    "        # pairs of group & item to be tested\n",
    "        self.test_group_list = self.get_interaction_list(self.pathes['test']['group'])\n",
    "\n",
    "        self.test_group_negative_list = self.get_negatives(self.pathes['test']['group_negative'])\n",
    "        \n",
    "        \n",
    "    def get_user_dataloader(self, batch_size=256, shuffle=True, num_negatives=6):\n",
    "        \n",
    "        users, positives_negatives = self.get_train_instances(self.train_user_matrix, num_negatives=num_negatives)\n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(users, dtype=torch.float),\n",
    "            torch.tensor(positives_negatives, dtype=torch.float))\n",
    "        \n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        \n",
    "        return loader\n",
    "    \n",
    "    def get_group_dataloader(self, batch_size=256, shuffle=True, num_negatives=6):\n",
    "        \n",
    "        groups, positives_negatives = self.get_train_instances(self.train_group_matrix, num_negatives=num_negatives)\n",
    "        \n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(groups, dtype=torch.float),\n",
    "            torch.tensor(positives_negatives, dtype=torch.float))\n",
    "        \n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        \n",
    "        return loader\n",
    "    \n",
    "    # get number of groups, users and items\n",
    "    def get_sizes(self):\n",
    "        group_size = len(self.group_members)\n",
    "        \n",
    "        num_user, _ = self.train_user_matrix.shape\n",
    "        \n",
    "        return (group_size, num_user, self.max_iid+1)\n",
    "        \n",
    "    def get_train_instances(self, interaction_matrix, num_negatives=6):\n",
    "        \n",
    "        users, positive_items, negative_items = [], [], []\n",
    "        \n",
    "        for (uid, iid) in interaction_matrix.keys():\n",
    "            \n",
    "            # positive instance\n",
    "            for _ in range(num_negatives):\n",
    "                \n",
    "                # positive instances\n",
    "                positive_items.append(iid)\n",
    "                \n",
    "                # negative instances ---> need to be fixed\n",
    "                negative_iid = np.random.randint(self.max_iid+1)\n",
    "                while (uid, negative_iid) in interaction_matrix:\n",
    "                    negative_iid = np.random.randint(self.max_iid+1) # re-generate an negative iid\n",
    "                negative_items.append(negative_iid)\n",
    "                \n",
    "                # users\n",
    "                users.append(uid)\n",
    "\n",
    "        positives_negatives = [[positive_iid, negative_iid] for positive_iid, negative_iid in zip(positive_items, negative_items)]\n",
    "        \n",
    "        return users, positives_negatives\n",
    "        \n",
    "        \n",
    "    def get_group_user_mapping(self):\n",
    "    \n",
    "        mapping = {}\n",
    "\n",
    "        # read mapping file\n",
    "        with open(self.pathes['group_user'], 'r') as file:\n",
    "\n",
    "            line = file.readline().strip()\n",
    "            while line != None and line != \"\":\n",
    "\n",
    "                # sample line format: [gid] [uid 1],[uid 2],[uid 3],[uid 4]\n",
    "                sequences = line.split(' ')\n",
    "                gid = int(sequences[0])\n",
    "                mapping[gid] = []\n",
    "                for uid in sequences[1].split(','):\n",
    "                    mapping[gid].append(int(uid))\n",
    "                line = file.readline().strip()\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    # parse all interactions in dataset to 2D sparse matrix\n",
    "    def get_interaction_matrix(self, rating_file_path):\n",
    "\n",
    "        # get number of users and items\n",
    "        num_users, num_items = 0, 0\n",
    "        with open(rating_file_path, \"r\") as file:\n",
    "\n",
    "            line = file.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\" \")\n",
    "                uid, iid = int(arr[0]), int(arr[1])\n",
    "\n",
    "                # update num_items if bigger iid appears\n",
    "                num_items = max(num_items, iid)\n",
    "                # update num_items if bigger iid appears\n",
    "                num_users = max(num_users, uid)\n",
    "                \n",
    "                line = file.readline()\n",
    "                \n",
    "        # update the max iid / uid of the whole dataset\n",
    "        self.max_iid = max(self.max_iid, num_items)\n",
    "\n",
    "        # construct interaction matrix\n",
    "        # dok_matrix: Dictionary Of Keys based sparse matrix, an efficient structure for constructing sparse matrices incrementally.\n",
    "        matrix = sparse.dok_matrix((num_users + 1, num_items + 1), dtype=np.float32) # iid and uid starts from 1\n",
    "        with open(rating_file_path, \"r\") as file:\n",
    "            line = file.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\" \")\n",
    "                if len(arr) > 2:\n",
    "                    uid, iid, rating = int(arr[0]), int(arr[1]), int(arr[2])\n",
    "                    if (rating > 0):\n",
    "                        matrix[uid, iid] = 1.0\n",
    "                else:\n",
    "                    uid, iid = int(arr[0]), int(arr[1])\n",
    "                    matrix[uid, iid] = 1.0\n",
    "                line = file.readline()\n",
    "\n",
    "        return matrix\n",
    "    \n",
    "    # parse all interactions in dataset to list\n",
    "    def get_interaction_list(self, rating_file_path):\n",
    "\n",
    "        interaction_list = []\n",
    "        with open(rating_file_path, \"r\") as file:\n",
    "            line = file.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\" \")\n",
    "                uid, iid = int(arr[0]), int(arr[1])\n",
    "                interaction_list.append([uid, iid])\n",
    "                \n",
    "                # update max iid/uid if bigger iid/uid appears\n",
    "                self.max_iid = max(self.max_iid, iid)\n",
    "                \n",
    "                line = file.readline()\n",
    "\n",
    "        return interaction_list\n",
    "\n",
    "    # parse negative sample lists for pairs in test set\n",
    "    # negative samples: the items which never been interacted\n",
    "    # the order of returned sample lists must be paired with test list\n",
    "    def get_negatives(self, file_path):\n",
    "\n",
    "        negative_samples_list = []\n",
    "\n",
    "        with open(file_path, \"r\") as file:\n",
    "\n",
    "            line = file.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\" \")\n",
    "\n",
    "                negative_iids = []\n",
    "                for iid in arr[1:]:\n",
    "                    negative_iids.append(int(iid))\n",
    "\n",
    "                negative_samples_list.append(negative_iids)\n",
    "                line = file.readline()\n",
    "\n",
    "        return negative_samples_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGREE Model (Attentive Group Representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding networks\n",
    "\n",
    "class UserEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_users, embedding_dim):\n",
    "        super(UserEmbeddingLayer, self).__init__()\n",
    "        self.userEmbedding = nn.Embedding(num_users, embedding_dim)\n",
    "\n",
    "    def forward(self, uids):\n",
    "        user_embedded = self.userEmbedding(uids)\n",
    "        return user_embedded\n",
    "    \n",
    "class ItemEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim):\n",
    "        super(ItemEmbeddingLayer, self).__init__()\n",
    "        self.itemEmbedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "    def forward(self, iids):\n",
    "        item_embedded = self.itemEmbedding(iids)\n",
    "        return item_embedded\n",
    "    \n",
    "class GroupEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_groups, embedding_dim):\n",
    "        super(GroupEmbeddingLayer, self).__init__()\n",
    "        self.groupEmbedding = nn.Embedding(num_groups, embedding_dim)\n",
    "\n",
    "    def forward(self, gids):\n",
    "        group_embedded = self.groupEmbedding(gids)\n",
    "        return group_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention network\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, drop_ratio=0):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(embedding_dim, 16)\n",
    "        self.linear2 = nn.Linear(16, 1)\n",
    "        self.dropout = nn.Dropout(p=drop_ratio)\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_ratio),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        weights = F.softmax(x.view(1, -1), dim=1)\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final layers of AGREE for prediction\n",
    "\n",
    "class PredictLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, drop_ratio=0):\n",
    "        super(PredictLayer, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(embedding_dim, 8)\n",
    "        self.dropout = nn.Dropout(p=drop_ratio)\n",
    "        self.linear2 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        out = self.linear2(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGREE(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_groups, embedding_dim, group_member_mapping, drop_ratio):\n",
    "        super(AGREE, self).__init__()\n",
    "        \n",
    "        self.user_embedding = UserEmbeddingLayer(num_users, embedding_dim)\n",
    "        self.item_embedding = ItemEmbeddingLayer(num_items, embedding_dim)\n",
    "        self.group_embedding = GroupEmbeddingLayer(num_groups, embedding_dim)\n",
    "        self.attention = AttentionLayer(2 * embedding_dim, drop_ratio)\n",
    "        self.predict = PredictLayer(3 * embedding_dim, drop_ratio)\n",
    "        \n",
    "        self.group_members = group_member_mapping\n",
    "        \n",
    "        self.num_users = num_users\n",
    "        self.num_groups = num_groups\n",
    "        self.num_items = num_items\n",
    "        \n",
    "        # initial model's parameters\n",
    "        for m in self.modules():\n",
    "            \n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=1) # normal distribution\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.xavier_normal_(m.weight) # Glorot initialization\n",
    "\n",
    "    def forward(self, gids, uids, iids):\n",
    "        \n",
    "        # group prediction\n",
    "        if (gids is not None) and (uids is None):\n",
    "            output = self.group_forward(gids, iids)\n",
    "        # user prediction\n",
    "        else:\n",
    "            output = self.user_forward(uids, iids)\n",
    "            \n",
    "        return output\n",
    "\n",
    "    # group forwarding\n",
    "    def group_forward(self, gids, iids):\n",
    "        \n",
    "        # group_embeds = Variable(torch.Tensor())\n",
    "        group_embeddeds = torch.empty(0).to(DEVICE)\n",
    "        \n",
    "        # generate embedding vector for item\n",
    "        item_embeddeds = self.item_embedding(\n",
    "            iids.clone().type(torch.long).unsqueeze(dim=1).to(DEVICE)) # shape: (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        \n",
    "        # get attentive group embedding\n",
    "        for gid, iid in zip(gids, iids):\n",
    "            member_uids = self.group_members[gid.item()]\n",
    "            \n",
    "            # generate user embedding vector\n",
    "            member_embeddeds = self.user_embedding(\n",
    "                torch.tensor(member_uids, dtype=torch.long).unsqueeze(dim=1).to(DEVICE)) # shape: (num_member, 1, embedding_dim)\n",
    "            \n",
    "            # generate item embedding vector\n",
    "            one_items = [iid.item() for _ in member_uids]\n",
    "            one_item_embeddeds = self.item_embedding(\n",
    "                torch.tensor(one_items, dtype=torch.long).unsqueeze(dim=1).to(DEVICE)) # shape: (num_member, 1, embedding_dim)\n",
    "            \n",
    "            # get attentive weights for each user-item pair\n",
    "            user_item_embeddeds = torch.cat((member_embeddeds, one_item_embeddeds), dim=2) # shape: (num_member, 1, 2 * embedding_dim)\n",
    "            user_item_embeddeds = user_item_embeddeds.squeeze(dim=1) # shape: (num_member, 2 * embedding_dim)\n",
    "            attentive_weights = self.attention(user_item_embeddeds) # shape: (num_member, 1)\n",
    "            \n",
    "            # aggregation\n",
    "            member_embeddeds = member_embeddeds.squeeze(dim=1) # shape: (num_member, embedding_dim)\n",
    "            aggregated_user_item_embeddeds = torch.matmul(attentive_weights, member_embeddeds) # shape: (1, embedding_dim)\n",
    "            \n",
    "            # generate group-item embedding vector\n",
    "            group_embedded = self.group_embedding(torch.tensor([[gid.item()]], dtype=torch.long).to(DEVICE)) # shape: (1, embedding_dim)\n",
    "            \n",
    "            # group embedding = user embedding aggregation + group preference embedding\n",
    "            aggregated_all_embedded = aggregated_user_item_embeddeds + group_embedded # shape: (1, embedding_dim)\n",
    "            \n",
    "            aggregated_all_embedded = aggregated_all_embedded.squeeze(dim=0) # shape: (embedding_dim, )\n",
    "            \n",
    "            # append group embedding\n",
    "            group_embeddeds = torch.cat((group_embeddeds, aggregated_all_embedded), dim=0) # shape: (batch_size, embedding_dim)\n",
    "            \n",
    "        item_embeddeds = item_embeddeds.squeeze(dim=1) # shape: (batch_size, embedding_dim)\n",
    "            \n",
    "        # element-wise product: group-item interaction\n",
    "        interacted_embeddeds = torch.mul(group_embeddeds, item_embeddeds) # shape: (batch_size, embedding_dim)\n",
    "        \n",
    "        # pooling: [group x item, group, item]\n",
    "        pooled_embeddeds = torch.cat((interacted_embeddeds, group_embeddeds, item_embeddeds), dim=1) # shape: (batch_size, 3*embedding_dim)\n",
    "        \n",
    "        y = torch.sigmoid(self.predict(pooled_embeddeds))\n",
    "        return y\n",
    "\n",
    "    # user forwarding\n",
    "    def user_forward(self, uids, iids):\n",
    "        \n",
    "        # uids.shape: (batch_size, )\n",
    "        # iids.shape: (batch_size, )\n",
    "        \n",
    "        # generate user embedding vectors\n",
    "        user_embeddeds = self.user_embedding(\n",
    "            uids.clone().type(torch.long).unsqueeze(dim=1).to(DEVICE)) # (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # generate item embedding vectors\n",
    "        item_embeddeds = self.item_embedding(\n",
    "            iids.clone().type(torch.long).unsqueeze(dim=1).to(DEVICE)) # (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # element-wise product: user-item interactions\n",
    "        interacted_embeddeds = torch.mul(user_embeddeds, item_embeddeds) # (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # pooling: [user x item, group, item]\n",
    "        pooled_embeddeds = torch.cat((interacted_embeddeds, user_embeddeds, item_embeddeds), dim=2) # (batch_size, 1, 3 * embedding_dim)\n",
    "        \n",
    "        # reshape x from (batch_size, 1, 3 * embedding_dim) to (batch_size, 3 * embedding_dim)\n",
    "        pooled_embeddeds = pooled_embeddeds.squeeze(dim=1)\n",
    "        \n",
    "        y = torch.sigmoid(self.predict(pooled_embeddeds))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, uid_piid_list, niids_list, K, input_type):\n",
    "    \"\"\"\n",
    "    Evaluate the performance (Hit_Ratio, NDCG) of top-K recommendation\n",
    "    Return: score of each test rating.\n",
    "    \"\"\"\n",
    "    \n",
    "    # uid_piid_list shape: (testset size, 2)\n",
    "    # niids_list shape: (testset size, 100)\n",
    "    \n",
    "    testset_size = len(uid_piid_list)\n",
    "    \n",
    "    hits, ndcgs = [], []\n",
    "\n",
    "    # for printing progress\n",
    "    max_percentage = -1\n",
    "    \n",
    "    for idx in range(testset_size):\n",
    "        \n",
    "        # print progress\n",
    "        if idx%10 == 0:\n",
    "            percentage = idx*100/testset_size\n",
    "            if int(percentage/10) > max_percentage:\n",
    "                max_percentage = int(percentage/10)\n",
    "                print(\"{:.1f}%\".format(percentage), end=\" \")\n",
    "        \n",
    "        (hr, ndcg) = evaluate_one_example(model, uid_piid_list, niids_list, K, input_type, idx)\n",
    "        \n",
    "        hits.append(hr)\n",
    "        ndcgs.append(ndcg)\n",
    "        \n",
    "    print(\"\") # line-break\n",
    "        \n",
    "    return (hits, ndcgs)\n",
    "\n",
    "\n",
    "def evaluate_one_example(model, uid_piid_list, niids_list, K, input_type, idx):\n",
    "    \n",
    "    uid, piid = uid_piid_list[idx]\n",
    "    \n",
    "    # items to be predicted\n",
    "    iids = torch.tensor(niids_list[idx] + [piid], dtype=torch.long).to(DEVICE)\n",
    "    uids = torch.full(iids.shape, uid, dtype=torch.long).to(DEVICE)\n",
    "    \n",
    "    # store prediction scores\n",
    "    iid_scores = {}\n",
    "\n",
    "    if input_type == 'group':\n",
    "        predictions = model(uids, None, iids)\n",
    "    elif input_type == 'user':\n",
    "        predictions = model(None, uids, iids)\n",
    "\n",
    "    for idx in range(len(iids)):\n",
    "        iid = iids[idx]\n",
    "        iid_scores[iid] = torch.flatten(predictions)[idx]\n",
    "\n",
    "    # Evaluate top rank list\n",
    "    top_k_iids = heapq.nlargest(K, iid_scores, key=iid_scores.get)\n",
    "    \n",
    "    hr = evaluate_HR(top_k_iids, piid)\n",
    "    ndcg = evaluate_NDCG(top_k_iids, piid)\n",
    "    \n",
    "    return (hr, ndcg)\n",
    "\n",
    "def evaluate_HR(top_k_iids, piid):\n",
    "    \n",
    "    for iid in top_k_iids:\n",
    "        if iid == piid:\n",
    "            return 1\n",
    "        \n",
    "    return 0\n",
    "\n",
    "def evaluate_NDCG(top_k_iids, piid):\n",
    "    \n",
    "    for idx in range(len(top_k_iids)):\n",
    "        iid = top_k_iids[idx]\n",
    "        \n",
    "        if iid == piid:\n",
    "            return math.log(2) / math.log(idx+2)\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, uid_piid_list, niids_list, K, input_type):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    (hrs, ndcgs) = evaluate_model(model, uid_piid_list, niids_list, K, input_type)\n",
    "    avg_hr, avg_ndcg = np.mean(hrs), np.mean(ndcgs)\n",
    "    \n",
    "    return avg_hr, avg_ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training procedure\n",
    "def training(model, dataloader, epoch_id, config, input_type):\n",
    "    \n",
    "    # user trainning\n",
    "    learning_rate = config.lr\n",
    "\n",
    "    # lr decay: halve for every five epochs\n",
    "    for _ in range(0, epoch_id, 5):\n",
    "        learning_rate /= 2\n",
    "\n",
    "    # create optimizer\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"Epoch {}, lr = {}, input_type = {}\".format(epoch_id, learning_rate, input_type))\n",
    "\n",
    "    losses = []\n",
    "    for batch_id, (uids, piids_niids) in enumerate(dataloader):\n",
    "        \n",
    "        if batch_id%10 == 0:\n",
    "            print(\".\", end=\"\")\n",
    "        \n",
    "        # Data Load\n",
    "        p_iids = piids_niids[:, 0]\n",
    "        n_iids = piids_niids[:, 1]\n",
    "        \n",
    "        # Forward\n",
    "        if input_type == 'user':\n",
    "            positive_predictions = model(None, uids, p_iids)\n",
    "            negative_predictions = model(None, uids, n_iids)\n",
    "        elif input_type == 'group':\n",
    "            positive_predictions = model(uids, None, p_iids)\n",
    "            negative_predictions = model(uids, None, n_iids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = torch.mean((positive_predictions - negative_predictions -1) **2)\n",
    "        \n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # record loss history\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    print(\"\") # line-break\n",
    "    \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGREE at embedding size: 6, epoch: 5, NDCG % HR: Top-5\n"
     ]
    }
   ],
   "source": [
    "agree = AGREE(num_user, num_item, num_group, config.embedding_size, dataset.group_members, config.drop_ratio).to(DEVICE)\n",
    "print(\"AGREE at embedding size: {}, epoch: {}, NDCG % HR: Top-{}\".format(config.embedding_size, config.epoch, config.top_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, lr = 5e-06, input_type = user\n",
      "...............................................................................................................................................................................\n",
      "Epoch 0, lr = 5e-06, input_type = group\n",
      ".................................................................................................................................................................................\n",
      "Losses: 0.9993191635240406, 0.9961314196277924\n",
      "0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.1432, average Top-5 NDCG: 0.0931\n",
      "0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.1138, average Top-5 NDCG: 0.0707\n",
      "Epoch 1, lr = 2.5e-06, input_type = user\n",
      "...............................................................................................................................................................................\n",
      "Epoch 1, lr = 2.5e-06, input_type = group\n",
      ".................................................................................................................................................................................\n",
      "Losses: 0.9909340673098576, 0.9891235741751225\n",
      "0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.2163, average Top-5 NDCG: 0.1416\n",
      "0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.1834, average Top-5 NDCG: 0.1116\n",
      "Epoch 2, lr = 2.5e-06, input_type = user\n",
      "...............................................................................................................................................................................\n",
      "Epoch 2, lr = 2.5e-06, input_type = group\n",
      ".................................................................................................................................................................................\n",
      "Losses: 0.985144954728862, 0.9828256085337326\n",
      "0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.3010, average Top-5 NDCG: 0.1980\n",
      "0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.2572, average Top-5 NDCG: 0.1640\n",
      "Epoch 3, lr = 2.5e-06, input_type = user\n",
      "...............................................................................................................................................................................\n",
      "Epoch 3, lr = 2.5e-06, input_type = group\n",
      ".................................................................................................................................................................................\n",
      "Losses: 0.9788145056714728, 0.9750502145066984\n",
      "0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.3824, average Top-5 NDCG: 0.2511\n",
      "0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.3386, average Top-5 NDCG: 0.2191\n",
      "Epoch 4, lr = 2.5e-06, input_type = user\n",
      "...............................................................................................................................................................................\n",
      "Epoch 4, lr = 2.5e-06, input_type = group\n",
      ".................................................................................................................................................................................\n",
      "Losses: 0.9714556552248088, 0.966421520520448\n",
      "0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.4415, average Top-5 NDCG: 0.2910\n",
      "0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4028, average Top-5 NDCG: 0.2572\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "for epoch in range(config.epoch):\n",
    "    \n",
    "    # set the model in train mode (some network like dropout will behave differently in train mode / evaluaiton mode)\n",
    "    agree.train(mode=True)\n",
    "    \n",
    "    # train the model using user interactions\n",
    "    user_loss = training(agree,\n",
    "                 dataset.get_user_dataloader(batch_size=config.batch_size, shuffle=True, num_negatives=config.num_negatives),\n",
    "                 epoch, config, 'user')\n",
    "    \n",
    "    history.append(user_loss)\n",
    "    \n",
    "    # train the model using group & members interactions\n",
    "    group_loss = training(agree,\n",
    "                    dataset.get_group_dataloader(batch_size=config.batch_size, shuffle=True, num_negatives=config.num_negatives),\n",
    "                    epoch, config, 'group')\n",
    "    \n",
    "    history.append(group_loss)\n",
    "    \n",
    "    print(\"Losses: {}, {}\".format(user_loss, group_loss))\n",
    "    \n",
    "    avg_user_hr, avg_user_ndcg = evaluation(agree, dataset.test_user_list, dataset.test_user_negative_list, config.top_k, 'user')\n",
    "    print(\"User-- average Top-{} Hit Rate: {:.4f}, average Top-{} NDCG: {:.4f}\"\n",
    "          .format(config.top_k, avg_user_hr, config.top_k, avg_user_ndcg))\n",
    "    \n",
    "    avg_group_hr, avg_group_ndcg = evaluation(agree, dataset.test_group_list, dataset.test_group_negative_list, config.top_k, 'group')\n",
    "    print(\"Group-- average Top-{} Hit Rate: {:.4f}, average Top-{} NDCG: {:.4f}\"\n",
    "          .format(config.top_k, avg_group_hr, config.top_k, avg_group_ndcg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training(name=\"my_training\", embedding_dim=6, train_epoch=30,\n",
    "        num_negatives=4, batch_size=512, lr=0.00005, drop_ratio=0.2, top_k=5):\n",
    "    \n",
    "    # create dir to store configs, models and training history\n",
    "    training_directory = 'models/'+ name\n",
    "    os.mkdir(training_directory)\n",
    "    \n",
    "    # create configuration object\n",
    "    config = Config(embedding_size=embedding_dim, epoch=train_epoch,\n",
    "        num_negatives=num_negatives, batch_size=batch_size, lr=lr, drop_ratio=drop_ratio, top_k=top_k)\n",
    "    \n",
    "    # save configs to file\n",
    "    config.export_json(training_directory + \"/config.json\")\n",
    "    \n",
    "    # load and parse dataset\n",
    "    dataset = CAMRa2011Dataset(config.data_dir)\n",
    "    num_group, num_user, num_item = dataset.get_sizes()\n",
    "    print(\"number of groups: {}\".format(num_group))\n",
    "    print(\"number of users: {}\".format(num_user))\n",
    "    print(\"number of items: {}\".format(num_item))\n",
    "    \n",
    "    # create model object\n",
    "    agree = AGREE(num_user, num_item, num_group, config.embedding_size, dataset.group_members, config.drop_ratio).to(DEVICE)\n",
    "    print(\"AGREE at embedding size: {}, epoch: {}, NDCG & HR: Top-{}\".format(config.embedding_size, config.epoch, config.top_k))\n",
    "    \n",
    "    # start training\n",
    "    all_history = []\n",
    "    for epoch in range(config.epoch):\n",
    "\n",
    "        history = { 'loss': {}, 'hr': {}, 'ndcg': {} }\n",
    "        \n",
    "        # set the model in train mode (some network like dropout will behave differently in train mode / evaluaiton mode)\n",
    "        agree.train(mode=True)\n",
    "\n",
    "        # train the model using user interactions\n",
    "        user_loss = training(agree,\n",
    "                     dataset.get_user_dataloader(batch_size=config.batch_size, shuffle=True, num_negatives=config.num_negatives),\n",
    "                     epoch, config, 'user')\n",
    "\n",
    "        history['loss']['user'] = user_loss\n",
    "\n",
    "        # train the model using group & members interactions\n",
    "        group_loss = training(agree,\n",
    "                        dataset.get_group_dataloader(batch_size=config.batch_size, shuffle=True, num_negatives=config.num_negatives),\n",
    "                        epoch, config, 'group')\n",
    "\n",
    "        history['loss']['group'] = group_loss\n",
    "\n",
    "        print(\"Losses: {}\".format(history['loss']))\n",
    "\n",
    "        # evaluation\n",
    "        avg_user_hr, avg_user_ndcg = evaluation(agree, dataset.test_user_list, dataset.test_user_negative_list, config.top_k, 'user')\n",
    "        print(\"User-- average Top-{} Hit Rate: {:.4f}, average Top-{} NDCG: {:.4f}\"\n",
    "              .format(config.top_k, avg_user_hr, config.top_k, avg_user_ndcg))\n",
    "        history['hr']['user'] = avg_user_hr\n",
    "        history['ndcg']['user'] = avg_user_ndcg\n",
    "\n",
    "        avg_group_hr, avg_group_ndcg = evaluation(agree, dataset.test_group_list, dataset.test_group_negative_list, config.top_k, 'group')\n",
    "        print(\"Group-- average Top-{} Hit Rate: {:.4f}, average Top-{} NDCG: {:.4f}\"\n",
    "              .format(config.top_k, avg_group_hr, config.top_k, avg_group_ndcg))\n",
    "        history['hr']['group'] = avg_group_hr\n",
    "        history['ndcg']['group'] = avg_group_ndcg\n",
    "        \n",
    "        all_history.append(history)\n",
    "        \n",
    "        # save model to file\n",
    "        torch.save(agree.state_dict(), training_directory+ \"/agree_e{}.pt\".format(epoch))\n",
    "        \n",
    "    # save training & evaluation result to file\n",
    "    with open(training_directory+\"/history.json\", 'w') as file:\n",
    "            file.write(json.dumps(all_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of groups: 290\n",
      "number of users: 602\n",
      "number of items: 7710\n",
      "AGREE at embedding size: 6, epoch: 20, NDCG & HR: Top-5\n",
      "Epoch 0, lr = 5e-05, input_type = user\n",
      "........................................................................................\n",
      "Epoch 0, lr = 5e-05, input_type = group\n",
      ".........................................................................................\n",
      "Losses: {'user': 0.9272214188592112, 'group': 0.7656031855795359}\n",
      "0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.6090, average Top-5 NDCG: 0.4045\n",
      "0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5703, average Top-5 NDCG: 0.3835\n",
      "Epoch 1, lr = 2.5e-05, input_type = user\n",
      "........................................................................................\n",
      "Epoch 1, lr = 2.5e-05, input_type = group\n",
      ".........................................................................................\n",
      "Losses: {'user': 0.6247747577982582, 'group': 0.5463321560086781}\n",
      "0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.6236, average Top-5 NDCG: 0.4150\n",
      "0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5745, average Top-5 NDCG: 0.3892\n",
      "Epoch 2, lr = 2.5e-05, input_type = user\n",
      "........................................................................................\n",
      "Epoch 2, lr = 2.5e-05, input_type = group\n",
      ".........................................................................................\n",
      "Losses: {'user': 0.4877299404185206, 'group': 0.43155189124057564}\n",
      "0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.6262, average Top-5 NDCG: 0.4195\n",
      "0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5793, average Top-5 NDCG: 0.3949\n",
      "Epoch 3, lr = 2.5e-05, input_type = user\n",
      "........................................................................................\n",
      "Epoch 3, lr = 2.5e-05, input_type = group\n",
      ".........................................................................................\n",
      "Losses: {'user': 0.405435414452029, 'group': 0.36645950801526783}\n",
      "0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.6262, average Top-5 NDCG: 0.4205\n",
      "0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5800, average Top-5 NDCG: 0.3975\n",
      "Epoch 4, lr = 2.5e-05, input_type = user\n",
      "........................................................................................\n",
      "Epoch 4, lr = 2.5e-05, input_type = group\n",
      ".........................................................................................\n",
      "Losses: {'user': 0.3585997889802985, 'group': 0.33034924058313403}\n",
      "0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.6282, average Top-5 NDCG: 0.4232\n",
      "0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5855, average Top-5 NDCG: 0.4003\n",
      "Epoch 5, lr = 2.5e-05, input_type = user\n",
      "........................................................................................\n",
      "Epoch 5, lr = 2.5e-05, input_type = group\n",
      ".........................................................................................\n",
      "Losses: {'user': 0.333450619702481, 'group': 0.3091278758141023}\n",
      "0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.6246, average Top-5 NDCG: 0.4219\n",
      "0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5855, average Top-5 NDCG: 0.3992\n",
      "Epoch 6, lr = 1.25e-05, input_type = user\n",
      "........................................................................................\n",
      "Epoch 6, lr = 1.25e-05, input_type = group\n",
      ".........................................................................................\n",
      "Losses: {'user': 0.31820200148381683, 'group': 0.30047240631737315}\n",
      "0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.6249, average Top-5 NDCG: 0.4212\n",
      "0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5855, average Top-5 NDCG: 0.3989\n",
      "Epoch 7, lr = 1.25e-05, input_type = user\n",
      "........................................................................................\n",
      "Epoch 7, lr = 1.25e-05, input_type = group\n",
      ".........................................................................................\n",
      "Losses: {'user': 0.31194245071923976, 'group': 0.2943039859870235}\n",
      "0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.6272, average Top-5 NDCG: 0.4234\n",
      "0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5876, average Top-5 NDCG: 0.4000\n",
      "Epoch 8, lr = 1.25e-05, input_type = user\n",
      "........................................................................................\n",
      "Epoch 8, lr = 1.25e-05, input_type = group\n",
      ".........................................................................................\n",
      "Losses: {'user': 0.3067825683126188, 'group': 0.29007758582423143}\n",
      "0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.6259, average Top-5 NDCG: 0.4226\n",
      "0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5855, average Top-5 NDCG: 0.3978\n",
      "Epoch 9, lr = 1.25e-05, input_type = user\n",
      "........................................................................................\n",
      "Epoch 9, lr = 1.25e-05, input_type = group\n",
      ".........................................................................................\n",
      "Losses: {'user': 0.3028276412485666, 'group': 0.286305950800217}\n",
      "0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.6252, average Top-5 NDCG: 0.4231\n",
      "0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5855, average Top-5 NDCG: 0.3974\n",
      "Epoch 10, lr = 1.25e-05, input_type = user\n",
      "........................................................................................\n",
      "Epoch 10, lr = 1.25e-05, input_type = group\n",
      ".........................................................................................\n",
      "Losses: {'user': 0.2999125640813764, 'group': 0.2826597133606977}\n",
      "0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.6249, average Top-5 NDCG: 0.4226\n",
      "0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5869, average Top-5 NDCG: 0.3979\n",
      "Epoch 11, lr = 6.25e-06, input_type = user\n",
      "........................................................................................\n",
      "Epoch 11, lr = 6.25e-06, input_type = group\n",
      ".........................................................................................\n",
      "Losses: {'user': 0.2970742189263588, 'group': 0.281345500594912}\n",
      "0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.6246, average Top-5 NDCG: 0.4222\n",
      "0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5876, average Top-5 NDCG: 0.3982\n",
      "Epoch 12, lr = 6.25e-06, input_type = user\n",
      "........................................................................................\n",
      "Epoch 12, lr = 6.25e-06, input_type = group\n",
      ".................................."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-703dfb2aa069>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m start_training(name=\"basic\", embedding_dim=6, train_epoch=20,\n\u001b[1;32m----> 2\u001b[1;33m         num_negatives=4, batch_size=512, lr=0.00005, drop_ratio=0.2, top_k=5)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-18464159350a>\u001b[0m in \u001b[0;36mstart_training\u001b[1;34m(name, embedding_dim, train_epoch, num_negatives, batch_size, lr, drop_ratio, top_k)\u001b[0m\n\u001b[0;32m     43\u001b[0m         group_loss = training(agree,\n\u001b[0;32m     44\u001b[0m                         \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_group_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_negatives\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_negatives\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                         epoch, config, 'group')\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'group'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-dd2b9b754a82>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(model, dataloader, epoch_id, config, input_type)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m# back propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\envs\\attentive\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\envs\\attentive\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_training(name=\"basic\", embedding_dim=6, train_epoch=20,\n",
    "        num_negatives=4, batch_size=512, lr=0.00005, drop_ratio=0.2, top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
