{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages initialization\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import heapq\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self, embedding_size=6, epoch=30,\n",
    "                 num_negatives=4, batch_size=512, lr=0.00005, drop_ratio=0.2, top_k=5):\n",
    "        \n",
    "        self.data_dir = './data/CAMRa2011/'\n",
    "        self.embedding_size = embedding_size\n",
    "        self.epoch = epoch\n",
    "        self.num_negatives = num_negatives\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.top_k = top_k\n",
    "        \n",
    "    def export_json(self, file_path):\n",
    "        \n",
    "        config_dict = {\n",
    "            'embedding_size': self.embedding_size,\n",
    "            'epoch': self.epoch,\n",
    "            'num_negatives': self.num_negatives,\n",
    "            'batch_size': self.batch_size,\n",
    "            'lr': self.lr,\n",
    "            'drop_ratio': self.drop_ratio,\n",
    "            'top_k': self.top_k\n",
    "        }\n",
    "        \n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(json.dumps(config_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to process dataset:\n",
    "\n",
    "1. combine user train set with user test set, group train set with group test set\n",
    "2. compute average user's rating count & average group's rating count\n",
    "3. filter out examples which its users/groups has total rating count lower than 30% of avg\n",
    "4. generate negative instances: pick items that haven't been interacted by users/groups\n",
    "5. split into trainset & testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or:\n",
    "\n",
    "1. generate negative instances using items that haven't been interacted by users/groups\n",
    "2. recalculate user number and group number and item number from both train & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAMRa2011Dataset(object):\n",
    "    \"\"\"CAMRa2011 dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_dir):\n",
    "        \n",
    "        self.pathes = {\n",
    "            'train': {\n",
    "                'user': dataset_dir + \"userRatingTrain.txt\",\n",
    "                'group': dataset_dir + \"groupRatingTrain.txt\"\n",
    "            },\n",
    "            'test': {\n",
    "                'user': dataset_dir + \"userRatingTest.txt\",\n",
    "                'user_negative': dataset_dir + \"userRatingNegative.txt\",\n",
    "                'group': dataset_dir + \"groupRatingTest.txt\",\n",
    "                'group_negative': dataset_dir + \"groupRatingNegative.txt\",\n",
    "            },\n",
    "            'group_user': dataset_dir + \"groupMember.txt\"\n",
    "        }\n",
    "        \n",
    "        # number of items & users (needed by embedding layer)\n",
    "        self.max_iid = 0\n",
    "        \n",
    "        # get the mapping of users and groups\n",
    "        # format: {gid: [uid, uid, ..], gid: [uid, uid, ..], ...}\n",
    "        self.group_members = self.get_group_user_mapping()\n",
    "        \n",
    "        # get interaction matrix from uid-iid training set\n",
    "        # train_user_matrix[uid, iid] = [1 | 0]\n",
    "        self.train_user_matrix = self.get_interaction_matrix(self.pathes['train']['user'])\n",
    "        \n",
    "        # format: [[uid, iid], [uid, iid], ...]\n",
    "        # only pairs of users & items have interactions would appear in the list\n",
    "        self.test_user_list = self.get_interaction_list(self.pathes['test']['user'])\n",
    "        \n",
    "        # format: [[uid, ...], [uid, ...], ...]\n",
    "        # test_user_negative_list & test_user_list follow the same order\n",
    "        # e.g. test_user_negative_list[0] is for test_user_list[0]\n",
    "        self.test_user_negative_list = self.get_negatives(self.pathes['test']['user_negative'])\n",
    "        \n",
    "        # get interaction matrix from gid-iid training set\n",
    "        self.train_group_matrix = self.get_interaction_matrix(self.pathes['train']['group'])\n",
    "\n",
    "        # pairs of group & item to be tested\n",
    "        self.test_group_list = self.get_interaction_list(self.pathes['test']['group'])\n",
    "\n",
    "        self.test_group_negative_list = self.get_negatives(self.pathes['test']['group_negative'])\n",
    "        \n",
    "        \n",
    "    def get_user_dataloader(self, batch_size=256, shuffle=True, num_negatives=6):\n",
    "        \n",
    "        users, positives_negatives = self.get_train_instances(self.train_user_matrix, num_negatives=num_negatives)\n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(users, dtype=torch.float),\n",
    "            torch.tensor(positives_negatives, dtype=torch.float))\n",
    "        \n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        \n",
    "        return loader\n",
    "    \n",
    "    def get_group_dataloader(self, batch_size=256, shuffle=True, num_negatives=6):\n",
    "        \n",
    "        groups, positives_negatives = self.get_train_instances(self.train_group_matrix, num_negatives=num_negatives)\n",
    "        \n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(groups, dtype=torch.float),\n",
    "            torch.tensor(positives_negatives, dtype=torch.float))\n",
    "        \n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        \n",
    "        return loader\n",
    "    \n",
    "    # get number of groups, users and items\n",
    "    def get_sizes(self):\n",
    "        group_size = len(self.group_members)\n",
    "        \n",
    "        num_user, _ = self.train_user_matrix.shape\n",
    "        \n",
    "        return (group_size, num_user, self.max_iid+1)\n",
    "        \n",
    "    def get_train_instances(self, interaction_matrix, num_negatives=6):\n",
    "        \n",
    "        users, positive_items, negative_items = [], [], []\n",
    "        \n",
    "        for (uid, iid) in interaction_matrix.keys():\n",
    "            \n",
    "            # positive instance\n",
    "            for _ in range(num_negatives):\n",
    "                \n",
    "                # positive instances\n",
    "                positive_items.append(iid)\n",
    "                \n",
    "                # negative instances ---> need to be fixed\n",
    "                negative_iid = np.random.randint(self.max_iid+1)\n",
    "                while (uid, negative_iid) in interaction_matrix:\n",
    "                    negative_iid = np.random.randint(self.max_iid+1) # re-generate an negative iid\n",
    "                negative_items.append(negative_iid)\n",
    "                \n",
    "                # users\n",
    "                users.append(uid)\n",
    "\n",
    "        positives_negatives = [[positive_iid, negative_iid] for positive_iid, negative_iid in zip(positive_items, negative_items)]\n",
    "        \n",
    "        return users, positives_negatives\n",
    "        \n",
    "        \n",
    "    def get_group_user_mapping(self):\n",
    "    \n",
    "        mapping = {}\n",
    "\n",
    "        # read mapping file\n",
    "        with open(self.pathes['group_user'], 'r') as file:\n",
    "\n",
    "            line = file.readline().strip()\n",
    "            while line != None and line != \"\":\n",
    "\n",
    "                # sample line format: [gid] [uid 1],[uid 2],[uid 3],[uid 4]\n",
    "                sequences = line.split(' ')\n",
    "                gid = int(sequences[0])\n",
    "                mapping[gid] = []\n",
    "                for uid in sequences[1].split(','):\n",
    "                    mapping[gid].append(int(uid))\n",
    "                line = file.readline().strip()\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    # parse all interactions in dataset to 2D sparse matrix\n",
    "    def get_interaction_matrix(self, rating_file_path):\n",
    "\n",
    "        # get number of users and items\n",
    "        num_users, num_items = 0, 0\n",
    "        with open(rating_file_path, \"r\") as file:\n",
    "\n",
    "            line = file.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\" \")\n",
    "                uid, iid = int(arr[0]), int(arr[1])\n",
    "\n",
    "                # update num_items if bigger iid appears\n",
    "                num_items = max(num_items, iid)\n",
    "                # update num_items if bigger iid appears\n",
    "                num_users = max(num_users, uid)\n",
    "                \n",
    "                line = file.readline()\n",
    "                \n",
    "        # update the max iid / uid of the whole dataset\n",
    "        self.max_iid = max(self.max_iid, num_items)\n",
    "\n",
    "        # construct interaction matrix\n",
    "        # dok_matrix: Dictionary Of Keys based sparse matrix, an efficient structure for constructing sparse matrices incrementally.\n",
    "        matrix = sparse.dok_matrix((num_users + 1, num_items + 1), dtype=np.float32) # iid and uid starts from 1\n",
    "        with open(rating_file_path, \"r\") as file:\n",
    "            line = file.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\" \")\n",
    "                if len(arr) > 2:\n",
    "                    uid, iid, rating = int(arr[0]), int(arr[1]), int(arr[2])\n",
    "                    if (rating > 0):\n",
    "                        matrix[uid, iid] = 1.0\n",
    "                else:\n",
    "                    uid, iid = int(arr[0]), int(arr[1])\n",
    "                    matrix[uid, iid] = 1.0\n",
    "                line = file.readline()\n",
    "\n",
    "        return matrix\n",
    "    \n",
    "    # parse all interactions in dataset to list\n",
    "    def get_interaction_list(self, rating_file_path):\n",
    "\n",
    "        interaction_list = []\n",
    "        with open(rating_file_path, \"r\") as file:\n",
    "            line = file.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\" \")\n",
    "                uid, iid = int(arr[0]), int(arr[1])\n",
    "                interaction_list.append([uid, iid])\n",
    "                \n",
    "                # update max iid/uid if bigger iid/uid appears\n",
    "                self.max_iid = max(self.max_iid, iid)\n",
    "                \n",
    "                line = file.readline()\n",
    "\n",
    "        return interaction_list\n",
    "\n",
    "    # parse negative sample lists for pairs in test set\n",
    "    # negative samples: the items which never been interacted\n",
    "    # the order of returned sample lists must be paired with test list\n",
    "    def get_negatives(self, file_path):\n",
    "\n",
    "        negative_samples_list = []\n",
    "\n",
    "        with open(file_path, \"r\") as file:\n",
    "\n",
    "            line = file.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\" \")\n",
    "\n",
    "                negative_iids = []\n",
    "                for iid in arr[1:]:\n",
    "                    negative_iids.append(int(iid))\n",
    "\n",
    "                negative_samples_list.append(negative_iids)\n",
    "                line = file.readline()\n",
    "\n",
    "        return negative_samples_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGREE Model (Attentive Group Representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding networks\n",
    "\n",
    "class UserEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_users, embedding_dim):\n",
    "        super(UserEmbeddingLayer, self).__init__()\n",
    "        self.userEmbedding = nn.Embedding(num_users, embedding_dim)\n",
    "\n",
    "    def forward(self, uids):\n",
    "        user_embedded = self.userEmbedding(uids)\n",
    "        return user_embedded\n",
    "    \n",
    "class ItemEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim):\n",
    "        super(ItemEmbeddingLayer, self).__init__()\n",
    "        self.itemEmbedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "    def forward(self, iids):\n",
    "        item_embedded = self.itemEmbedding(iids)\n",
    "        return item_embedded\n",
    "    \n",
    "class GroupEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_groups, embedding_dim):\n",
    "        super(GroupEmbeddingLayer, self).__init__()\n",
    "        self.groupEmbedding = nn.Embedding(num_groups, embedding_dim)\n",
    "\n",
    "    def forward(self, gids):\n",
    "        group_embedded = self.groupEmbedding(gids)\n",
    "        return group_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention network\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, drop_ratio=0):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(embedding_dim, 16)\n",
    "        self.linear2 = nn.Linear(16, 1)\n",
    "        self.dropout = nn.Dropout(p=drop_ratio)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        weights = F.softmax(x.view(1, -1), dim=1)\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final layers of AGREE for prediction\n",
    "\n",
    "class PredictLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, drop_ratio=0):\n",
    "        super(PredictLayer, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(embedding_dim, 8)\n",
    "        self.dropout = nn.Dropout(p=drop_ratio)\n",
    "        self.linear2 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        out = self.linear2(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGREE model\n",
    "\n",
    "class AGREE(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_groups, embedding_dim, group_member_mapping, drop_ratio):\n",
    "        super(AGREE, self).__init__()\n",
    "        \n",
    "        self.user_embedding = UserEmbeddingLayer(num_users, embedding_dim)\n",
    "        self.item_embedding = ItemEmbeddingLayer(num_items, embedding_dim)\n",
    "        self.group_embedding = GroupEmbeddingLayer(num_groups, embedding_dim)\n",
    "        self.attention = AttentionLayer(2 * embedding_dim, drop_ratio)\n",
    "        self.predict = PredictLayer(3 * embedding_dim, drop_ratio)\n",
    "        \n",
    "        self.group_members = group_member_mapping\n",
    "        \n",
    "        self.num_users = num_users\n",
    "        self.num_groups = num_groups\n",
    "        self.num_items = num_items\n",
    "        \n",
    "        # initial model's parameters\n",
    "        for m in self.modules():\n",
    "            \n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=1) # normal distribution\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.xavier_normal_(m.weight) # Glorot initialization\n",
    "\n",
    "    def forward(self, gids, uids, iids):\n",
    "        \n",
    "        # group prediction\n",
    "        if (gids is not None) and (uids is None):\n",
    "            output = self.group_forward(gids, iids)\n",
    "        # user prediction\n",
    "        else:\n",
    "            output = self.user_forward(uids, iids)\n",
    "            \n",
    "        return output\n",
    "\n",
    "    # group forwarding\n",
    "    def group_forward(self, gids, iids):\n",
    "        \n",
    "        # group_embeds = Variable(torch.Tensor())\n",
    "        group_embeddeds = torch.empty(0).to(DEVICE)\n",
    "        \n",
    "        # generate embedding vector for item\n",
    "        item_embeddeds = self.item_embedding(\n",
    "            iids.clone().type(torch.long).unsqueeze(dim=1).to(DEVICE)) # shape: (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        \n",
    "        # get attentive group embedding\n",
    "        for gid, iid in zip(gids, iids):\n",
    "            member_uids = self.group_members[gid.item()]\n",
    "            \n",
    "            # generate user embedding vector\n",
    "            member_embeddeds = self.user_embedding(\n",
    "                torch.tensor(member_uids, dtype=torch.long).unsqueeze(dim=1).to(DEVICE)) # shape: (num_member, 1, embedding_dim)\n",
    "            \n",
    "            # generate item embedding vector\n",
    "            one_items = [iid.item() for _ in member_uids]\n",
    "            one_item_embeddeds = self.item_embedding(\n",
    "                torch.tensor(one_items, dtype=torch.long).unsqueeze(dim=1).to(DEVICE)) # shape: (num_member, 1, embedding_dim)\n",
    "            \n",
    "            # get attentive weights for each user-item pair\n",
    "            user_item_embeddeds = torch.cat((member_embeddeds, one_item_embeddeds), dim=2) # shape: (num_member, 1, 2 * embedding_dim)\n",
    "            user_item_embeddeds = user_item_embeddeds.squeeze(dim=1) # shape: (num_member, 2 * embedding_dim)\n",
    "            attentive_weights = self.attention(user_item_embeddeds) # shape: (num_member, 1)\n",
    "            \n",
    "            # aggregation\n",
    "            member_embeddeds = member_embeddeds.squeeze(dim=1) # shape: (num_member, embedding_dim)\n",
    "            aggregated_user_item_embeddeds = torch.matmul(attentive_weights, member_embeddeds) # shape: (1, embedding_dim)\n",
    "            \n",
    "            # generate group-item embedding vector\n",
    "            group_embedded = self.group_embedding(torch.tensor([[gid.item()]], dtype=torch.long).to(DEVICE)) # shape: (1, embedding_dim)\n",
    "            \n",
    "            # group embedding = user embedding aggregation + group preference embedding\n",
    "            aggregated_all_embedded = aggregated_user_item_embeddeds + group_embedded # shape: (1, embedding_dim)\n",
    "            \n",
    "            aggregated_all_embedded = aggregated_all_embedded.squeeze(dim=0) # shape: (embedding_dim, )\n",
    "            \n",
    "            # append group embedding\n",
    "            group_embeddeds = torch.cat((group_embeddeds, aggregated_all_embedded), dim=0) # shape: (batch_size, embedding_dim)\n",
    "            \n",
    "        item_embeddeds = item_embeddeds.squeeze(dim=1) # shape: (batch_size, embedding_dim)\n",
    "            \n",
    "        # element-wise product: group-item interaction\n",
    "        interacted_embeddeds = torch.mul(group_embeddeds, item_embeddeds) # shape: (batch_size, embedding_dim)\n",
    "        \n",
    "        # pooling: [group x item, group, item]\n",
    "        pooled_embeddeds = torch.cat((interacted_embeddeds, group_embeddeds, item_embeddeds), dim=1) # shape: (batch_size, 3*embedding_dim)\n",
    "        \n",
    "        y = torch.sigmoid(self.predict(pooled_embeddeds))\n",
    "        return y\n",
    "\n",
    "    # user forwarding\n",
    "    def user_forward(self, uids, iids):\n",
    "        \n",
    "        # uids.shape: (batch_size, )\n",
    "        # iids.shape: (batch_size, )\n",
    "        \n",
    "        # generate user embedding vectors\n",
    "        user_embeddeds = self.user_embedding(\n",
    "            uids.clone().type(torch.long).unsqueeze(dim=1).to(DEVICE)) # (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # generate item embedding vectors\n",
    "        item_embeddeds = self.item_embedding(\n",
    "            iids.clone().type(torch.long).unsqueeze(dim=1).to(DEVICE)) # (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # element-wise product: user-item interactions\n",
    "        interacted_embeddeds = torch.mul(user_embeddeds, item_embeddeds) # (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # pooling: [user x item, group, item]\n",
    "        pooled_embeddeds = torch.cat((interacted_embeddeds, user_embeddeds, item_embeddeds), dim=2) # (batch_size, 1, 3 * embedding_dim)\n",
    "        \n",
    "        # reshape x from (batch_size, 1, 3 * embedding_dim) to (batch_size, 3 * embedding_dim)\n",
    "        pooled_embeddeds = pooled_embeddeds.squeeze(dim=1)\n",
    "        \n",
    "        y = torch.sigmoid(self.predict(pooled_embeddeds))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GREE model: using average strategy to aggregate group members' preference\n",
    "\n",
    "class GREE(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_groups, embedding_dim, group_member_mapping, drop_ratio):\n",
    "        super(GREE, self).__init__()\n",
    "        \n",
    "        self.user_embedding = UserEmbeddingLayer(num_users, embedding_dim)\n",
    "        self.item_embedding = ItemEmbeddingLayer(num_items, embedding_dim)\n",
    "        self.group_embedding = GroupEmbeddingLayer(num_groups, embedding_dim)\n",
    "        self.predict = PredictLayer(3 * embedding_dim, drop_ratio)\n",
    "        \n",
    "        self.group_members = group_member_mapping\n",
    "        \n",
    "        self.num_users = num_users\n",
    "        self.num_groups = num_groups\n",
    "        self.num_items = num_items\n",
    "        \n",
    "        # initial model's parameters\n",
    "        for m in self.modules():\n",
    "            \n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=1) # normal distribution\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.xavier_normal_(m.weight) # Glorot initialization\n",
    "\n",
    "    def forward(self, gids, uids, iids):\n",
    "        \n",
    "        # group prediction\n",
    "        if (gids is not None) and (uids is None):\n",
    "            output = self.group_forward(gids, iids)\n",
    "        # user prediction\n",
    "        else:\n",
    "            output = self.user_forward(uids, iids)\n",
    "            \n",
    "        return output\n",
    "\n",
    "    # group forwarding\n",
    "    def group_forward(self, gids, iids):\n",
    "        \n",
    "        # group_embeds = Variable(torch.Tensor())\n",
    "        group_embeddeds = torch.empty(0).to(DEVICE)\n",
    "        \n",
    "        # generate embedding vector for item\n",
    "        item_embeddeds = self.item_embedding(\n",
    "            iids.clone().type(torch.long).unsqueeze(dim=1).to(DEVICE)) # shape: (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        \n",
    "        # get attentive group embedding\n",
    "        for gid, iid in zip(gids, iids):\n",
    "            member_uids = self.group_members[gid.item()]\n",
    "            num_member = len(member_uids)\n",
    "            \n",
    "            # generate user embedding vector\n",
    "            member_embeddeds = self.user_embedding(\n",
    "                torch.tensor(member_uids, dtype=torch.long).unsqueeze(dim=1).to(DEVICE)) # shape: (num_member, 1, embedding_dim)\n",
    "            \n",
    "            # generate item embedding vector\n",
    "            one_items = [iid.item() for _ in member_uids]\n",
    "            one_item_embeddeds = self.item_embedding(\n",
    "                torch.tensor(one_items, dtype=torch.long).unsqueeze(dim=1).to(DEVICE)) # shape: (num_member, 1, embedding_dim)\n",
    "            \n",
    "            # get attentive weights for each user-item pair\n",
    "            user_item_embeddeds = torch.cat((member_embeddeds, one_item_embeddeds), dim=2) # shape: (num_member, 1, 2 * embedding_dim)\n",
    "            user_item_embeddeds = user_item_embeddeds.squeeze(dim=1) # shape: (num_member, 2 * embedding_dim)\n",
    "            \n",
    "            # uniform aggregation weights\n",
    "            attentive_weights = torch.full(\n",
    "                (1, num_member), (1/num_member), dtype=torch.float).to(DEVICE) # shape: (1, num_member)\n",
    "            \n",
    "            # aggregation\n",
    "            member_embeddeds = member_embeddeds.squeeze(dim=1) # shape: (num_member, embedding_dim)\n",
    "            aggregated_user_item_embeddeds = torch.matmul(attentive_weights, member_embeddeds) # shape: (1, embedding_dim)\n",
    "            \n",
    "            # generate group-item embedding vector\n",
    "            group_embedded = self.group_embedding(torch.tensor([[gid.item()]], dtype=torch.long).to(DEVICE)) # shape: (1, embedding_dim)\n",
    "            \n",
    "            # group embedding = user embedding aggregation + group preference embedding\n",
    "            aggregated_all_embedded = aggregated_user_item_embeddeds + group_embedded # shape: (1, embedding_dim)\n",
    "            \n",
    "            aggregated_all_embedded = aggregated_all_embedded.squeeze(dim=0) # shape: (embedding_dim, )\n",
    "            \n",
    "            # append group embedding\n",
    "            group_embeddeds = torch.cat((group_embeddeds, aggregated_all_embedded), dim=0) # shape: (batch_size, embedding_dim)\n",
    "            \n",
    "        item_embeddeds = item_embeddeds.squeeze(dim=1) # shape: (batch_size, embedding_dim)\n",
    "            \n",
    "        # element-wise product: group-item interaction\n",
    "        interacted_embeddeds = torch.mul(group_embeddeds, item_embeddeds) # shape: (batch_size, embedding_dim)\n",
    "        \n",
    "        # pooling: [group x item, group, item]\n",
    "        pooled_embeddeds = torch.cat((interacted_embeddeds, group_embeddeds, item_embeddeds), dim=1) # shape: (batch_size, 3*embedding_dim)\n",
    "        \n",
    "        y = torch.sigmoid(self.predict(pooled_embeddeds))\n",
    "        return y\n",
    "\n",
    "    # user forwarding\n",
    "    def user_forward(self, uids, iids):\n",
    "        \n",
    "        # uids.shape: (batch_size, )\n",
    "        # iids.shape: (batch_size, )\n",
    "        \n",
    "        # generate user embedding vectors\n",
    "        user_embeddeds = self.user_embedding(\n",
    "            uids.clone().type(torch.long).unsqueeze(dim=1).to(DEVICE)) # (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # generate item embedding vectors\n",
    "        item_embeddeds = self.item_embedding(\n",
    "            iids.clone().type(torch.long).unsqueeze(dim=1).to(DEVICE)) # (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # element-wise product: user-item interactions\n",
    "        interacted_embeddeds = torch.mul(user_embeddeds, item_embeddeds) # (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # pooling: [user x item, group, item]\n",
    "        pooled_embeddeds = torch.cat((interacted_embeddeds, user_embeddeds, item_embeddeds), dim=2) # (batch_size, 1, 3 * embedding_dim)\n",
    "        \n",
    "        # reshape x from (batch_size, 1, 3 * embedding_dim) to (batch_size, 3 * embedding_dim)\n",
    "        pooled_embeddeds = pooled_embeddeds.squeeze(dim=1)\n",
    "        \n",
    "        y = torch.sigmoid(self.predict(pooled_embeddeds))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without any member aggregation, treat group as indivudual\n",
    "\n",
    "class GNCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_groups, embedding_dim, group_member_mapping, drop_ratio):\n",
    "        super(GNCF, self).__init__()\n",
    "        \n",
    "        self.user_embedding = UserEmbeddingLayer(num_users, embedding_dim)\n",
    "        self.item_embedding = ItemEmbeddingLayer(num_items, embedding_dim)\n",
    "        self.group_embedding = GroupEmbeddingLayer(num_groups, embedding_dim)\n",
    "        self.predict = PredictLayer(3 * embedding_dim, drop_ratio)\n",
    "        \n",
    "        self.group_members = group_member_mapping\n",
    "        \n",
    "        self.num_users = num_users\n",
    "        self.num_groups = num_groups\n",
    "        self.num_items = num_items\n",
    "        \n",
    "        # initial model's parameters\n",
    "        for m in self.modules():\n",
    "            \n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=1) # normal distribution\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.xavier_normal_(m.weight) # Glorot initialization\n",
    "\n",
    "    def forward(self, gids, uids, iids):\n",
    "        \n",
    "        # group prediction\n",
    "        if (gids is not None) and (uids is None):\n",
    "            output = self.group_forward(gids, iids)\n",
    "        # user prediction\n",
    "        else:\n",
    "            output = self.user_forward(uids, iids)\n",
    "            \n",
    "        return output\n",
    "\n",
    "    # group forwarding\n",
    "    def group_forward(self, gids, iids):\n",
    "        \n",
    "        # gids.shape: (batch_size, )\n",
    "        # iids.shape: (batch_size, )\n",
    "        \n",
    "        # generate user embedding vectors\n",
    "        group_embeddeds = self.group_embedding(\n",
    "            gids.clone().type(torch.long).unsqueeze(dim=1).to(DEVICE)) # (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # generate item embedding vectors\n",
    "        item_embeddeds = self.item_embedding(\n",
    "            iids.clone().type(torch.long).unsqueeze(dim=1).to(DEVICE)) # (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # element-wise product: user-item interactions\n",
    "        interacted_embeddeds = torch.mul(group_embeddeds, item_embeddeds) # (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # pooling: [user x item, group, item]\n",
    "        pooled_embeddeds = torch.cat((interacted_embeddeds, group_embeddeds, item_embeddeds), dim=2) # (batch_size, 1, 3 * embedding_dim)\n",
    "        \n",
    "        # reshape x from (batch_size, 1, 3 * embedding_dim) to (batch_size, 3 * embedding_dim)\n",
    "        pooled_embeddeds = pooled_embeddeds.squeeze(dim=1)\n",
    "        \n",
    "        y = torch.sigmoid(self.predict(pooled_embeddeds))\n",
    "        \n",
    "        return y\n",
    "\n",
    "    # user forwarding\n",
    "    def user_forward(self, uids, iids):\n",
    "        \n",
    "        # uids.shape: (batch_size, )\n",
    "        # iids.shape: (batch_size, )\n",
    "        \n",
    "        # generate user embedding vectors\n",
    "        user_embeddeds = self.user_embedding(\n",
    "            uids.clone().type(torch.long).unsqueeze(dim=1).to(DEVICE)) # (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # generate item embedding vectors\n",
    "        item_embeddeds = self.item_embedding(\n",
    "            iids.clone().type(torch.long).unsqueeze(dim=1).to(DEVICE)) # (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # element-wise product: user-item interactions\n",
    "        interacted_embeddeds = torch.mul(user_embeddeds, item_embeddeds) # (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # pooling: [user x item, group, item]\n",
    "        pooled_embeddeds = torch.cat((interacted_embeddeds, user_embeddeds, item_embeddeds), dim=2) # (batch_size, 1, 3 * embedding_dim)\n",
    "        \n",
    "        # reshape x from (batch_size, 1, 3 * embedding_dim) to (batch_size, 3 * embedding_dim)\n",
    "        pooled_embeddeds = pooled_embeddeds.squeeze(dim=1)\n",
    "        \n",
    "        y = torch.sigmoid(self.predict(pooled_embeddeds))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, uid_piid_list, niids_list, K, input_type):\n",
    "    \"\"\"\n",
    "    Evaluate the performance (Hit_Ratio, NDCG) of top-K recommendation\n",
    "    Return: score of each test rating.\n",
    "    \"\"\"\n",
    "    \n",
    "    # uid_piid_list shape: (testset size, 2)\n",
    "    # niids_list shape: (testset size, 100)\n",
    "    \n",
    "    testset_size = len(uid_piid_list)\n",
    "    \n",
    "    hits, ndcgs = [], []\n",
    "\n",
    "    # for printing progress\n",
    "    max_percentage = -1\n",
    "    print(\"Evaluating: \", end=\"\")\n",
    "    for idx in range(testset_size):\n",
    "        \n",
    "        # print progress\n",
    "        if idx%10 == 0:\n",
    "            percentage = idx*100/testset_size\n",
    "            if int(percentage/10) > max_percentage:\n",
    "                max_percentage = int(percentage/10)\n",
    "                print(\"{:.1f}%\".format(percentage), end=\" \")\n",
    "        \n",
    "        (hr, ndcg) = evaluate_one_example(model, uid_piid_list, niids_list, K, input_type, idx)\n",
    "        \n",
    "        hits.append(hr)\n",
    "        ndcgs.append(ndcg)\n",
    "        \n",
    "    print(\"\") # line-break\n",
    "        \n",
    "    return (hits, ndcgs)\n",
    "\n",
    "\n",
    "def evaluate_one_example(model, uid_piid_list, niids_list, K, input_type, idx):\n",
    "    \n",
    "    uid, piid = uid_piid_list[idx]\n",
    "    \n",
    "    # items to be predicted\n",
    "    iids = torch.tensor(niids_list[idx] + [piid], dtype=torch.long).to(DEVICE)\n",
    "    uids = torch.full(iids.shape, uid, dtype=torch.long).to(DEVICE)\n",
    "    \n",
    "    # store prediction scores\n",
    "    iid_scores = {}\n",
    "\n",
    "    if input_type == 'group':\n",
    "        predictions = model(uids, None, iids)\n",
    "    elif input_type == 'user':\n",
    "        predictions = model(None, uids, iids)\n",
    "\n",
    "    for idx in range(len(iids)):\n",
    "        iid = iids[idx]\n",
    "        iid_scores[iid] = torch.flatten(predictions)[idx]\n",
    "\n",
    "    # Evaluate top rank list\n",
    "    top_k_iids = heapq.nlargest(K, iid_scores, key=iid_scores.get)\n",
    "    \n",
    "    hr = evaluate_HR(top_k_iids, piid)\n",
    "    ndcg = evaluate_NDCG(top_k_iids, piid)\n",
    "    \n",
    "    return (hr, ndcg)\n",
    "\n",
    "def evaluate_HR(top_k_iids, piid):\n",
    "    \n",
    "    for iid in top_k_iids:\n",
    "        if iid == piid:\n",
    "            return 1\n",
    "        \n",
    "    return 0\n",
    "\n",
    "def evaluate_NDCG(top_k_iids, piid):\n",
    "    \n",
    "    for idx in range(len(top_k_iids)):\n",
    "        iid = top_k_iids[idx]\n",
    "        \n",
    "        if iid == piid:\n",
    "            return math.log(2) / math.log(idx+2)\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, uid_piid_list, niids_list, K, input_type):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    (hrs, ndcgs) = evaluate_model(model, uid_piid_list, niids_list, K, input_type)\n",
    "    avg_hr, avg_ndcg = np.mean(hrs), np.mean(ndcgs)\n",
    "    \n",
    "    return avg_hr, avg_ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training procedure\n",
    "def training(model, dataloader, epoch_id, config, input_type):\n",
    "    \n",
    "    # user trainning\n",
    "    learning_rate = config.lr\n",
    "\n",
    "    # lr decay: halve for every five epochs\n",
    "    for _ in range(0, epoch_id, 5):\n",
    "        learning_rate /= 2\n",
    "\n",
    "    # create optimizer\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"Epoch {}, lr = {}, input_type = {}\".format(epoch_id, learning_rate, input_type))\n",
    "\n",
    "    losses = []\n",
    "    for batch_id, (uids, piids_niids) in enumerate(dataloader):\n",
    "        \n",
    "        if batch_id%10 == 0:\n",
    "            print(\".\", end=\"\")\n",
    "        \n",
    "        # Data Load\n",
    "        p_iids = piids_niids[:, 0]\n",
    "        n_iids = piids_niids[:, 1]\n",
    "        \n",
    "        # Forward\n",
    "        if input_type == 'user':\n",
    "            positive_predictions = model(None, uids, p_iids)\n",
    "            negative_predictions = model(None, uids, n_iids)\n",
    "        elif input_type == 'group':\n",
    "            positive_predictions = model(uids, None, p_iids)\n",
    "            negative_predictions = model(uids, None, n_iids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = torch.mean((positive_predictions - negative_predictions -1) **2)\n",
    "        \n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # record loss history\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    print(\"\") # line-break\n",
    "    \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training(model_name='AGREE', name=\"my_training\", embedding_dim=6, train_epoch=30,\n",
    "        num_negatives=4, batch_size=512, lr=0.00005, drop_ratio=0.2, top_k=5):\n",
    "    \n",
    "    # create dir to store configs, models and training history\n",
    "    training_directory = 'models/'+ name\n",
    "    os.mkdir(training_directory)\n",
    "    \n",
    "    # create configuration object\n",
    "    config = Config(embedding_size=embedding_dim, epoch=train_epoch,\n",
    "        num_negatives=num_negatives, batch_size=batch_size, lr=lr, drop_ratio=drop_ratio, top_k=top_k)\n",
    "    \n",
    "    # save configs to file\n",
    "    config.export_json(training_directory + \"/config.json\")\n",
    "    \n",
    "    # load and parse dataset\n",
    "    dataset = CAMRa2011Dataset(config.data_dir)\n",
    "    num_group, num_user, num_item = dataset.get_sizes()\n",
    "    print(\"number of groups: {}\".format(num_group))\n",
    "    print(\"number of users: {}\".format(num_user))\n",
    "    print(\"number of items: {}\".format(num_item))\n",
    "    \n",
    "    # create model object\n",
    "    if model_name == 'AGREE':\n",
    "        model = AGREE(num_user, num_item, num_group, config.embedding_size,\n",
    "                      dataset.group_members, config.drop_ratio).to(DEVICE)\n",
    "        print(\"AGREE at embedding size: {}, epoch: {}, NDCG & HR: Top-{}\"\n",
    "              .format(config.embedding_size, config.epoch, config.top_k))\n",
    "    elif model_name == 'GREE':\n",
    "        model = GREE(num_user, num_item, num_group, config.embedding_size,\n",
    "                      dataset.group_members, config.drop_ratio).to(DEVICE)\n",
    "        print(\"GREE at embedding size: {}, epoch: {}, NDCG & HR: Top-{}\"\n",
    "              .format(config.embedding_size, config.epoch, config.top_k))\n",
    "        \n",
    "    elif model_name == 'GNCF':\n",
    "        model = GNCF(num_user, num_item, num_group, config.embedding_size,\n",
    "                      dataset.group_members, config.drop_ratio).to(DEVICE)\n",
    "        print(\"GNCF at embedding size: {}, epoch: {}, NDCG & HR: Top-{}\"\n",
    "              .format(config.embedding_size, config.epoch, config.top_k))\n",
    "    \n",
    "    # start training\n",
    "    all_history = []\n",
    "    for epoch in range(config.epoch):\n",
    "\n",
    "        history = { 'loss': {}, 'hr': {}, 'ndcg': {} }\n",
    "        \n",
    "        # set the model in train mode (some network like dropout will behave differently in train mode / evaluaiton mode)\n",
    "        model.train(mode=True)\n",
    "\n",
    "        # train the model using user interactions\n",
    "        user_loss = training(model,\n",
    "                     dataset.get_user_dataloader(batch_size=config.batch_size, shuffle=True, num_negatives=config.num_negatives),\n",
    "                     epoch, config, 'user')\n",
    "        history['loss']['user'] = user_loss\n",
    "\n",
    "        # train the model using group & members interactions\n",
    "        group_loss = training(model,\n",
    "                        dataset.get_group_dataloader(batch_size=config.batch_size, shuffle=True, num_negatives=config.num_negatives),\n",
    "                        epoch, config, 'group')\n",
    "        history['loss']['group'] = group_loss\n",
    "\n",
    "        print(\"Losses: {}\".format(history['loss']))\n",
    "\n",
    "        # evaluation\n",
    "        avg_user_hr, avg_user_ndcg = evaluation(model, dataset.test_user_list, dataset.test_user_negative_list, config.top_k, 'user')\n",
    "        print(\"User-- average Top-{} Hit Rate: {:.4f}, average Top-{} NDCG: {:.4f}\"\n",
    "              .format(config.top_k, avg_user_hr, config.top_k, avg_user_ndcg))\n",
    "        history['hr']['user'] = avg_user_hr\n",
    "        history['ndcg']['user'] = avg_user_ndcg\n",
    "\n",
    "        avg_group_hr, avg_group_ndcg = evaluation(model, dataset.test_group_list, dataset.test_group_negative_list, config.top_k, 'group')\n",
    "        print(\"Group-- average Top-{} Hit Rate: {:.4f}, average Top-{} NDCG: {:.4f}\"\n",
    "              .format(config.top_k, avg_group_hr, config.top_k, avg_group_ndcg))\n",
    "        history['hr']['group'] = avg_group_hr\n",
    "        history['ndcg']['group'] = avg_group_ndcg\n",
    "        \n",
    "        all_history.append(history)\n",
    "        \n",
    "        # save model to file\n",
    "        torch.save(model.state_dict(), training_directory+ \"/{}_e{}.pt\".format(model_name, epoch))\n",
    "        \n",
    "        # save training & evaluation result to file\n",
    "        with open(training_directory+\"/history.json\", 'w') as file:\n",
    "                file.write(json.dumps(all_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of groups: 290\n",
      "number of users: 602\n",
      "number of items: 7710\n",
      "AGREE at embedding size: 6, epoch: 20, NDCG & HR: Top-5\n",
      "Epoch 0, lr = 1e-05, input_type = user\n",
      "............................................\n",
      "Epoch 0, lr = 1e-05, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.995277810833274, 'group': 0.9872857651202317}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.1837, average Top-5 NDCG: 0.1187\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.1745, average Top-5 NDCG: 0.1131\n",
      "Epoch 1, lr = 5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 1, lr = 5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9788238283425774, 'group': 0.974697018696878}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.2801, average Top-5 NDCG: 0.1855\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.2683, average Top-5 NDCG: 0.1739\n",
      "Epoch 2, lr = 5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 2, lr = 5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9673250745854062, 'group': 0.9623411878977233}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.3668, average Top-5 NDCG: 0.2437\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.3400, average Top-5 NDCG: 0.2269\n",
      "Epoch 3, lr = 5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 3, lr = 5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9540670881009484, 'group': 0.9489722084296264}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.4385, average Top-5 NDCG: 0.2888\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.3986, average Top-5 NDCG: 0.2647\n",
      "Epoch 4, lr = 5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 4, lr = 5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9391727210182347, 'group': 0.9339175225655778}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.4937, average Top-5 NDCG: 0.3200\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4524, average Top-5 NDCG: 0.2974\n",
      "Epoch 5, lr = 5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 5, lr = 5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9229159982580888, 'group': 0.9174510856874946}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5296, average Top-5 NDCG: 0.3429\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4903, average Top-5 NDCG: 0.3167\n",
      "Epoch 6, lr = 2.5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 6, lr = 2.5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9078296286563305, 'group': 0.9066189265305222}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5369, average Top-5 NDCG: 0.3488\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5028, average Top-5 NDCG: 0.3235\n",
      "Epoch 7, lr = 2.5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 7, lr = 2.5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.8987630789830974, 'group': 0.8979863447396934}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5439, average Top-5 NDCG: 0.3536\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5166, average Top-5 NDCG: 0.3298\n",
      "Epoch 8, lr = 2.5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 8, lr = 2.5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.8891153334207338, 'group': 0.8886372583523359}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5472, average Top-5 NDCG: 0.3570\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5241, average Top-5 NDCG: 0.3354\n",
      "Epoch 9, lr = 2.5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 9, lr = 2.5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.8793117732696184, 'group': 0.8792609033130464}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5528, average Top-5 NDCG: 0.3612\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5269, average Top-5 NDCG: 0.3399\n",
      "Epoch 10, lr = 2.5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 10, lr = 2.5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.8694367794750484, 'group': 0.8693506193269137}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5581, average Top-5 NDCG: 0.3652\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5331, average Top-5 NDCG: 0.3446\n",
      "Epoch 11, lr = 1.25e-06, input_type = user\n",
      "............................................\n",
      "Epoch 11, lr = 1.25e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.8603582949605632, 'group': 0.8631893300415437}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5625, average Top-5 NDCG: 0.3676\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5372, average Top-5 NDCG: 0.3466\n",
      "Epoch 12, lr = 1.25e-06, input_type = user\n",
      "............................................\n",
      "Epoch 12, lr = 1.25e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.8554055483444877, 'group': 0.858338885972289}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5671, average Top-5 NDCG: 0.3708\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5372, average Top-5 NDCG: 0.3469\n",
      "Epoch 13, lr = 1.25e-06, input_type = user\n",
      "............................................\n",
      "Epoch 13, lr = 1.25e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.8500934556910866, 'group': 0.853113609511836}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5698, average Top-5 NDCG: 0.3728\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5393, average Top-5 NDCG: 0.3483\n",
      "Epoch 14, lr = 1.25e-06, input_type = user\n",
      "............................................\n",
      "Epoch 14, lr = 1.25e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.8445355395703349, 'group': 0.8480155508534438}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5721, average Top-5 NDCG: 0.3742\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5434, average Top-5 NDCG: 0.3516\n",
      "Epoch 15, lr = 1.25e-06, input_type = user\n",
      "............................................\n",
      "Epoch 15, lr = 1.25e-06, input_type = group\n",
      "....."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-6027a4be6cdc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m start_training(model_name=\"AGREE\", name=\"AGREE-basic\", embedding_dim=6, train_epoch=20,\n\u001b[1;32m----> 2\u001b[1;33m         num_negatives=4, batch_size=1024, lr=0.00001, drop_ratio=0.1, top_k=5)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-a8ec1f5685b4>\u001b[0m in \u001b[0;36mstart_training\u001b[1;34m(model_name, name, embedding_dim, train_epoch, num_negatives, batch_size, lr, drop_ratio, top_k)\u001b[0m\n\u001b[0;32m     56\u001b[0m         group_loss = training(model,\n\u001b[0;32m     57\u001b[0m                         \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_group_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_negatives\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_negatives\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                         epoch, config, 'group')\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'group'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-dd2b9b754a82>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(model, dataloader, epoch_id, config, input_type)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0minput_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'group'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mpositive_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_iids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mnegative_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\envs\\attentive\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-c0b068c83d00>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, gids, uids, iids)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# group prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0muids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;31m# user prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-c0b068c83d00>\u001b[0m in \u001b[0;36mgroup_forward\u001b[1;34m(self, gids, iids)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0muser_item_embeddeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmember_embeddeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_item_embeddeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# shape: (num_member, 1, 2 * embedding_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0muser_item_embeddeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_item_embeddeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# shape: (num_member, 2 * embedding_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0mattentive_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_item_embeddeds\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# shape: (num_member, 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# aggregation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\envs\\attentive\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-154cbda41dd5>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\envs\\attentive\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\envs\\attentive\\lib\\site-packages\\torch\\nn\\modules\\dropout.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\envs\\attentive\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m    934\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[0;32m    935\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_training(model_name=\"AGREE\", name=\"AGREE-basic\", embedding_dim=6, train_epoch=20,\n",
    "        num_negatives=4, batch_size=1024, lr=0.00001, drop_ratio=0.1, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of groups: 290\n",
      "number of users: 602\n",
      "number of items: 7710\n",
      "AGREE at embedding size: 6, epoch: 5, NDCG & HR: Top-5\n",
      "Epoch 0, lr = 1e-05, input_type = user\n",
      ".......................................................\n",
      "Epoch 0, lr = 1e-05, input_type = group\n",
      "........................................................\n",
      "Losses: {'user': 0.9960147632759275, 'group': 0.9812008813158787}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.2412, average Top-5 NDCG: 0.1539\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.2207, average Top-5 NDCG: 0.1453\n",
      "Epoch 1, lr = 5e-06, input_type = user\n",
      ".......................................................\n",
      "Epoch 1, lr = 5e-06, input_type = group\n",
      "........................................................\n",
      "Losses: {'user': 0.9632067706510833, 'group': 0.9575228952885539}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.3588, average Top-5 NDCG: 0.2275\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.3221, average Top-5 NDCG: 0.2107\n",
      "Epoch 2, lr = 5e-06, input_type = user\n",
      ".......................................................\n",
      "Epoch 2, lr = 5e-06, input_type = group\n",
      "........................................................\n",
      "Losses: {'user': 0.9425803198456982, 'group': 0.9371402949473386}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.4362, average Top-5 NDCG: 0.2780\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.3972, average Top-5 NDCG: 0.2596\n",
      "Epoch 3, lr = 5e-06, input_type = user\n",
      ".......................................................\n",
      "Epoch 3, lr = 5e-06, input_type = group\n",
      "........................................................\n",
      "Losses: {'user': 0.9206137925222979, 'group': 0.9141517029483608}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.4934, average Top-5 NDCG: 0.3165\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4483, average Top-5 NDCG: 0.2946\n",
      "Epoch 4, lr = 5e-06, input_type = user\n",
      ".......................................................\n",
      "Epoch 4, lr = 5e-06, input_type = group\n",
      "........................................................\n",
      "Losses: {'user': 0.8985922698346963, 'group': 0.8892562928087265}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5329, average Top-5 NDCG: 0.3425\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4800, average Top-5 NDCG: 0.3182\n"
     ]
    }
   ],
   "source": [
    "# AGREE: use more negative instance(5)\n",
    "start_training(model_name=\"AGREE\", name=\"AGREE-n5\", embedding_dim=6, train_epoch=5,\n",
    "        num_negatives=5, batch_size=1024, lr=0.00001, drop_ratio=0.1, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of groups: 290\n",
      "number of users: 602\n",
      "number of items: 7710\n",
      "AGREE at embedding size: 6, epoch: 5, NDCG & HR: Top-5\n",
      "Epoch 0, lr = 1e-05, input_type = user\n",
      "..................................................................\n",
      "Epoch 0, lr = 1e-05, input_type = group\n",
      "...................................................................\n",
      "Losses: {'user': 0.9891845682045308, 'group': 0.9679760510910658}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.2535, average Top-5 NDCG: 0.1566\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.2317, average Top-5 NDCG: 0.1515\n",
      "Epoch 1, lr = 5e-06, input_type = user\n",
      "..................................................................\n",
      "Epoch 1, lr = 5e-06, input_type = group\n",
      "...................................................................\n",
      "Losses: {'user': 0.9429591259033214, 'group': 0.9357978438285043}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.3734, average Top-5 NDCG: 0.2374\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.3414, average Top-5 NDCG: 0.2213\n",
      "Epoch 2, lr = 5e-06, input_type = user\n",
      "..................................................................\n",
      "Epoch 2, lr = 5e-06, input_type = group\n",
      "...................................................................\n",
      "Losses: {'user': 0.9149598534695986, 'group': 0.9078478401078759}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.4615, average Top-5 NDCG: 0.2940\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4290, average Top-5 NDCG: 0.2789\n",
      "Epoch 3, lr = 5e-06, input_type = user\n",
      "..................................................................\n",
      "Epoch 3, lr = 5e-06, input_type = group\n",
      "...................................................................\n",
      "Losses: {'user': 0.8851720945137304, 'group': 0.8784487239351432}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5213, average Top-5 NDCG: 0.3339\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4793, average Top-5 NDCG: 0.3124\n",
      "Epoch 4, lr = 5e-06, input_type = user\n",
      "..................................................................\n",
      "Epoch 4, lr = 5e-06, input_type = group\n",
      "...................................................................\n",
      "Losses: {'user': 0.8549167787338176, 'group': 0.847793330583558}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5512, average Top-5 NDCG: 0.3554\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5103, average Top-5 NDCG: 0.3318\n"
     ]
    }
   ],
   "source": [
    "# AGREE: use more negative instance(5)\n",
    "start_training(model_name=\"AGREE\", name=\"AGREE-n6\", embedding_dim=6, train_epoch=5,\n",
    "        num_negatives=6, batch_size=1024, lr=0.00001, drop_ratio=0.1, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of groups: 290\n",
      "number of users: 602\n",
      "number of items: 7710\n",
      "GREE at embedding size: 6, epoch: 15, NDCG & HR: Top-5\n",
      "Epoch 0, lr = 1e-05, input_type = user\n",
      "............................................\n",
      "Epoch 0, lr = 1e-05, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9990028688509349, 'group': 0.9961803443307509}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.1030, average Top-5 NDCG: 0.0621\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.1069, average Top-5 NDCG: 0.0632\n",
      "Epoch 1, lr = 5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 1, lr = 5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9924351422955843, 'group': 0.9916884389594028}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.1741, average Top-5 NDCG: 0.1072\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.1634, average Top-5 NDCG: 0.0991\n",
      "Epoch 2, lr = 5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 2, lr = 5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9880384568218781, 'group': 0.9873522041606254}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.2439, average Top-5 NDCG: 0.1556\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.2372, average Top-5 NDCG: 0.1448\n",
      "Epoch 3, lr = 5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 3, lr = 5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9829999331205879, 'group': 0.9823914458151577}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.3276, average Top-5 NDCG: 0.2088\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.2986, average Top-5 NDCG: 0.1895\n",
      "Epoch 4, lr = 5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 4, lr = 5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9774620044695158, 'group': 0.9767678700336794}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.3894, average Top-5 NDCG: 0.2511\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.3600, average Top-5 NDCG: 0.2293\n",
      "Epoch 5, lr = 5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 5, lr = 5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9714526415416797, 'group': 0.970530356409328}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.4468, average Top-5 NDCG: 0.2852\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4166, average Top-5 NDCG: 0.2629\n",
      "Epoch 6, lr = 2.5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 6, lr = 2.5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9657218232580399, 'group': 0.966523562158857}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.4648, average Top-5 NDCG: 0.2972\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4317, average Top-5 NDCG: 0.2744\n",
      "Epoch 7, lr = 2.5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 7, lr = 2.5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9623192835181474, 'group': 0.9630779340153649}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.4847, average Top-5 NDCG: 0.3103\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4510, average Top-5 NDCG: 0.2855\n",
      "Epoch 8, lr = 2.5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 8, lr = 2.5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.958669314258977, 'group': 0.9591552032635056}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.4950, average Top-5 NDCG: 0.3176\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4607, average Top-5 NDCG: 0.2912\n",
      "Epoch 9, lr = 2.5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 9, lr = 2.5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9548591577215653, 'group': 0.9552388760238008}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5063, average Top-5 NDCG: 0.3246\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4779, average Top-5 NDCG: 0.3009\n",
      "Epoch 10, lr = 2.5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 10, lr = 2.5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9507652422928974, 'group': 0.9510902497503493}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5186, average Top-5 NDCG: 0.3314\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4890, average Top-5 NDCG: 0.3085\n",
      "Epoch 11, lr = 1.25e-06, input_type = user\n",
      "............................................\n",
      "Epoch 11, lr = 1.25e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9472699487236599, 'group': 0.948622135078015}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5236, average Top-5 NDCG: 0.3349\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4938, average Top-5 NDCG: 0.3121\n",
      "Epoch 12, lr = 1.25e-06, input_type = user\n",
      "............................................\n",
      "Epoch 12, lr = 1.25e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9451928235573408, 'group': 0.9461878892245477}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5279, average Top-5 NDCG: 0.3388\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5000, average Top-5 NDCG: 0.3157\n",
      "Epoch 13, lr = 1.25e-06, input_type = user\n",
      "............................................\n",
      "Epoch 13, lr = 1.25e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9426836839256898, 'group': 0.9441859741059561}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5319, average Top-5 NDCG: 0.3414\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5041, average Top-5 NDCG: 0.3186\n",
      "Epoch 14, lr = 1.25e-06, input_type = user\n",
      "............................................\n",
      "Epoch 14, lr = 1.25e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9404967940099179, 'group': 0.9419899081426953}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5359, average Top-5 NDCG: 0.3446\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5083, average Top-5 NDCG: 0.3212\n"
     ]
    }
   ],
   "source": [
    "start_training(model_name=\"GREE\", name=\"GREE-basic\", embedding_dim=6, train_epoch=15,\n",
    "        num_negatives=4, batch_size=1024, lr=0.00001, drop_ratio=0.1, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of groups: 290\n",
      "number of users: 602\n",
      "number of items: 7710\n",
      "GNCF at embedding size: 6, epoch: 15, NDCG & HR: Top-5\n",
      "Epoch 0, lr = 1e-05, input_type = user\n",
      "............................................\n",
      "Epoch 0, lr = 1e-05, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9970118863086133, 'group': 0.9893124192750373}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.1661, average Top-5 NDCG: 0.1045\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.1628, average Top-5 NDCG: 0.1049\n",
      "Epoch 1, lr = 5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 1, lr = 5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9820374517746321, 'group': 0.9780903872178526}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.2502, average Top-5 NDCG: 0.1633\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.2331, average Top-5 NDCG: 0.1512\n",
      "Epoch 2, lr = 5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 2, lr = 5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9718807365583337, 'group': 0.9672182902457223}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.3439, average Top-5 NDCG: 0.2214\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.3041, average Top-5 NDCG: 0.1958\n",
      "Epoch 3, lr = 5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 3, lr = 5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9603572860189652, 'group': 0.9557785674287619}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.4050, average Top-5 NDCG: 0.2651\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.3772, average Top-5 NDCG: 0.2383\n",
      "Epoch 4, lr = 5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 4, lr = 5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9476078310874835, 'group': 0.943191213537506}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.4532, average Top-5 NDCG: 0.2975\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4221, average Top-5 NDCG: 0.2657\n",
      "Epoch 5, lr = 5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 5, lr = 5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9334794183344808, 'group': 0.9294579580527584}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.4887, average Top-5 NDCG: 0.3213\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4572, average Top-5 NDCG: 0.2861\n",
      "Epoch 6, lr = 2.5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 6, lr = 2.5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9206731371257616, 'group': 0.9207429987232701}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5033, average Top-5 NDCG: 0.3297\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4731, average Top-5 NDCG: 0.2951\n",
      "Epoch 7, lr = 2.5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 7, lr = 2.5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9130956407815423, 'group': 0.913189791632888}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5209, average Top-5 NDCG: 0.3390\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4814, average Top-5 NDCG: 0.3012\n",
      "Epoch 8, lr = 2.5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 8, lr = 2.5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.9049237210908798, 'group': 0.9056857033651702}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5336, average Top-5 NDCG: 0.3458\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4869, average Top-5 NDCG: 0.3076\n",
      "Epoch 9, lr = 2.5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 9, lr = 2.5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.896492890689684, 'group': 0.8980668430425682}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5442, average Top-5 NDCG: 0.3518\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.4938, average Top-5 NDCG: 0.3129\n",
      "Epoch 10, lr = 2.5e-06, input_type = user\n",
      "............................................\n",
      "Epoch 10, lr = 2.5e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.8876545547893445, 'group': 0.8899965561977049}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5508, average Top-5 NDCG: 0.3566\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5021, average Top-5 NDCG: 0.3202\n",
      "Epoch 11, lr = 1.25e-06, input_type = user\n",
      "............................................\n",
      "Epoch 11, lr = 1.25e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.8801597685508379, 'group': 0.8853112544332232}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5561, average Top-5 NDCG: 0.3599\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5062, average Top-5 NDCG: 0.3224\n",
      "Epoch 12, lr = 1.25e-06, input_type = user\n",
      "............................................\n",
      "Epoch 12, lr = 1.25e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.8756168919788072, 'group': 0.8808863700922925}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5621, average Top-5 NDCG: 0.3627\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5083, average Top-5 NDCG: 0.3230\n",
      "Epoch 13, lr = 1.25e-06, input_type = user\n",
      "............................................\n",
      "Epoch 13, lr = 1.25e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.8714240897165556, 'group': 0.8767652773803054}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5651, average Top-5 NDCG: 0.3636\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5138, average Top-5 NDCG: 0.3260\n",
      "Epoch 14, lr = 1.25e-06, input_type = user\n",
      "............................................\n",
      "Epoch 14, lr = 1.25e-06, input_type = group\n",
      ".............................................\n",
      "Losses: {'user': 0.8665758738255883, 'group': 0.872523874517471}\n",
      "Evaluating: 0.0% 10.3% 20.3% 30.2% 40.2% 50.2% 60.1% 70.1% 80.1% 90.0% \n",
      "User-- average Top-5 Hit Rate: 0.5661, average Top-5 NDCG: 0.3658\n",
      "Evaluating: 0.0% 10.3% 20.0% 30.3% 40.0% 50.3% 60.0% 70.3% 80.0% 90.3% \n",
      "Group-- average Top-5 Hit Rate: 0.5179, average Top-5 NDCG: 0.3286\n"
     ]
    }
   ],
   "source": [
    "start_training(model_name=\"GNCF\", name=\"GNCF-basic\", embedding_dim=6, train_epoch=15,\n",
    "        num_negatives=4, batch_size=1024, lr=0.00001, drop_ratio=0.1, top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
