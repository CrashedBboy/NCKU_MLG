{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages initialization\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "# time\n",
    "# math\n",
    "# heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.data_dir = './data/CAMRa2011/'\n",
    "        self.embedding_size = 32\n",
    "        self.epoch = 30\n",
    "        self.num_negatives = 6\n",
    "        self.batch_size = 256\n",
    "        self.lr = [0.000005, 0.000001, 0.0000005]\n",
    "        self.drop_ratio = 0.2\n",
    "        self.topK = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAMRa2011Dataset(object):\n",
    "    \"\"\"CAMRa2011 dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_dir):\n",
    "        \n",
    "        self.pathes = {\n",
    "            'train': {\n",
    "                'user': dataset_dir + \"userRatingTrain.txt\",\n",
    "                'group': dataset_dir + \"groupRatingTrain.txt\"\n",
    "            },\n",
    "            'test': {\n",
    "                'user': dataset_dir + \"userRatingTest.txt\",\n",
    "                'user_negative': dataset_dir + \"userRatingNegative.txt\",\n",
    "                'group': dataset_dir + \"groupRatingTest.txt\",\n",
    "                'group_negative': dataset_dir + \"groupRatingNegative.txt\",\n",
    "            },\n",
    "            'group_user': dataset_dir + \"groupMember.txt\"\n",
    "        }\n",
    "        \n",
    "        # get the mapping of users and groups\n",
    "        # format: {gid: [uid, uid, ..], gid: [uid, uid, ..], ...}\n",
    "        self.group_members = self.get_group_user_mapping()\n",
    "        \n",
    "        # get interaction matrix from uid-iid training set\n",
    "        # train_user_matrix[uid, iid] = [1 | 0]\n",
    "        self.train_user_matrix = self.get_interaction_matrix(self.pathes['train']['user'])\n",
    "        \n",
    "        # format: [[uid, iid], [uid, iid], ...]\n",
    "        # only pairs of users & items have interactions would appear in the list\n",
    "        self.test_user_list = self.get_interaction_list(self.pathes['test']['user'])\n",
    "        \n",
    "        # format: [[uid, ...], [uid, ...], ...]\n",
    "        # test_user_negative_list & test_user_list follow the same order\n",
    "        # e.g. test_user_negative_list[0] is for test_user_list[0]\n",
    "        self.test_user_negative_list = self.get_negatives(self.pathes['test']['user_negative'])\n",
    "        \n",
    "        # get interaction matrix from gid-iid training set\n",
    "        self.train_group_matrix = self.get_interaction_matrix(self.pathes['train']['group'])\n",
    "\n",
    "        # pairs of group & item to be tested\n",
    "        self.test_group_list = self.get_interaction_list(self.pathes['test']['group'])\n",
    "\n",
    "        self.test_group_negative_list = self.get_negatives(self.pathes['test']['group_negative'])\n",
    "        \n",
    "    def get_user_dataloader(self, batch_size=256, shuffle=True):\n",
    "        \n",
    "        users, positives_negatives = self.get_train_instances(self.train_user_matrix)\n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(users, dtype=torch.float),\n",
    "            torch.tensor(positives_negatives, dtype=torch.float))\n",
    "        \n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        \n",
    "        return loader\n",
    "    \n",
    "    def get_group_dataloader(self, batch_size=256, shuffle=True):\n",
    "        \n",
    "        groups, positives_negatives = self.get_train_instances(self.train_group_matrix)\n",
    "        \n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(groups, dtype=torch.float),\n",
    "            torch.tensor(positives_negatives, dtype=torch.float))\n",
    "        \n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        \n",
    "        return loader\n",
    "        \n",
    "    def get_train_instances(self, interaction_matrix, num_negatives=6):\n",
    "        \n",
    "        users, positive_items, negative_items = [], [], []\n",
    "        \n",
    "        num_users, num_items = interaction_matrix.shape\n",
    "        \n",
    "        for (uid, iid) in interaction_matrix.keys():\n",
    "            \n",
    "            # positive instance\n",
    "            for _ in range(num_negatives):\n",
    "                \n",
    "                # positive instances\n",
    "                positive_items.append(iid)\n",
    "                \n",
    "                # negative instances\n",
    "                negative_iid = np.random.randint(num_items+1)\n",
    "                while (uid, negative_iid) in interaction_matrix:\n",
    "                    negative_iid = np.random.randint(num_items+1) # re-generate an negative iid\n",
    "                negative_items.append(negative_iid)\n",
    "                \n",
    "                # users\n",
    "                users.append(uid)\n",
    "\n",
    "        positives_negatives = [[positive_iid, negative_iid] for positive_iid, negative_iid in zip(positive_items, negative_items)]\n",
    "        \n",
    "        return users, positives_negatives\n",
    "        \n",
    "        \n",
    "    def get_group_user_mapping(self):\n",
    "    \n",
    "        mapping = {}\n",
    "\n",
    "        # read mapping file\n",
    "        with open(self.pathes['group_user'], 'r') as file:\n",
    "\n",
    "            line = file.readline().strip()\n",
    "            while line != None and line != \"\":\n",
    "\n",
    "                # sample line format: [gid] [uid 1],[uid 2],[uid 3],[uid 4]\n",
    "                sequences = line.split(' ')\n",
    "                gid = int(sequences[0])\n",
    "                mapping[gid] = []\n",
    "                for uid in sequences[1].split(','):\n",
    "                    mapping[gid].append(int(uid))\n",
    "                line = file.readline().strip()\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    # parse all interactions in dataset to 2D sparse matrix\n",
    "    def get_interaction_matrix(self, rating_file_path):\n",
    "\n",
    "        # get number of users and items\n",
    "        num_users, num_items = 0, 0\n",
    "        with open(rating_file_path, \"r\") as file:\n",
    "\n",
    "            line = file.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\" \")\n",
    "                uid, iid = int(arr[0]), int(arr[1])\n",
    "                num_users = max(num_users, uid)\n",
    "                num_items = max(num_items, iid)\n",
    "                line = file.readline()\n",
    "\n",
    "        # construct interaction matrix\n",
    "        # dok_matrix: Dictionary Of Keys based sparse matrix, an efficient structure for constructing sparse matrices incrementally.\n",
    "        matrix = sparse.dok_matrix((num_users + 1, num_items + 1), dtype=np.float32) # iid and uid starts from 1\n",
    "        with open(rating_file_path, \"r\") as file:\n",
    "            line = file.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\" \")\n",
    "                if len(arr) > 2:\n",
    "                    uid, iid, rating = int(arr[0]), int(arr[1]), int(arr[2])\n",
    "                    if (rating > 0):\n",
    "                        matrix[uid, iid] = 1.0\n",
    "                else:\n",
    "                    uid, iid = int(arr[0]), int(arr[1])\n",
    "                    matrix[uid, iid] = 1.0\n",
    "                line = file.readline()\n",
    "\n",
    "        return matrix\n",
    "    \n",
    "    # parse all interactions in dataset to list\n",
    "    def get_interaction_list(self, rating_file_path):\n",
    "\n",
    "        interaction_list = []\n",
    "        with open(rating_file_path, \"r\") as file:\n",
    "            line = file.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\" \")\n",
    "                uid, iid = int(arr[0]), int(arr[1])\n",
    "                interaction_list.append([uid, iid])\n",
    "                line = file.readline()\n",
    "\n",
    "        return interaction_list\n",
    "\n",
    "    # parse negative sample lists for pairs in test set\n",
    "    # negative samples: the items which never been interacted\n",
    "    # the order of returned sample lists must be paired with test list\n",
    "    def get_negatives(self, file_path):\n",
    "\n",
    "        negative_samples_list = []\n",
    "\n",
    "        with open(file_path, \"r\") as file:\n",
    "\n",
    "            line = file.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\" \")\n",
    "\n",
    "                negative_iids = []\n",
    "                for iid in arr[1:]:\n",
    "                    negative_iids.append(int(iid))\n",
    "\n",
    "                negative_samples_list.append(negative_iids)\n",
    "                line = file.readline()\n",
    "\n",
    "        return negative_samples_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CAMRa2011Dataset(config.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_loader = dataset.get_user_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_loader = dataset.get_group_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGREE Model (Attentive Group Representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding networks\n",
    "\n",
    "class UserEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_users, embedding_dim):\n",
    "        super(UserEmbeddingLayer, self).__init__()\n",
    "        self.userEmbedding = nn.Embedding(num_users, embedding_dim)\n",
    "\n",
    "    def forward(self, uids):\n",
    "        user_embedded = self.userEmbedding(uids)\n",
    "        return user_embedded\n",
    "    \n",
    "class ItemEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim):\n",
    "        super(ItemEmbeddingLayer, self).__init__()\n",
    "        self.itemEmbedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "    def forward(self, iids):\n",
    "        item_embedded = self.itemEmbedding(iids)\n",
    "        return item_embedded\n",
    "    \n",
    "class GroupEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_groups, embedding_dim):\n",
    "        super(GroupEmbeddingLayer, self).__init__()\n",
    "        self.groupEmbedding = nn.Embedding(num_groups, embedding_dim)\n",
    "\n",
    "    def forward(self, gids):\n",
    "        group_embedded = self.groupEmbedding(gids)\n",
    "        return group_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention network\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, drop_ratio=0):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(embedding_dim, 16)\n",
    "        self.linear2 = nn.Linear(16, 1)\n",
    "        self.dropout = nn.Dropout(p=drop_ratio)\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_ratio),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        weights = F.softmax(x.view(1, -1), dim=1)\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final layers of AGREE for prediction\n",
    "\n",
    "class PredictLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, drop_ratio=0):\n",
    "        super(PredictLayer, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(embedding_dim, 8)\n",
    "        self.dropout = nn.Dropout(p=drop_ratio)\n",
    "        self.linear2 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        out = self.linear2(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGREE(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_groups, embedding_dim, group_member_mapping, drop_ratio):\n",
    "        super(AGREE, self).__init__()\n",
    "        \n",
    "        self.user_embedding = UserEmbeddingLayer(num_users, embedding_dim)\n",
    "        self.item_embedding = ItemEmbeddingLayer(num_items, embedding_dim)\n",
    "        self.group_embedding = GroupEmbeddingLayer(num_groups, embedding_dim)\n",
    "        self.attention = AttentionLayer(2 * embedding_dim, drop_ratio)\n",
    "        self.predict = PredictLayer(3 * embedding_dim, drop_ratio)\n",
    "        \n",
    "        self.group_members = group_member_mapping\n",
    "        \n",
    "        self.num_users = num_users\n",
    "        self.num_groups = len(self.group_members)\n",
    "        \n",
    "        # initial model's parameters\n",
    "        for m in self.modules():\n",
    "            \n",
    "            print(m)\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=1) # normal distribution\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.xavier_normal_(m.weight) # Glorot initialization\n",
    "\n",
    "    def forward(self, gids, uids, iids):\n",
    "        \n",
    "        # group prediction\n",
    "        if (gids is not None) and (uids is None):\n",
    "            output = self.group_forward(gids, iids)\n",
    "        # user prediction\n",
    "        else:\n",
    "            output = self.user_forward(uids, iids)\n",
    "            \n",
    "        return output\n",
    "\n",
    "    # group forwarding\n",
    "    def group_forward(self, gids, iids):\n",
    "        \n",
    "        # group_embeds = Variable(torch.Tensor())\n",
    "        group_embeddeds = torch.empty(0)\n",
    "        \n",
    "        # generate embedding vector for item\n",
    "        item_embeddeds = self.item_embedding(\n",
    "            torch.tensor(iids, dtype=torch.long).unsqueeze(dim=1)) # shape: (num_item, embedding_dim)\n",
    "        \n",
    "        # get attentive group embedding\n",
    "        for gid, iid in zip(gids, iids):\n",
    "            member_uids = self.group_members[gid]\n",
    "            \n",
    "            # generate user embedding vector\n",
    "            member_embeddeds = self.user_embedding(\n",
    "                torch.tensor(member_uids, dtype=torch.long).unsqueeze(dim=1)) # shape: (num_member, embedding_dim)\n",
    "            \n",
    "            # generate item embedding vector\n",
    "            items = [iid for _ in member_uids]\n",
    "            item_embeddeds = self.item_embedding(\n",
    "                torch.tensor(items, dtype=torch.long).unsqueeze(dim=1)) # shape: (num_member, embedding_dim)\n",
    "            \n",
    "            # get attentive weights for each user-item pair\n",
    "            user_item_embeddeds = torch.cat((member_embeddeds, item_embeddeds), dim=1) # shape: (num_member, 2 * embedding_dim)\n",
    "            attentive_weights = self.attention(user_item_embeddeds) # shape: (num_member, 1)\n",
    "            # aggregation\n",
    "            aggregated_user_item_embeddeds = torch.matmul(attentive_weights.T, member_embeddeds) # shape: (1, embedding_dim)\n",
    "            \n",
    "            # generate group-item embedding vector\n",
    "            group_embedded = self.group_embedding(torch.tensor([[gid]], dtype=torch.long)) # shape: (1, embedding_dim)\n",
    "            \n",
    "            # group embedding = user embedding aggregation + group preference embedding\n",
    "            aggregated_all_embedded = aggregated_user_item_embeddeds + group_embedded # shape: (1, embedding_dim)\n",
    "            \n",
    "            # append group embedding\n",
    "            group_embeddeds = torch.cat((group_embeddeds, group_embedded), dim=0)\n",
    "            \n",
    "        # element-wise product: group-item interaction\n",
    "        interacted_embeddeds = torch.mul(group_embeddeds, item_embeddeds)\n",
    "        \n",
    "        # pooling: [group x item, group, item]\n",
    "        pooled_embeddeds = torch.cat((interacted_embeddeds, group_embeddeds, item_embeddeds), dim=1)\n",
    "        \n",
    "        y = F.sigmoid(self.predict(pooled_embeddeds))\n",
    "        return y\n",
    "\n",
    "    # user forwarding\n",
    "    def usr_forward(self, uids, iids):\n",
    "        # user_inputs_var, item_inputs_var = Variable(user_inputs), Variable(item_inputs)\n",
    "        \n",
    "        # generate user embedding vectors\n",
    "        user_embeddeds = self.user_embedding(\n",
    "            torch.tensor(uids, dtype=torch.long).unsqueeze(dim=1)) # (num_pair, embedding_dim)\n",
    "        \n",
    "        # generate item embedding vectors\n",
    "        item_embeddeds = self.item_embedding(\n",
    "            torch.tensor(iids, dtype=torch.long).unsqueeze(dim=1)) # (num_pair, embedding_dim)\n",
    "        \n",
    "        # element-wise product: user-item interactions\n",
    "        interacted_embeddeds = torch.mul(user_embeddeds, item_embeddeds)\n",
    "        \n",
    "        # pooling: [user x item, group, item]\n",
    "        pooled_embeddeds = torch.cat((interacted_embeddeds, user_embeddeds, item_embeddeds), dim=1)\n",
    "        \n",
    "        y = F.sigmoid(self.predict(pooled_embeddeds))\n",
    "        \n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
